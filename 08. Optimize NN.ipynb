{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Квантизация нейронных сетей\n",
    "\n",
    "Нейронные сети - это модели, которые могут быть потенциально произвольно сложными. Помимо того, что теперь для обучения таких моделей нам требуются гигантские датасеты, сами модели стали слишком большими. \n",
    "\n",
    "Это означает, что они занимают очень много места и например могут физически не помещаться на устройство пользователя. А также они очень медленно делают предсказания.\n",
    "\n",
    "В этот раз поговорим про квантизацию и то, как она помогает сделать модели быстрее и меньше."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть 1. Классическая теория\n",
    "\n",
    "\n",
    "Пойдем по этому блогпосту:\n",
    "https://leimao.github.io/article/Neural-Networks-Quantization/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import copy\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from torch.quantization import QuantStub, DeQuantStub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fb6e4b98310>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SEED=9876\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вновь пробуем обучить сеть из трех полносвязных слоев на fashion mnist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-03-21 00:44:27--  https://media.githubusercontent.com/media/fpleoni/fashion_mnist/master/fashion-mnist_train.csv\n",
      "Resolving media.githubusercontent.com (media.githubusercontent.com)... 185.199.110.133, 185.199.109.133, 185.199.111.133, ...\n",
      "Connecting to media.githubusercontent.com (media.githubusercontent.com)|185.199.110.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 133047193 (127M) [text/plain]\n",
      "Saving to: ‘fashion-mnist_train.csv’\n",
      "\n",
      "fashion-mnist_train 100%[===================>] 126.88M   123MB/s    in 1.0s    \n",
      "\n",
      "2024-03-21 00:44:32 (123 MB/s) - ‘fashion-mnist_train.csv’ saved [133047193/133047193]\n",
      "\n",
      "--2024-03-21 00:44:32--  https://media.githubusercontent.com/media/fpleoni/fashion_mnist/master/fashion-mnist_test.csv\n",
      "Resolving media.githubusercontent.com (media.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.109.133, ...\n",
      "Connecting to media.githubusercontent.com (media.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 22176691 (21M) [text/plain]\n",
      "Saving to: ‘fashion-mnist_test.csv’\n",
      "\n",
      "fashion-mnist_test. 100%[===================>]  21.15M  46.8MB/s    in 0.5s    \n",
      "\n",
      "2024-03-21 00:44:34 (46.8 MB/s) - ‘fashion-mnist_test.csv’ saved [22176691/22176691]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://media.githubusercontent.com/media/fpleoni/fashion_mnist/master/fashion-mnist_train.csv\n",
    "!wget https://media.githubusercontent.com/media/fpleoni/fashion_mnist/master/fashion-mnist_test.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_csv = pd.read_csv(\"fashion-mnist_train.csv\")\n",
    "test_csv = pd.read_csv(\"fashion-mnist_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>pixel9</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "      <th>pixel784</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>43</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  \\\n",
       "0      2       0       0       0       0       0       0       0       0   \n",
       "1      9       0       0       0       0       0       0       0       0   \n",
       "2      6       0       0       0       0       0       0       0       5   \n",
       "3      0       0       0       0       1       2       0       0       0   \n",
       "4      3       0       0       0       0       0       0       0       0   \n",
       "\n",
       "   pixel9  ...  pixel775  pixel776  pixel777  pixel778  pixel779  pixel780  \\\n",
       "0       0  ...         0         0         0         0         0         0   \n",
       "1       0  ...         0         0         0         0         0         0   \n",
       "2       0  ...         0         0         0        30        43         0   \n",
       "3       0  ...         3         0         0         0         0         1   \n",
       "4       0  ...         0         0         0         0         0         0   \n",
       "\n",
       "   pixel781  pixel782  pixel783  pixel784  \n",
       "0         0         0         0         0  \n",
       "1         0         0         0         0  \n",
       "2         0         0         0         0  \n",
       "3         0         0         0         0  \n",
       "4         0         0         0         0  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_csv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train_csv['label'].values\n",
    "X_train = train_csv.drop(['label'],axis=1).values\n",
    "\n",
    "y_test = test_csv['label'].values\n",
    "X_test = test_csv.drop(['label'],axis=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fb6d1d01d60>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X_train[3].reshape(28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fb6d1c47700>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X_train[4].reshape(28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "torch_X_train = torch.from_numpy(X_train).type(torch.LongTensor)\n",
    "torch_y_train = torch.from_numpy(y_train).type(torch.LongTensor)\n",
    "torch_X_test = torch.from_numpy(X_test).type(torch.LongTensor)\n",
    "torch_y_test = torch.from_numpy(y_test).type(torch.LongTensor)\n",
    "\n",
    "train = torch.utils.data.TensorDataset(torch_X_train,torch_y_train)\n",
    "test = torch.utils.data.TensorDataset(torch_X_test,torch_y_test)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train, batch_size = BATCH_SIZE, shuffle = False)\n",
    "test_loader = torch.utils.data.DataLoader(test, batch_size = BATCH_SIZE, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (linear1): Linear(in_features=784, out_features=250, bias=True)\n",
      "  (linear2): Linear(in_features=250, out_features=100, bias=True)\n",
      "  (linear3): Linear(in_features=100, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.linear1 = nn.Linear(784,250)\n",
    "        self.linear2 = nn.Linear(250,100)\n",
    "        self.linear3 = nn.Linear(100,10)\n",
    "    \n",
    "    def forward(self,X):\n",
    "        X = F.relu(self.linear1(X))\n",
    "        X = F.relu(self.linear2(X))\n",
    "        X = self.linear3(X)\n",
    "        return X\n",
    "\n",
    "mlp = MLP()\n",
    "print(mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, train_loader, epoch_number=2):\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "    error = nn.CrossEntropyLoss()\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(epoch_number):\n",
    "        correct = 0\n",
    "        \n",
    "        for batch_idx, (X_batch, y_batch) in enumerate(train_loader):\n",
    "            var_X_batch = Variable(X_batch).float()\n",
    "            var_y_batch = Variable(y_batch)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output = model(var_X_batch)\n",
    "            loss = error(output, var_y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            predicted = torch.max(output.data, 1)[1] \n",
    "            correct += (predicted == var_y_batch).sum()\n",
    "            if batch_idx % 200 == 0:\n",
    "                print('Epoch : {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\t Accuracy:{:.3f}%'.format(\n",
    "                    epoch, batch_idx*len(X_batch), len(train_loader.dataset), 100.*batch_idx / len(train_loader), loss.data, float(correct*100) / float(BATCH_SIZE*(batch_idx+1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loader=test_loader):\n",
    "    correct = 0 \n",
    "    for test_imgs, test_labels in loader:\n",
    "        test_imgs = Variable(test_imgs).float()\n",
    "        \n",
    "        output = model(test_imgs)\n",
    "        predicted = torch.max(output,1)[1]\n",
    "        correct += (predicted == test_labels).sum()\n",
    "    print(\"Test accuracy:{:.3f}% \".format( float(correct) / (len(loader)*BATCH_SIZE)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0 [0/60000 (0%)]\tLoss: 0.392104\t Accuracy:90.625%\n",
      "Epoch : 0 [6400/60000 (11%)]\tLoss: 0.481568\t Accuracy:83.302%\n",
      "Epoch : 0 [12800/60000 (21%)]\tLoss: 0.661964\t Accuracy:82.941%\n",
      "Epoch : 0 [19200/60000 (32%)]\tLoss: 0.290032\t Accuracy:82.992%\n",
      "Epoch : 0 [25600/60000 (43%)]\tLoss: 0.271368\t Accuracy:83.002%\n",
      "Epoch : 0 [32000/60000 (53%)]\tLoss: 0.549211\t Accuracy:82.986%\n",
      "Epoch : 0 [38400/60000 (64%)]\tLoss: 0.501826\t Accuracy:83.157%\n",
      "Epoch : 0 [44800/60000 (75%)]\tLoss: 0.387613\t Accuracy:83.315%\n",
      "Epoch : 0 [51200/60000 (85%)]\tLoss: 0.167846\t Accuracy:83.401%\n",
      "Epoch : 0 [57600/60000 (96%)]\tLoss: 0.148662\t Accuracy:83.487%\n",
      "Epoch : 1 [0/60000 (0%)]\tLoss: 0.448169\t Accuracy:87.500%\n",
      "Epoch : 1 [6400/60000 (11%)]\tLoss: 0.496726\t Accuracy:84.981%\n",
      "Epoch : 1 [12800/60000 (21%)]\tLoss: 0.665119\t Accuracy:84.624%\n",
      "Epoch : 1 [19200/60000 (32%)]\tLoss: 0.241231\t Accuracy:84.900%\n",
      "Epoch : 1 [25600/60000 (43%)]\tLoss: 0.261912\t Accuracy:84.988%\n",
      "Epoch : 1 [32000/60000 (53%)]\tLoss: 0.515492\t Accuracy:84.915%\n",
      "Epoch : 1 [38400/60000 (64%)]\tLoss: 0.413043\t Accuracy:84.960%\n",
      "Epoch : 1 [44800/60000 (75%)]\tLoss: 0.399193\t Accuracy:85.102%\n",
      "Epoch : 1 [51200/60000 (85%)]\tLoss: 0.199164\t Accuracy:85.084%\n",
      "Epoch : 1 [57600/60000 (96%)]\tLoss: 0.146579\t Accuracy:85.102%\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(SEED)\n",
    "fit(mlp, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:0.857% \n"
     ]
    }
   ],
   "source": [
    "evaluate(mlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Уменьшать количество самих весов в этот раз мы не будем, поэтому в качестве размера нейронной сети будет использовать буквально количество памяти, которая она занимает."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_size(model):\n",
    "    torch.save(model.state_dict(), \"/tmp/model.p\")\n",
    "    size=os.path.getsize(\"/tmp/model.p\")\n",
    "    os.remove('/tmp/model.p')\n",
    "    return \"{:.3f} KB\".format(size / 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'870.926 KB'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_size(mlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Полученная сеть весит почти мегабайт.\n",
    "\n",
    "Посмотрим на веса, которые используются внутри нашей сети"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0326,  0.0279,  0.0418,  ..., -0.0028,  0.0241,  0.0297],\n",
       "        [-0.0152,  0.0219, -0.0283,  ..., -0.0007, -0.0119,  0.0365],\n",
       "        [-0.0097, -0.0296,  0.0242,  ..., -0.0385,  0.0172,  0.0285],\n",
       "        ...,\n",
       "        [-0.0203, -0.0002,  0.0249,  ...,  0.0117,  0.0209,  0.0146],\n",
       "        [-0.0187, -0.0464,  0.0138,  ...,  0.0272, -0.0126, -0.0211],\n",
       "        [-0.0037,  0.0156,  0.0021,  ..., -0.0133, -0.0365,  0.0120]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.linear1.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (linear1): Linear(in_features=784, out_features=250, bias=True)\n",
       "  (linear2): Linear(in_features=250, out_features=100, bias=True)\n",
       "  (linear3): Linear(in_features=100, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посчитаем дополнительно, за сколько в среднем она делает предсказания. Для замеров по времени будем использовать один поток, чтобы все сети были в одинаковых условиях."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from contextlib import contextmanager\n",
    "\n",
    "@contextmanager\n",
    "def single_thread():  \n",
    "    num = torch.get_num_threads()\n",
    "    torch.set_num_threads(1)\n",
    "    yield\n",
    "    torch.set_num_threads(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:0.862% \n",
      "Test accuracy:0.862% \n",
      "Test accuracy:0.862% \n",
      "Test accuracy:0.862% \n",
      "Test accuracy:0.862% \n",
      "Test accuracy:0.862% \n",
      "Test accuracy:0.862% \n",
      "Test accuracy:0.862% \n",
      "Test accuracy:0.862% \n",
      "Test accuracy:0.862% \n",
      "Test accuracy:0.862% \n",
      "2.09 s ± 17.4 ms per loop (mean ± std. dev. of 10 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -r10\n",
    "\n",
    "with single_thread():\n",
    "    evaluate(mlp, train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Динамическая квантизация\n",
    "\n",
    "Динамическая квантизация - пожалуй самый простой способ квантизации. \n",
    "\n",
    "Все веса из float мы сразу переводим в int, а вот активации мы пересчитываем на лету во время работы сети. Для каждого примера в нейроне мы аккумулируем взвешенную сумму по-честному во float, подбираем для конкретного получившегося числа лучшие параметры квантизации, квантуем и отправляем дальше по сети.\n",
    "\n",
    "Таким образом, из-за этих автоматических квантований во время работы сети сеть все еще может работать медленно.\n",
    "\n",
    "Попробуем применить динамическую квантизацию к нашей модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "qd_mlp = torch.quantization.quantize_dynamic(\n",
    "    mlp, {nn.Linear}, dtype=torch.qint8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (linear1): DynamicQuantizedLinear(in_features=784, out_features=250, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "  (linear2): DynamicQuantizedLinear(in_features=250, out_features=100, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "  (linear3): DynamicQuantizedLinear(in_features=100, out_features=10, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qd_mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'222.455 KB'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_size(qd_mlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видно, что получилось уменьшить размер сети почти в 4 раза. Посмотрим, что случилось с качеством полученной модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:0.857% \n"
     ]
    }
   ],
   "source": [
    "evaluate(qd_mlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Качество осталось в точности таким же.\n",
    "\n",
    "Заглянем в то, какие теперь веса используются внутри."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  9,   7,  11,  ...,  -1,   6,   8],\n",
       "        [ -4,   6,  -7,  ...,   0,  -3,  10],\n",
       "        [ -3,  -8,   6,  ..., -10,   5,   8],\n",
       "        ...,\n",
       "        [ -5,   0,   7,  ...,   3,   6,   4],\n",
       "        [ -5, -12,   4,  ...,   7,  -3,  -6],\n",
       "        [ -1,   4,   1,  ...,  -4, -10,   3]], dtype=torch.int8)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qd_mlp.linear1.weight().int_repr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0326,  0.0279,  0.0418,  ..., -0.0028,  0.0241,  0.0297],\n",
       "        [-0.0152,  0.0219, -0.0283,  ..., -0.0007, -0.0119,  0.0365],\n",
       "        [-0.0097, -0.0296,  0.0242,  ..., -0.0385,  0.0172,  0.0285],\n",
       "        ...,\n",
       "        [-0.0203, -0.0002,  0.0249,  ...,  0.0117,  0.0209,  0.0146],\n",
       "        [-0.0187, -0.0464,  0.0138,  ...,  0.0272, -0.0126, -0.0211],\n",
       "        [-0.0037,  0.0156,  0.0021,  ..., -0.0133, -0.0365,  0.0120]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.linear1.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Помотрим, насколько быстро получается делать предсказания квантизированной моделью."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:0.862% \n",
      "Test accuracy:0.862% \n",
      "Test accuracy:0.862% \n",
      "Test accuracy:0.862% \n",
      "Test accuracy:0.862% \n",
      "Test accuracy:0.862% \n",
      "Test accuracy:0.862% \n",
      "Test accuracy:0.862% \n",
      "Test accuracy:0.862% \n",
      "Test accuracy:0.862% \n",
      "Test accuracy:0.862% \n",
      "1.92 s ± 11.6 ms per loop (mean ± std. dev. of 10 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -r10\n",
    "\n",
    "with single_thread():\n",
    "    evaluate(qd_mlp, train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На моем компьютере получился прирост примерно в 10-15%. Таким образом динамическая квантизация - достаточно простой прием, который не дает большого проигрышал по качеству, при этом уменьшает размер сети и ускоряет предсказания.\n",
    "\n",
    "Однако это не единственный подход для квантования. \n",
    "\n",
    "### Статическая квантизация\n",
    "\n",
    "Статическая квантизация позволяет сразу все операции перевести в int, без необходимости дополнительно что-то расчитывать в процессе предсказания.\n",
    "\n",
    "Для того, чтобы при этом качество не сильно пострадало, параметры квантования для разных слоем настраиваются по обучающей выборке.\n",
    "\n",
    "Таким образом для того, чтобы статически квантизировать сеть, необходимо вначале подключить к ней модуль подсчета параметров (Observer), который будет для каждого слоя расчитывать необходимые параметры квантования по обучающей выборке. После этого один раз необходимо всю выборку прогнать через сеть, чтобы эти модули смогли подсчитать нужные параметры. После чего можно фиксировать полученные параметры и итоговую квантизированную сеть."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantizedMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(QuantizedMLP, self).__init__()\n",
    "        self.linear1 = nn.Linear(784,250)\n",
    "        self.linear2 = nn.Linear(250,100)\n",
    "        self.linear3 = nn.Linear(100,10)\n",
    "        # Так как теперь квантизация не происходит динамически, необходимо дополнительно \n",
    "        # руками квантовать входные данные и деквантовать ответ\n",
    "        self.quant = QuantStub()\n",
    "        self.dequant = DeQuantStub()\n",
    "    \n",
    "    def forward(self,X):\n",
    "        # Квантуем входные данные\n",
    "        X = self.quant(X)\n",
    "        X = F.relu(self.linear1(X))\n",
    "        X = F.relu(self.linear2(X))\n",
    "        X = self.linear3(X)\n",
    "        # Деквантуем ответ\n",
    "        X = self.dequant(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "ptq_mlp = QuantizedMLP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0 [0/60000 (0%)]\tLoss: 16.807035\t Accuracy:0.000%\n",
      "Epoch : 0 [6400/60000 (11%)]\tLoss: 0.844625\t Accuracy:69.481%\n",
      "Epoch : 0 [12800/60000 (21%)]\tLoss: 0.862578\t Accuracy:73.808%\n",
      "Epoch : 0 [19200/60000 (32%)]\tLoss: 0.460852\t Accuracy:75.619%\n",
      "Epoch : 0 [25600/60000 (43%)]\tLoss: 0.388418\t Accuracy:76.717%\n",
      "Epoch : 0 [32000/60000 (53%)]\tLoss: 0.640597\t Accuracy:77.448%\n",
      "Epoch : 0 [38400/60000 (64%)]\tLoss: 0.462235\t Accuracy:78.203%\n",
      "Epoch : 0 [44800/60000 (75%)]\tLoss: 0.321292\t Accuracy:78.861%\n",
      "Epoch : 0 [51200/60000 (85%)]\tLoss: 0.219215\t Accuracy:79.312%\n",
      "Epoch : 0 [57600/60000 (96%)]\tLoss: 0.159714\t Accuracy:79.671%\n",
      "Epoch : 1 [0/60000 (0%)]\tLoss: 0.339226\t Accuracy:87.500%\n",
      "Epoch : 1 [6400/60000 (11%)]\tLoss: 0.530107\t Accuracy:83.085%\n",
      "Epoch : 1 [12800/60000 (21%)]\tLoss: 0.722787\t Accuracy:83.151%\n",
      "Epoch : 1 [19200/60000 (32%)]\tLoss: 0.257429\t Accuracy:83.335%\n",
      "Epoch : 1 [25600/60000 (43%)]\tLoss: 0.404835\t Accuracy:83.517%\n",
      "Epoch : 1 [32000/60000 (53%)]\tLoss: 0.703874\t Accuracy:83.616%\n",
      "Epoch : 1 [38400/60000 (64%)]\tLoss: 0.485659\t Accuracy:83.787%\n",
      "Epoch : 1 [44800/60000 (75%)]\tLoss: 0.261478\t Accuracy:83.958%\n",
      "Epoch : 1 [51200/60000 (85%)]\tLoss: 0.177726\t Accuracy:84.082%\n",
      "Epoch : 1 [57600/60000 (96%)]\tLoss: 0.119136\t Accuracy:84.066%\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(SEED)\n",
    "fit(ptq_mlp, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:0.848% \n"
     ]
    }
   ],
   "source": [
    "evaluate(ptq_mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Для моделей мы также можем указать конфиг квантования,\n",
    "# где в частности может указать библиотеку для работы с квантованными значениями\n",
    "\n",
    "ptq_mlp.qconfig = torch.quantization.get_default_qconfig('fbgemm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.8/site-packages/torch/ao/quantization/observer.py:220: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "QuantizedMLP(\n",
       "  (linear1): Linear(\n",
       "    in_features=784, out_features=250, bias=True\n",
       "    (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  )\n",
       "  (linear2): Linear(\n",
       "    in_features=250, out_features=100, bias=True\n",
       "    (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  )\n",
       "  (linear3): Linear(\n",
       "    in_features=100, out_features=10, bias=True\n",
       "    (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  )\n",
       "  (quant): QuantStub(\n",
       "    (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  )\n",
       "  (dequant): DeQuantStub()\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Устанавливаем модули подсчета параметров квантования. По умолчанию исползуется HistogramObserver, то есть\n",
    "# модуль, который рассчтывает параметры на основе гистрограммы распределения значнеий для конкретного слоя\n",
    "torch.quantization.prepare(ptq_mlp, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:0.853% \n"
     ]
    }
   ],
   "source": [
    "# Прогоняем всю обучающую выборку через сеть. Для этого просто считаем качество на данных обучающей выборки\n",
    "# Само значение нам не интересно, нам важно, чтобы посчитались параметры\n",
    "evaluate(ptq_mlp, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QuantizedMLP(\n",
       "  (linear1): QuantizedLinear(in_features=784, out_features=250, scale=30.2002010345459, zero_point=96, qscheme=torch.per_channel_affine)\n",
       "  (linear2): QuantizedLinear(in_features=250, out_features=100, scale=3.877804756164551, zero_point=81, qscheme=torch.per_channel_affine)\n",
       "  (linear3): QuantizedLinear(in_features=100, out_features=10, scale=0.7409247159957886, zero_point=62, qscheme=torch.per_channel_affine)\n",
       "  (quant): Quantize(scale=tensor([2.0069]), zero_point=tensor([0]), dtype=torch.quint8)\n",
       "  (dequant): DeQuantize()\n",
       ")"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Фиксируем полученные веса и параметры квантизации\n",
    "torch.quantization.convert(ptq_mlp, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно видеть, какие именно параметры (а именно scale и zero_point) подсчитались для каждого слоя."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'229.916 KB'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_size(ptq_mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  56,   48,  -18,  ...,  -15,   29,   73],\n",
       "        [ -32, -107,   44,  ...,   30,   -7,   13],\n",
       "        [  30,   27,  -18,  ...,  -52,  -27,   -6],\n",
       "        ...,\n",
       "        [ -17,  -22,  -26,  ...,   -2,    7,  -23],\n",
       "        [  36,   80,   21,  ..., -115,  -52,   39],\n",
       "        [ -65,   12,   60,  ...,  -82,   16,  -88]], dtype=torch.int8)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ptq_mlp.linear1.weight().int_repr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:0.846% \n"
     ]
    }
   ],
   "source": [
    "evaluate(ptq_mlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видно, что модель все еще хорошо сжалась, при этом качество немного упало. \n",
    "\n",
    "Посмотрим, что по скорости выполнения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:0.849% \n",
      "Test accuracy:0.849% \n",
      "Test accuracy:0.849% \n",
      "Test accuracy:0.849% \n",
      "Test accuracy:0.849% \n",
      "Test accuracy:0.849% \n",
      "Test accuracy:0.849% \n",
      "Test accuracy:0.849% \n",
      "Test accuracy:0.849% \n",
      "Test accuracy:0.849% \n",
      "Test accuracy:0.849% \n",
      "1.97 s ± 20.4 ms per loop (mean ± std. dev. of 10 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -r10\n",
    "\n",
    "with single_thread():\n",
    "    evaluate(ptq_mlp, train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видно, что с текущей моделью статическая квантизация работает не так хорошо - качество немного просело, а значительного прироста по времени не наблюдается.\n",
    "\n",
    "Вместо простых схем квантизации, когда уже обученная модель квантуется, есть и более продвинутые схемы.\n",
    "\n",
    "### Квантизация в процессе обучения\n",
    "\n",
    "Этот метод заключается в том, что квантование происходит на каждом шаге градиентного спуска. Теоретически это должно делать более аккуратными относительно квантованных параметров и тем самым получать качество лучше."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_mlp = QuantizedMLP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantized_fit(model, train_loader, epoch_number=5):\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "    \n",
    "    # Ничем особенным процесс обучения не отличается\n",
    "    # Добавляем конфигурацию, после чего подготавливаем модель для обучения с квантованием\n",
    "    # Модель внутри себя автоматически будет обновлять веса с учетом квантования\n",
    "    model.qconfig = torch.quantization.get_default_qat_qconfig('fbgemm')\n",
    "    torch.quantization.prepare_qat(model, inplace=True)\n",
    "    \n",
    "    error = nn.CrossEntropyLoss()\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(epoch_number):\n",
    "        correct = 0\n",
    "        \n",
    "        for batch_idx, (X_batch, y_batch) in enumerate(train_loader):\n",
    "            var_X_batch = Variable(X_batch).float()\n",
    "            var_y_batch = Variable(y_batch)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output = model(var_X_batch)\n",
    "            loss = error(output, var_y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            predicted = torch.max(output.data, 1)[1] \n",
    "            correct += (predicted == var_y_batch).sum()\n",
    "            if batch_idx % 200 == 0:\n",
    "                print('Epoch : {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\t Accuracy:{:.3f}%'.format(\n",
    "                    epoch, batch_idx*len(X_batch), len(train_loader.dataset), 100.*batch_idx / len(train_loader), loss.data, float(correct*100) / float(BATCH_SIZE*(batch_idx+1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0 [0/60000 (0%)]\tLoss: 17.979427\t Accuracy:12.500%\n",
      "Epoch : 0 [6400/60000 (11%)]\tLoss: 3.951177\t Accuracy:38.044%\n",
      "Epoch : 0 [12800/60000 (21%)]\tLoss: 1.418032\t Accuracy:46.852%\n",
      "Epoch : 0 [19200/60000 (32%)]\tLoss: 0.579798\t Accuracy:55.969%\n",
      "Epoch : 0 [25600/60000 (43%)]\tLoss: 0.258367\t Accuracy:61.669%\n",
      "Epoch : 0 [32000/60000 (53%)]\tLoss: 0.681209\t Accuracy:65.241%\n",
      "Epoch : 0 [38400/60000 (64%)]\tLoss: 0.560007\t Accuracy:67.988%\n",
      "Epoch : 0 [44800/60000 (75%)]\tLoss: 0.369635\t Accuracy:69.932%\n",
      "Epoch : 0 [51200/60000 (85%)]\tLoss: 0.363466\t Accuracy:71.535%\n",
      "Epoch : 0 [57600/60000 (96%)]\tLoss: 0.253792\t Accuracy:72.710%\n",
      "Epoch : 1 [0/60000 (0%)]\tLoss: 0.493794\t Accuracy:84.375%\n",
      "Epoch : 1 [6400/60000 (11%)]\tLoss: 0.547739\t Accuracy:82.929%\n",
      "Epoch : 1 [12800/60000 (21%)]\tLoss: 0.731098\t Accuracy:83.003%\n",
      "Epoch : 1 [19200/60000 (32%)]\tLoss: 0.274485\t Accuracy:83.288%\n",
      "Epoch : 1 [25600/60000 (43%)]\tLoss: 0.264762\t Accuracy:83.439%\n",
      "Epoch : 1 [32000/60000 (53%)]\tLoss: 0.731743\t Accuracy:83.357%\n",
      "Epoch : 1 [38400/60000 (64%)]\tLoss: 0.464321\t Accuracy:83.407%\n",
      "Epoch : 1 [44800/60000 (75%)]\tLoss: 0.355192\t Accuracy:83.556%\n",
      "Epoch : 1 [51200/60000 (85%)]\tLoss: 0.227366\t Accuracy:83.696%\n",
      "Epoch : 1 [57600/60000 (96%)]\tLoss: 0.148764\t Accuracy:83.721%\n",
      "Epoch : 2 [0/60000 (0%)]\tLoss: 0.589955\t Accuracy:78.125%\n",
      "Epoch : 2 [6400/60000 (11%)]\tLoss: 0.432732\t Accuracy:84.095%\n",
      "Epoch : 2 [12800/60000 (21%)]\tLoss: 0.644744\t Accuracy:84.461%\n",
      "Epoch : 2 [19200/60000 (32%)]\tLoss: 0.222477\t Accuracy:84.708%\n",
      "Epoch : 2 [25600/60000 (43%)]\tLoss: 0.253612\t Accuracy:84.714%\n",
      "Epoch : 2 [32000/60000 (53%)]\tLoss: 0.541813\t Accuracy:84.584%\n",
      "Epoch : 2 [38400/60000 (64%)]\tLoss: 0.353012\t Accuracy:84.690%\n",
      "Epoch : 2 [44800/60000 (75%)]\tLoss: 0.249103\t Accuracy:84.779%\n",
      "Epoch : 2 [51200/60000 (85%)]\tLoss: 0.312722\t Accuracy:84.865%\n",
      "Epoch : 2 [57600/60000 (96%)]\tLoss: 0.136892\t Accuracy:84.786%\n",
      "Epoch : 3 [0/60000 (0%)]\tLoss: 0.442052\t Accuracy:81.250%\n",
      "Epoch : 3 [6400/60000 (11%)]\tLoss: 0.489906\t Accuracy:85.619%\n",
      "Epoch : 3 [12800/60000 (21%)]\tLoss: 0.646818\t Accuracy:85.419%\n",
      "Epoch : 3 [19200/60000 (32%)]\tLoss: 0.209006\t Accuracy:85.462%\n",
      "Epoch : 3 [25600/60000 (43%)]\tLoss: 0.246734\t Accuracy:85.479%\n",
      "Epoch : 3 [32000/60000 (53%)]\tLoss: 0.441770\t Accuracy:85.477%\n",
      "Epoch : 3 [38400/60000 (64%)]\tLoss: 0.443472\t Accuracy:85.481%\n",
      "Epoch : 3 [44800/60000 (75%)]\tLoss: 0.269944\t Accuracy:85.548%\n",
      "Epoch : 3 [51200/60000 (85%)]\tLoss: 0.220043\t Accuracy:85.597%\n",
      "Epoch : 3 [57600/60000 (96%)]\tLoss: 0.158332\t Accuracy:85.614%\n",
      "Epoch : 4 [0/60000 (0%)]\tLoss: 0.504573\t Accuracy:78.125%\n",
      "Epoch : 4 [6400/60000 (11%)]\tLoss: 0.529612\t Accuracy:86.210%\n",
      "Epoch : 4 [12800/60000 (21%)]\tLoss: 0.709628\t Accuracy:85.840%\n",
      "Epoch : 4 [19200/60000 (32%)]\tLoss: 0.348098\t Accuracy:85.935%\n",
      "Epoch : 4 [25600/60000 (43%)]\tLoss: 0.286055\t Accuracy:85.869%\n",
      "Epoch : 4 [32000/60000 (53%)]\tLoss: 0.482412\t Accuracy:85.752%\n",
      "Epoch : 4 [38400/60000 (64%)]\tLoss: 0.359898\t Accuracy:85.772%\n",
      "Epoch : 4 [44800/60000 (75%)]\tLoss: 0.252874\t Accuracy:85.832%\n",
      "Epoch : 4 [51200/60000 (85%)]\tLoss: 0.139832\t Accuracy:85.882%\n",
      "Epoch : 4 [57600/60000 (96%)]\tLoss: 0.133855\t Accuracy:85.822%\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(SEED)\n",
    "quantized_fit(qa_mlp, train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно заметить, что обучение идет немного дольше чем обычно"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QuantizedMLP(\n",
       "  (linear1): QuantizedLinear(in_features=784, out_features=250, scale=43.581661224365234, zero_point=90, qscheme=torch.per_channel_affine)\n",
       "  (linear2): QuantizedLinear(in_features=250, out_features=100, scale=7.072278022766113, zero_point=84, qscheme=torch.per_channel_affine)\n",
       "  (linear3): QuantizedLinear(in_features=100, out_features=10, scale=1.3312269449234009, zero_point=92, qscheme=torch.per_channel_affine)\n",
       "  (quant): Quantize(scale=tensor([2.0079]), zero_point=tensor([0]), dtype=torch.quint8)\n",
       "  (dequant): DeQuantize()\n",
       ")"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# После обучения с квантованием, фиксируем квантованные веса и параметры\n",
    "quantized_model = torch.quantization.convert(qa_mlp, inplace=False)\n",
    "quantized_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:0.847% \n"
     ]
    }
   ],
   "source": [
    "evaluate(quantized_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На моем компьютере качество получилось даже чуть выше, чем у оригинальной модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'229.746 KB'"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_size(quantized_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  -9,   13,  -17,  ...,  -34,  -34,  -65],\n",
       "        [   9,    4,  -45,  ...,  -13,  -24,   65],\n",
       "        [ -15,  -88,   29,  ...,  -72,   52,   53],\n",
       "        ...,\n",
       "        [  74,   50,  106,  ...,   75,  101,   31],\n",
       "        [ -58,  -68,   57,  ..., -106,  -79,    8],\n",
       "        [ -15,   58,  -44,  ...,  115,   37,  -63]], dtype=torch.int8)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantized_model.linear1.weight().int_repr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:0.876% \n",
      "Test accuracy:0.876% \n",
      "Test accuracy:0.876% \n",
      "Test accuracy:0.876% \n",
      "Test accuracy:0.876% \n",
      "Test accuracy:0.876% \n",
      "Test accuracy:0.876% \n",
      "Test accuracy:0.876% \n",
      "Test accuracy:0.876% \n",
      "Test accuracy:0.876% \n",
      "Test accuracy:0.876% \n",
      "1.3 s ± 6.6 ms per loop (mean ± std. dev. of 10 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -r10\n",
    "\n",
    "with single_thread():\n",
    "    evaluate(quantized_model, train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По итогу этих экспериментов можно сказать следующее - универсального метода квантизации не существует, в каждом конкретном случае нужно искать свой подход.\n",
    "\n",
    "Однако все схемы показывают стабильное уменьшение размера модели при небольшом изменении метрики качества и ускорении расчета предсказаний."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть 2. LLMs?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![a](https://static.wixstatic.com/media/b45b25_41a7b8ed92ec4ff2a60b2dc81a5e3670~mv2.png/v1/fill/w_950,h_768,al_c,q_90,enc_auto/b45b25_41a7b8ed92ec4ff2a60b2dc81a5e3670~mv2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPTQ\n",
    "\n",
    "\n",
    "Читаем статью: https://arxiv.org/pdf/2210.17323.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NF4 и QLORA\n",
    "\n",
    "Читаем статью: https://medium.com/@dillipprasad60/qlora-explained-a-deep-dive-into-parametric-efficient-fine-tuning-in-large-language-models-llms-c1a4794b1766"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GGML\n",
    "\n",
    "Читаем про квантизации:\n",
    "https://github.com/ggerganov/llama.cpp/pull/1684"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'ggml'...\n",
      "remote: Enumerating objects: 7008, done.\u001b[K\n",
      "remote: Counting objects: 100% (3355/3355), done.\u001b[K\n",
      "remote: Compressing objects: 100% (525/525), done.\u001b[K\n",
      "remote: Total 7008 (delta 2936), reused 3033 (delta 2779), pack-reused 3653\u001b[K\n",
      "Receiving objects: 100% (7008/7008), 8.05 MiB | 18.15 MiB/s, done.\n",
      "Resolving deltas: 100% (4478/4478), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/ggerganov/ggml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘build’: File exists\r\n"
     ]
    }
   ],
   "source": [
    "!cd ggml && mkdir build && cd build && cmake .. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 18%] Built target common\n",
      "[ 63%] Built target ggml\n",
      "[ 81%] Built target common-ggml\n",
      "[100%] Built target gpt-2-backend\n"
     ]
    }
   ],
   "source": [
    "!cd ggml/build && make -j4 gpt-2-backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading model 117M ...\n",
      "models/gpt-2-117M/c 100%[===================>]      77  --.-KB/s    in 0s      \n",
      "models/gpt-2-117M/e 100%[===================>]   1018K   786KB/s    in 1.3s    \n",
      "models/gpt-2-117M/h 100%[===================>]      90  --.-KB/s    in 0s      \n",
      "f-00001              86%[================>   ] 408.43M  2.39MB/s    eta 25s    ^C\n"
     ]
    }
   ],
   "source": [
    "!cd ggml/build/ && ../examples/gpt-2/download-model.sh 117M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'models'...\n",
      "remote: Enumerating objects: 61, done.\u001b[K\n",
      "remote: Total 61 (delta 0), reused 0 (delta 0), pack-reused 61\u001b[K\n",
      "Unpacking objects: 100% (61/61), 566.26 KiB | 3.45 MiB/s, done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://huggingface.co/cerebras/Cerebras-GPT-111M models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /home/ubuntu/.local/lib/python3.8/site-packages (4.39.0)\n",
      "Collecting accelerate\n",
      "  Downloading accelerate-0.28.0-py3-none-any.whl (290 kB)\n",
      "\u001b[K     |████████████████████████████████| 290 kB 2.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /home/ubuntu/.local/lib/python3.8/site-packages (from transformers) (24.0)\n",
      "Requirement already satisfied: requests in /home/ubuntu/.local/lib/python3.8/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: filelock in /home/ubuntu/.local/lib/python3.8/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ubuntu/.local/lib/python3.8/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ubuntu/.local/lib/python3.8/site-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /home/ubuntu/.local/lib/python3.8/site-packages (from transformers) (0.21.4)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /home/ubuntu/.local/lib/python3.8/site-packages (from transformers) (0.15.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.66.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ubuntu/.local/lib/python3.8/site-packages (from transformers) (1.24.4)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/ubuntu/.local/lib/python3.8/site-packages (from transformers) (0.4.2)\n",
      "Requirement already satisfied: psutil in /home/ubuntu/.local/lib/python3.8/site-packages (from accelerate) (5.9.8)\n",
      "Requirement already satisfied: torch>=1.10.0 in /home/ubuntu/.local/lib/python3.8/site-packages (from accelerate) (2.2.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ubuntu/.local/lib/python3.8/site-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ubuntu/.local/lib/python3.8/site-packages (from requests->transformers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ubuntu/.local/lib/python3.8/site-packages (from requests->transformers) (2024.2.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ubuntu/.local/lib/python3.8/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ubuntu/.local/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.10.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/ubuntu/.local/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2024.2.0)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/ubuntu/.local/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\n",
      "Requirement already satisfied: networkx in /home/ubuntu/.local/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (3.1)\n",
      "Requirement already satisfied: jinja2 in /home/ubuntu/.local/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (3.1.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/ubuntu/.local/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (2.19.3)\n",
      "Requirement already satisfied: triton==2.2.0; platform_system == \"Linux\" and platform_machine == \"x86_64\" and python_version < \"3.12\" in /home/ubuntu/.local/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (2.2.0)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/ubuntu/.local/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/ubuntu/.local/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/ubuntu/.local/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/ubuntu/.local/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/ubuntu/.local/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/ubuntu/.local/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: sympy in /home/ubuntu/.local/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (1.12)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/ubuntu/.local/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/ubuntu/.local/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/ubuntu/.local/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ubuntu/.local/lib/python3.8/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/ubuntu/.local/lib/python3.8/site-packages (from nvidia-cusparse-cu12==12.1.0.106; platform_system == \"Linux\" and platform_machine == \"x86_64\"->torch>=1.10.0->accelerate) (12.4.99)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/ubuntu/.local/lib/python3.8/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
      "Installing collected packages: accelerate\n",
      "\u001b[33m  WARNING: The scripts accelerate, accelerate-config, accelerate-estimate-memory and accelerate-launch are installed in '/home/ubuntu/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "Successfully installed accelerate-0.28.0\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model_type': 'gpt2', 'attn_pdrop': 0.0, 'scale_attn_weights': True, 'resid_pdrop': 0.0, 'n_inner': 3072, 'n_embd': 768, 'layer_norm_epsilon': 1e-05, 'n_positions': 2048, 'activation_function': 'gelu', 'n_head': 12, 'n_layer': 10, 'tie_word_embeddings': True, 'vocab_size': 50257, 'embd_pdrop': 0.0}\n",
      "Processing variable: transformer.wte.weight with shape:  (50257, 768)\n",
      "  Converting to float16\n",
      "Processing variable: transformer.wpe.weight with shape:  (2048, 768)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.h.0.ln_1.weight with shape:  (768,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.h.0.ln_1.bias with shape:  (768,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.h.0.attn.c_attn.weight with shape:  (768, 2304)\n",
      "  Converting to float16\n",
      "  Transposing\n",
      "Processing variable: transformer.h.0.attn.c_attn.bias with shape:  (2304,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.h.0.attn.c_proj.weight with shape:  (768, 768)\n",
      "  Converting to float16\n",
      "  Transposing\n",
      "Processing variable: transformer.h.0.attn.c_proj.bias with shape:  (768,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.h.0.ln_2.weight with shape:  (768,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.h.0.ln_2.bias with shape:  (768,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.h.0.mlp.c_fc.weight with shape:  (768, 3072)\n",
      "  Converting to float16\n",
      "  Transposing\n",
      "Processing variable: transformer.h.0.mlp.c_fc.bias with shape:  (3072,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.h.0.mlp.c_proj.weight with shape:  (3072, 768)\n",
      "  Converting to float16\n",
      "  Transposing\n",
      "Processing variable: transformer.h.0.mlp.c_proj.bias with shape:  (768,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.h.1.ln_1.weight with shape:  (768,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.h.1.ln_1.bias with shape:  (768,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.h.1.attn.c_attn.weight with shape:  (768, 2304)\n",
      "  Converting to float16\n",
      "  Transposing\n",
      "Processing variable: transformer.h.1.attn.c_attn.bias with shape:  (2304,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.h.1.attn.c_proj.weight with shape:  (768, 768)\n",
      "  Converting to float16\n",
      "  Transposing\n",
      "Processing variable: transformer.h.1.attn.c_proj.bias with shape:  (768,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.h.1.ln_2.weight with shape:  (768,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.h.1.ln_2.bias with shape:  (768,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.h.1.mlp.c_fc.weight with shape:  (768, 3072)\n",
      "  Converting to float16\n",
      "  Transposing\n",
      "Processing variable: transformer.h.1.mlp.c_fc.bias with shape:  (3072,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.h.1.mlp.c_proj.weight with shape:  (3072, 768)\n",
      "  Converting to float16\n",
      "  Transposing\n",
      "Processing variable: transformer.h.1.mlp.c_proj.bias with shape:  (768,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.h.2.ln_1.weight with shape:  (768,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.h.2.ln_1.bias with shape:  (768,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.h.2.attn.c_attn.weight with shape:  (768, 2304)\n",
      "  Converting to float16\n",
      "  Transposing\n",
      "Processing variable: transformer.h.2.attn.c_attn.bias with shape:  (2304,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.h.2.attn.c_proj.weight with shape:  (768, 768)\n",
      "  Converting to float16\n",
      "  Transposing\n",
      "Processing variable: transformer.h.2.attn.c_proj.bias with shape:  (768,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.h.2.ln_2.weight with shape:  (768,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.h.2.ln_2.bias with shape:  (768,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.h.2.mlp.c_fc.weight with shape:  (768, 3072)\n",
      "  Converting to float16\n",
      "  Transposing\n",
      "Processing variable: transformer.h.2.mlp.c_fc.bias with shape:  (3072,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.h.2.mlp.c_proj.weight with shape:  (3072, 768)\n",
      "  Converting to float16\n",
      "  Transposing\n",
      "Processing variable: transformer.h.2.mlp.c_proj.bias with shape:  (768,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.h.3.ln_1.weight with shape:  (768,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.h.3.ln_1.bias with shape:  (768,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.h.3.attn.c_attn.weight with shape:  (768, 2304)\n",
      "  Converting to float16\n",
      "  Transposing\n",
      "Processing variable: transformer.h.3.attn.c_attn.bias with shape:  (2304,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.h.3.attn.c_proj.weight with shape:  (768, 768)\n",
      "  Converting to float16\n",
      "  Transposing\n",
      "Processing variable: transformer.h.3.attn.c_proj.bias with shape:  (768,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.h.3.ln_2.weight with shape:  (768,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.h.3.ln_2.bias with shape:  (768,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.h.3.mlp.c_fc.weight with shape:  (768, 3072)\n",
      "  Converting to float16\n",
      "  Transposing\n",
      "Processing variable: transformer.h.3.mlp.c_fc.bias with shape:  (3072,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.h.3.mlp.c_proj.weight with shape:  (3072, 768)\n",
      "  Converting to float16\n",
      "  Transposing\n",
      "Processing variable: transformer.h.3.mlp.c_proj.bias with shape:  (768,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.h.4.ln_1.weight with shape:  (768,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.h.4.ln_1.bias with shape:  (768,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.h.4.attn.c_attn.weight with shape:  (768, 2304)\n",
      "  Converting to float16\n",
      "  Transposing\n",
      "Processing variable: transformer.h.4.attn.c_attn.bias with shape:  (2304,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.h.4.attn.c_proj.weight with shape:  (768, 768)\n",
      "  Converting to float16\n",
      "  Transposing\n",
      "Processing variable: transformer.h.4.attn.c_proj.bias with shape:  (768,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.h.4.ln_2.weight with shape:  (768,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.h.4.ln_2.bias with shape:  (768,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.h.4.mlp.c_fc.weight with shape:  (768, 3072)\n",
      "  Converting to float16\n",
      "  Transposing\n",
      "Processing variable: transformer.h.4.mlp.c_fc.bias with shape:  (3072,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.h.4.mlp.c_proj.weight with shape:  (3072, 768)\n",
      "  Converting to float16\n",
      "  Transposing\n",
      "Processing variable: transformer.h.4.mlp.c_proj.bias with shape:  (768,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.h.5.ln_1.weight with shape:  (768,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.h.5.ln_1.bias with shape:  (768,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.h.5.attn.c_attn.weight with shape:  (768, 2304)\n",
      "  Converting to float16\n",
      "  Transposing\n",
      "Processing variable: transformer.h.5.attn.c_attn.bias with shape:  (2304,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.h.5.attn.c_proj.weight with shape:  (768, 768)\n",
      "  Converting to float16\n",
      "  Transposing\n",
      "Processing variable: transformer.h.5.attn.c_proj.bias with shape:  (768,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.h.5.ln_2.weight with shape:  (768,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.h.5.ln_2.bias with shape:  (768,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.h.5.mlp.c_fc.weight with shape:  (768, 3072)\n",
      "  Converting to float16\n",
      "  Transposing\n",
      "Processing variable: transformer.h.5.mlp.c_fc.bias with shape:  (3072,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.h.5.mlp.c_proj.weight with shape:  (3072, 768)\n",
      "  Converting to float16\n",
      "  Transposing\n",
      "Processing variable: transformer.h.5.mlp.c_proj.bias with shape:  (768,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.h.6.ln_1.weight with shape:  (768,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.h.6.ln_1.bias with shape:  (768,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.h.6.attn.c_attn.weight with shape:  (768, 2304)\n",
      "  Converting to float16\n",
      "  Transposing\n",
      "Processing variable: transformer.h.6.attn.c_attn.bias with shape:  (2304,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.h.6.attn.c_proj.weight with shape:  (768, 768)\n",
      "  Converting to float16\n",
      "  Transposing\n",
      "Processing variable: transformer.h.6.attn.c_proj.bias with shape:  (768,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.h.6.ln_2.weight with shape:  (768,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.h.6.ln_2.bias with shape:  (768,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.h.6.mlp.c_fc.weight with shape:  (768, 3072)\n",
      "  Converting to float16\n",
      "  Transposing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing variable: transformer.h.6.mlp.c_fc.bias with shape:  (3072,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.h.6.mlp.c_proj.weight with shape:  (3072, 768)\n",
      "  Converting to float16\n",
      "  Transposing\n",
      "Processing variable: transformer.h.6.mlp.c_proj.bias with shape:  (768,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.h.7.ln_1.weight with shape:  (768,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.h.7.ln_1.bias with shape:  (768,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.h.7.attn.c_attn.weight with shape:  (768, 2304)\n",
      "  Converting to float16\n",
      "  Transposing\n",
      "Processing variable: transformer.h.7.attn.c_attn.bias with shape:  (2304,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.h.7.attn.c_proj.weight with shape:  (768, 768)\n",
      "  Converting to float16\n",
      "  Transposing\n",
      "Processing variable: transformer.h.7.attn.c_proj.bias with shape:  (768,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.h.7.ln_2.weight with shape:  (768,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.h.7.ln_2.bias with shape:  (768,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.h.7.mlp.c_fc.weight with shape:  (768, 3072)\n",
      "  Converting to float16\n",
      "  Transposing\n",
      "Processing variable: transformer.h.7.mlp.c_fc.bias with shape:  (3072,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.h.7.mlp.c_proj.weight with shape:  (3072, 768)\n",
      "  Converting to float16\n",
      "  Transposing\n",
      "Processing variable: transformer.h.7.mlp.c_proj.bias with shape:  (768,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.h.8.ln_1.weight with shape:  (768,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.h.8.ln_1.bias with shape:  (768,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.h.8.attn.c_attn.weight with shape:  (768, 2304)\n",
      "  Converting to float16\n",
      "  Transposing\n",
      "Processing variable: transformer.h.8.attn.c_attn.bias with shape:  (2304,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.h.8.attn.c_proj.weight with shape:  (768, 768)\n",
      "  Converting to float16\n",
      "  Transposing\n",
      "Processing variable: transformer.h.8.attn.c_proj.bias with shape:  (768,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.h.8.ln_2.weight with shape:  (768,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.h.8.ln_2.bias with shape:  (768,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.h.8.mlp.c_fc.weight with shape:  (768, 3072)\n",
      "  Converting to float16\n",
      "  Transposing\n",
      "Processing variable: transformer.h.8.mlp.c_fc.bias with shape:  (3072,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.h.8.mlp.c_proj.weight with shape:  (3072, 768)\n",
      "  Converting to float16\n",
      "  Transposing\n",
      "Processing variable: transformer.h.8.mlp.c_proj.bias with shape:  (768,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.h.9.ln_1.weight with shape:  (768,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.h.9.ln_1.bias with shape:  (768,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.h.9.attn.c_attn.weight with shape:  (768, 2304)\n",
      "  Converting to float16\n",
      "  Transposing\n",
      "Processing variable: transformer.h.9.attn.c_attn.bias with shape:  (2304,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.h.9.attn.c_proj.weight with shape:  (768, 768)\n",
      "  Converting to float16\n",
      "  Transposing\n",
      "Processing variable: transformer.h.9.attn.c_proj.bias with shape:  (768,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.h.9.ln_2.weight with shape:  (768,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.h.9.ln_2.bias with shape:  (768,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.h.9.mlp.c_fc.weight with shape:  (768, 3072)\n",
      "  Converting to float16\n",
      "  Transposing\n",
      "Processing variable: transformer.h.9.mlp.c_fc.bias with shape:  (3072,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.h.9.mlp.c_proj.weight with shape:  (3072, 768)\n",
      "  Converting to float16\n",
      "  Transposing\n",
      "Processing variable: transformer.h.9.mlp.c_proj.bias with shape:  (768,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.ln_f.weight with shape:  (768,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.ln_f.bias with shape:  (768,)\n",
      "  Converting to float32\n",
      "Processing variable: lm_head.weight with shape:  (50257, 768)\n",
      "  Converting to float16\n",
      "Done. Output file: models//ggml-model-f16.bin\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python3 ggml/examples/gpt-2/convert-cerebras-to-ggml.py models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config.json\t    merges.txt\t       README.md\r\n",
      "ggml-model-f16.bin  pytorch_model.bin  vocab.json\r\n"
     ]
    }
   ],
   "source": [
    "!ls models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: seed = 1711012871\n",
      "gpt2_model_load: loading model from 'models//ggml-model-f16.bin'\n",
      "gpt2_model_load: n_vocab = 50257\n",
      "gpt2_model_load: n_ctx   = 2048\n",
      "gpt2_model_load: n_embd  = 768\n",
      "gpt2_model_load: n_head  = 12\n",
      "gpt2_model_load: n_layer = 10\n",
      "gpt2_model_load: ftype   = 1\n",
      "gpt2_model_load: qntvr   = 0\n",
      "gpt2_model_load: using CPU backend\n",
      "gpt2_model_load: ggml tensor size    = 368 bytes\n",
      "gpt2_model_load: backend buffer size = 288.62 MB\n",
      "gpt2_model_load: memory size =   120.00 MB, n_mem = 20480\n",
      "gpt2_model_load: model size  =   288.62 MB\n",
      "extract_tests_from_file : No test file found.\n",
      "test_gpt_tokenizer : 0 tests failed out of 0 tests.\n",
      "main: compute buffer size: 6.87 MB\n",
      "main: prompt: 'I love maching learning and'\n",
      "main: number of tokens in prompt = 6, first 8 tokens: 40 1842 3235 278 4673 290 \n",
      "\n",
      "I love maching learning and have been taught. It is really amazing how many different people you have, and it is absolutely worth trying, to be a student in a very difficult situation.\n",
      "\n",
      "I like to go through the many things you learn on the internet. The internet is so easy and accessible. So, I just don't know what you're thinking, and I want to learn on that and I'm really just trying to teach you the basics, and you can do that for me.\n",
      "\n",
      "You can learn how to use and use different techniques for different types of things. You can also learn how to use different types of techniques, including one type of teaching tool, one type of teaching tool, one type of teaching tool, and a variety of different teaching tools. The different techniques used to teach different kinds of things are really useful.\n",
      "\n",
      "I love what you do, and I want to learn on that. I really just love how you teach, and that is really nice.\n",
      "\n",
      "I love\n",
      "\n",
      "main:     load time =   209.03 ms\n",
      "main:   sample time =    52.65 ms\n",
      "main:  predict time =  4079.75 ms / 19.90 ms per token\n",
      "main:    total time =  4349.50 ms\n"
     ]
    }
   ],
   "source": [
    "!ggml/build//bin/gpt-2-backend -m models//ggml-model-f16.bin -p \"I love maching learning and\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Явно для счастья не хватает квантизации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 18%] Built target common\n",
      "[ 63%] Built target ggml\n",
      "[ 81%] Built target common-ggml\n",
      "[100%] Built target gpt-2-quantize\n"
     ]
    }
   ],
   "source": [
    "!cd ggml/build && make -j4 gpt-2-quantize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt2_model_quantize: loading model from 'models/ggml-model-f16.bin'\n",
      "gpt2_model_quantize: n_vocab     = 50257\n",
      "gpt2_model_quantize: n_ctx       = 2048\n",
      "gpt2_model_quantize: n_embd      = 768\n",
      "gpt2_model_quantize: n_head      = 12\n",
      "gpt2_model_quantize: n_layer     = 10\n",
      "gpt2_model_quantize: ftype (src) = 1\n",
      "gpt2_model_quantize: qntvr (src) = 0\n",
      "gpt2_model_quantize: ftype (dst) = 2003\n",
      "gpt2_model_quantize: qntvr (dst) = 2\n",
      "                                                       model/wte - [  768, 50257,     1], type =    f16 size =   147.24 MB ->    23.01 MB\n",
      "                                                       model/wpe - [  768,  2048,     1], type =    f32 size =    6.000 MB\n",
      "                                                 model/h0/ln_1/g - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
      "                                                 model/h0/ln_1/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
      "                                          model/h0/attn/c_attn/w - [  768,  2304,     1], type =    f16 size =     6.75 MB ->     1.05 MB\n",
      "                                          model/h0/attn/c_attn/b - [ 2304,     1,     1], type =    f32 size =    0.009 MB\n",
      "                                          model/h0/attn/c_proj/w - [  768,   768,     1], type =    f16 size =     2.25 MB ->     0.35 MB\n",
      "                                          model/h0/attn/c_proj/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
      "                                                 model/h0/ln_2/g - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
      "                                                 model/h0/ln_2/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
      "                                             model/h0/mlp/c_fc/w - [  768,  3072,     1], type =    f16 size =     9.00 MB ->     1.41 MB\n",
      "                                             model/h0/mlp/c_fc/b - [ 3072,     1,     1], type =    f32 size =    0.012 MB\n",
      "                                           model/h0/mlp/c_proj/w - [ 3072,   768,     1], type =    f16 size =     9.00 MB ->     1.41 MB\n",
      "                                           model/h0/mlp/c_proj/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
      "                                                 model/h1/ln_1/g - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
      "                                                 model/h1/ln_1/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
      "                                          model/h1/attn/c_attn/w - [  768,  2304,     1], type =    f16 size =     6.75 MB ->     1.05 MB\n",
      "                                          model/h1/attn/c_attn/b - [ 2304,     1,     1], type =    f32 size =    0.009 MB\n",
      "                                          model/h1/attn/c_proj/w - [  768,   768,     1], type =    f16 size =     2.25 MB ->     0.35 MB\n",
      "                                          model/h1/attn/c_proj/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
      "                                                 model/h1/ln_2/g - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
      "                                                 model/h1/ln_2/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
      "                                             model/h1/mlp/c_fc/w - [  768,  3072,     1], type =    f16 size =     9.00 MB ->     1.41 MB\n",
      "                                             model/h1/mlp/c_fc/b - [ 3072,     1,     1], type =    f32 size =    0.012 MB\n",
      "                                           model/h1/mlp/c_proj/w - [ 3072,   768,     1], type =    f16 size =     9.00 MB ->     1.41 MB\n",
      "                                           model/h1/mlp/c_proj/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
      "                                                 model/h2/ln_1/g - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
      "                                                 model/h2/ln_1/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
      "                                          model/h2/attn/c_attn/w - [  768,  2304,     1], type =    f16 size =     6.75 MB ->     1.05 MB\n",
      "                                          model/h2/attn/c_attn/b - [ 2304,     1,     1], type =    f32 size =    0.009 MB\n",
      "                                          model/h2/attn/c_proj/w - [  768,   768,     1], type =    f16 size =     2.25 MB ->     0.35 MB\n",
      "                                          model/h2/attn/c_proj/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
      "                                                 model/h2/ln_2/g - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
      "                                                 model/h2/ln_2/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
      "                                             model/h2/mlp/c_fc/w - [  768,  3072,     1], type =    f16 size =     9.00 MB ->     1.41 MB\n",
      "                                             model/h2/mlp/c_fc/b - [ 3072,     1,     1], type =    f32 size =    0.012 MB\n",
      "                                           model/h2/mlp/c_proj/w - [ 3072,   768,     1], type =    f16 size =     9.00 MB ->     1.41 MB\n",
      "                                           model/h2/mlp/c_proj/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
      "                                                 model/h3/ln_1/g - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
      "                                                 model/h3/ln_1/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
      "                                          model/h3/attn/c_attn/w - [  768,  2304,     1], type =    f16 size =     6.75 MB ->     1.05 MB\n",
      "                                          model/h3/attn/c_attn/b - [ 2304,     1,     1], type =    f32 size =    0.009 MB\n",
      "                                          model/h3/attn/c_proj/w - [  768,   768,     1], type =    f16 size =     2.25 MB ->     0.35 MB\n",
      "                                          model/h3/attn/c_proj/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
      "                                                 model/h3/ln_2/g - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
      "                                                 model/h3/ln_2/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
      "                                             model/h3/mlp/c_fc/w - [  768,  3072,     1], type =    f16 size =     9.00 MB ->     1.41 MB\n",
      "                                             model/h3/mlp/c_fc/b - [ 3072,     1,     1], type =    f32 size =    0.012 MB\n",
      "                                           model/h3/mlp/c_proj/w - [ 3072,   768,     1], type =    f16 size =     9.00 MB ->     1.41 MB\n",
      "                                           model/h3/mlp/c_proj/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
      "                                                 model/h4/ln_1/g - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
      "                                                 model/h4/ln_1/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
      "                                          model/h4/attn/c_attn/w - [  768,  2304,     1], type =    f16 size =     6.75 MB ->     1.05 MB\n",
      "                                          model/h4/attn/c_attn/b - [ 2304,     1,     1], type =    f32 size =    0.009 MB\n",
      "                                          model/h4/attn/c_proj/w - [  768,   768,     1], type =    f16 size =     2.25 MB ->     0.35 MB\n",
      "                                          model/h4/attn/c_proj/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
      "                                                 model/h4/ln_2/g - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
      "                                                 model/h4/ln_2/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
      "                                             model/h4/mlp/c_fc/w - [  768,  3072,     1], type =    f16 size =     9.00 MB ->     1.41 MB\n",
      "                                             model/h4/mlp/c_fc/b - [ 3072,     1,     1], type =    f32 size =    0.012 MB\n",
      "                                           model/h4/mlp/c_proj/w - [ 3072,   768,     1], type =    f16 size =     9.00 MB ->     1.41 MB\n",
      "                                           model/h4/mlp/c_proj/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
      "                                                 model/h5/ln_1/g - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
      "                                                 model/h5/ln_1/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          model/h5/attn/c_attn/w - [  768,  2304,     1], type =    f16 size =     6.75 MB ->     1.05 MB\n",
      "                                          model/h5/attn/c_attn/b - [ 2304,     1,     1], type =    f32 size =    0.009 MB\n",
      "                                          model/h5/attn/c_proj/w - [  768,   768,     1], type =    f16 size =     2.25 MB ->     0.35 MB\n",
      "                                          model/h5/attn/c_proj/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
      "                                                 model/h5/ln_2/g - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
      "                                                 model/h5/ln_2/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
      "                                             model/h5/mlp/c_fc/w - [  768,  3072,     1], type =    f16 size =     9.00 MB ->     1.41 MB\n",
      "                                             model/h5/mlp/c_fc/b - [ 3072,     1,     1], type =    f32 size =    0.012 MB\n",
      "                                           model/h5/mlp/c_proj/w - [ 3072,   768,     1], type =    f16 size =     9.00 MB ->     1.41 MB\n",
      "                                           model/h5/mlp/c_proj/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
      "                                                 model/h6/ln_1/g - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
      "                                                 model/h6/ln_1/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
      "                                          model/h6/attn/c_attn/w - [  768,  2304,     1], type =    f16 size =     6.75 MB ->     1.05 MB\n",
      "                                          model/h6/attn/c_attn/b - [ 2304,     1,     1], type =    f32 size =    0.009 MB\n",
      "                                          model/h6/attn/c_proj/w - [  768,   768,     1], type =    f16 size =     2.25 MB ->     0.35 MB\n",
      "                                          model/h6/attn/c_proj/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
      "                                                 model/h6/ln_2/g - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
      "                                                 model/h6/ln_2/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
      "                                             model/h6/mlp/c_fc/w - [  768,  3072,     1], type =    f16 size =     9.00 MB ->     1.41 MB\n",
      "                                             model/h6/mlp/c_fc/b - [ 3072,     1,     1], type =    f32 size =    0.012 MB\n",
      "                                           model/h6/mlp/c_proj/w - [ 3072,   768,     1], type =    f16 size =     9.00 MB ->     1.41 MB\n",
      "                                           model/h6/mlp/c_proj/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
      "                                                 model/h7/ln_1/g - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
      "                                                 model/h7/ln_1/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
      "                                          model/h7/attn/c_attn/w - [  768,  2304,     1], type =    f16 size =     6.75 MB ->     1.05 MB\n",
      "                                          model/h7/attn/c_attn/b - [ 2304,     1,     1], type =    f32 size =    0.009 MB\n",
      "                                          model/h7/attn/c_proj/w - [  768,   768,     1], type =    f16 size =     2.25 MB ->     0.35 MB\n",
      "                                          model/h7/attn/c_proj/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
      "                                                 model/h7/ln_2/g - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
      "                                                 model/h7/ln_2/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
      "                                             model/h7/mlp/c_fc/w - [  768,  3072,     1], type =    f16 size =     9.00 MB ->     1.41 MB\n",
      "                                             model/h7/mlp/c_fc/b - [ 3072,     1,     1], type =    f32 size =    0.012 MB\n",
      "                                           model/h7/mlp/c_proj/w - [ 3072,   768,     1], type =    f16 size =     9.00 MB ->     1.41 MB\n",
      "                                           model/h7/mlp/c_proj/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
      "                                                 model/h8/ln_1/g - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
      "                                                 model/h8/ln_1/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
      "                                          model/h8/attn/c_attn/w - [  768,  2304,     1], type =    f16 size =     6.75 MB ->     1.05 MB\n",
      "                                          model/h8/attn/c_attn/b - [ 2304,     1,     1], type =    f32 size =    0.009 MB\n",
      "                                          model/h8/attn/c_proj/w - [  768,   768,     1], type =    f16 size =     2.25 MB ->     0.35 MB\n",
      "                                          model/h8/attn/c_proj/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
      "                                                 model/h8/ln_2/g - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
      "                                                 model/h8/ln_2/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
      "                                             model/h8/mlp/c_fc/w - [  768,  3072,     1], type =    f16 size =     9.00 MB ->     1.41 MB\n",
      "                                             model/h8/mlp/c_fc/b - [ 3072,     1,     1], type =    f32 size =    0.012 MB\n",
      "                                           model/h8/mlp/c_proj/w - [ 3072,   768,     1], type =    f16 size =     9.00 MB ->     1.41 MB\n",
      "                                           model/h8/mlp/c_proj/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
      "                                                 model/h9/ln_1/g - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
      "                                                 model/h9/ln_1/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
      "                                          model/h9/attn/c_attn/w - [  768,  2304,     1], type =    f16 size =     6.75 MB ->     1.05 MB\n",
      "                                          model/h9/attn/c_attn/b - [ 2304,     1,     1], type =    f32 size =    0.009 MB\n",
      "                                          model/h9/attn/c_proj/w - [  768,   768,     1], type =    f16 size =     2.25 MB ->     0.35 MB\n",
      "                                          model/h9/attn/c_proj/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
      "                                                 model/h9/ln_2/g - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
      "                                                 model/h9/ln_2/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
      "                                             model/h9/mlp/c_fc/w - [  768,  3072,     1], type =    f16 size =     9.00 MB ->     1.41 MB\n",
      "                                             model/h9/mlp/c_fc/b - [ 3072,     1,     1], type =    f32 size =    0.012 MB\n",
      "                                           model/h9/mlp/c_proj/w - [ 3072,   768,     1], type =    f16 size =     9.00 MB ->     1.41 MB\n",
      "                                           model/h9/mlp/c_proj/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
      "                                                    model/ln_f/g - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
      "                                                    model/ln_f/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
      "                                                   model/lm_head - [  768, 50257,     1], type =    f16 size =   147.24 MB ->    23.01 MB\n",
      "ggml_common_quantize_0: model size  =   570.86 MB\n",
      "ggml_common_quantize_0: quant size  =    94.59 MB | ftype = 3 (q4_1)\n",
      "\n",
      "main: quantize time =  1632.94 ms\n",
      "main:    total time =  1632.94 ms\n"
     ]
    }
   ],
   "source": [
    "!ggml/build/bin/gpt-2-quantize models/ggml-model-f16.bin models/ggml-model-q4_1.bin 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: seed = 1711013187\n",
      "gpt2_model_load: loading model from 'models//ggml-model-q4_1.bin'\n",
      "gpt2_model_load: n_vocab = 50257\n",
      "gpt2_model_load: n_ctx   = 2048\n",
      "gpt2_model_load: n_embd  = 768\n",
      "gpt2_model_load: n_head  = 12\n",
      "gpt2_model_load: n_layer = 10\n",
      "gpt2_model_load: ftype   = 2003\n",
      "gpt2_model_load: qntvr   = 2\n",
      "gpt2_model_load: using CPU backend\n",
      "gpt2_model_load: ggml tensor size    = 368 bytes\n",
      "gpt2_model_load: backend buffer size =  94.59 MB\n",
      "gpt2_model_load: memory size =   120.00 MB, n_mem = 20480\n",
      "gpt2_model_load: model size  =    94.59 MB\n",
      "extract_tests_from_file : No test file found.\n",
      "test_gpt_tokenizer : 0 tests failed out of 0 tests.\n",
      "main: compute buffer size: 6.87 MB\n",
      "main: prompt: 'I love maching learning and'\n",
      "main: number of tokens in prompt = 6, first 8 tokens: 40 1842 3235 278 4673 290 \n",
      "\n",
      "I love maching learning and also love learning. It's not only that I love my first-person lovemaking but I've also found myself learning a lot of new skills with lots of great new skills.\n",
      "\n",
      "I want to share a few of these experiences with others to share with you. I'd love to share some of them in my own blog!\n",
      "\n",
      "I love to share my work with others.\n",
      "\n",
      "I love to share my work with others, and with a lot of people who have been around me, and I'm really grateful for those who are with me.\n",
      "\n",
      "I love to share my work with others and have been working with others in my life. I love to share my experiences with people, and to share my work with my family, friends and family and friends.\n",
      "\n",
      "I love to share my work with my family, friends and family. I love to share my work with people and people I love with. I love to share my work with others and to share my experiences\n",
      "\n",
      "main:     load time =    99.34 ms\n",
      "main:   sample time =    52.08 ms\n",
      "main:  predict time =  3378.52 ms / 16.48 ms per token\n",
      "main:    total time =  3537.80 ms\n"
     ]
    }
   ],
   "source": [
    "!ggml/build//bin/gpt-2-backend -m models//ggml-model-q4_1.bin -p \"I love maching learning and\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сейчас более универсальный и удобным стал формат gguf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пишем свой простой инференс:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gguf in /home/ubuntu/.local/lib/python3.8/site-packages (0.6.0)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ubuntu/.local/lib/python3.8/site-packages (from gguf) (1.24.4)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install gguf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gguf: This GGUF file is for Little Endian only\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import gguf\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def get_weight(model, name, dtype):\n",
    "    return model.state_dict()[name].numpy()\n",
    "\n",
    "def convert(model, output_name, dtype):\n",
    "    gguf_writer = gguf.GGUFWriter(output_name, \"mnist-artem\")\n",
    "\n",
    "\n",
    "    w0 = get_weight(model, \"0.weight\", dtype)\n",
    "    gguf_writer.add_tensor(\"w0\", w0)\n",
    "\n",
    "    b0 = get_weight(model, \"0.bias\", dtype)\n",
    "    gguf_writer.add_tensor(\"b0\", b0)\n",
    "\n",
    "    w1 = get_weight(model, \"2.weight\", dtype)\n",
    "    gguf_writer.add_tensor(\"w1\", w1)\n",
    "\n",
    "    b1 = get_weight(model, \"2.bias\", dtype)\n",
    "    gguf_writer.add_tensor(\"b1\", b1)\n",
    "    \n",
    "    gguf_writer.write_header_to_file()\n",
    "    gguf_writer.write_kv_data_to_file()\n",
    "    gguf_writer.write_tensors_to_file()\n",
    "    gguf_writer.close()\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    nn.Linear(784, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 10),\n",
    "    nn.Softmax(),\n",
    ")\n",
    "convert(model, \"model.gguf\", np.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ggml/examples/mnist/CMakeLists.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile ggml/examples/mnist/CMakeLists.txt\n",
    "#\n",
    "# mnist\n",
    "\n",
    "set(TEST_TARGET mnist)\n",
    "add_executable(${TEST_TARGET} main.cpp)\n",
    "target_link_libraries(${TEST_TARGET} PRIVATE ggml common)\n",
    "\n",
    "#\n",
    "# mnist-cnn\n",
    "\n",
    "set(TEST_TARGET mnist-cnn)\n",
    "add_executable(${TEST_TARGET} main-cnn.cpp)\n",
    "target_link_libraries(${TEST_TARGET} PRIVATE ggml common)\n",
    "\n",
    "set(TEST_TARGET mnist-artem)\n",
    "add_executable(${TEST_TARGET} mnist-artem.cpp)\n",
    "target_link_libraries(${TEST_TARGET} PRIVATE ggml common common-ggml)\n",
    "\n",
    "#\n",
    "# mnist-cpu\n",
    "\n",
    "set(TEST_TARGET mnist-cpu)\n",
    "add_executable(${TEST_TARGET} main-cpu.cpp)\n",
    "target_link_libraries(${TEST_TARGET} PRIVATE ggml)\n",
    "\n",
    "if (APPLE)\n",
    "    #\n",
    "    # mnist-mtl\n",
    "\n",
    "    find_library(FOUNDATION_LIBRARY Foundation REQUIRED)\n",
    "    find_library(METAL_FRAMEWORK    Metal      REQUIRED)\n",
    "    find_library(METALKIT_FRAMEWORK MetalKit   REQUIRED)\n",
    "    find_library(METALPERFORMANCE_FRAMEWORK MetalPerformanceShaders REQUIRED)\n",
    "\n",
    "    set(TEST_TARGET mnist-mtl)\n",
    "    add_executable(${TEST_TARGET} main-mtl.cpp main-mtl.h main-mtl.m)\n",
    "    target_link_libraries(${TEST_TARGET} PRIVATE\n",
    "        ggml\n",
    "        ${FOUNDATION_LIBRARY}\n",
    "        ${METAL_FRAMEWORK}\n",
    "        ${METALKIT_FRAMEWORK}\n",
    "        ${METALPERFORMANCE_FRAMEWORK}\n",
    "    )\n",
    "endif()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ggml/examples/mnist/mnist-artem.cpp\n"
     ]
    }
   ],
   "source": [
    "%%writefile ggml/examples/mnist/mnist-artem.cpp\n",
    "#include \"ggml/ggml.h\"\n",
    "\n",
    "#include \"common.h\"\n",
    "\n",
    "#include <cmath>\n",
    "#include <cstdio>\n",
    "#include <cstring>\n",
    "#include <ctime>\n",
    "#include <fstream>\n",
    "#include <string>\n",
    "#include <vector>\n",
    "#include <algorithm>\n",
    "\n",
    "#if defined(_MSC_VER)\n",
    "#pragma warning(disable: 4244 4267) // possible loss of data\n",
    "#endif\n",
    "\n",
    "struct mnist_model {\n",
    "    struct ggml_tensor * w0;\n",
    "    struct ggml_tensor * b0;\n",
    "    struct ggml_tensor * w1;\n",
    "    struct ggml_tensor * b1;\n",
    "    struct ggml_context * ctx;\n",
    "};\n",
    "\n",
    "bool mnist_model_load(const std::string & fname, mnist_model & model) {\n",
    "    struct gguf_init_params params = {\n",
    "        /*.no_alloc   =*/ false,\n",
    "        /*.ctx        =*/ &model.ctx,\n",
    "    };\n",
    "    gguf_context * ctx = gguf_init_from_file(fname.c_str(), params);\n",
    "    if (!ctx) {\n",
    "        fprintf(stderr, \"%s: gguf_init_from_file() failed\\n\", __func__);\n",
    "        return false;\n",
    "    }\n",
    "    model.w0 = ggml_get_tensor(model.ctx, \"w0\");\n",
    "    model.b0 = ggml_get_tensor(model.ctx, \"b0\");\n",
    "    model.w1 = ggml_get_tensor(model.ctx, \"w1\");\n",
    "    model.b1 = ggml_get_tensor(model.ctx, \"b1\");\n",
    "    return true;\n",
    "}\n",
    "\n",
    "int mnist_eval(\n",
    "        const mnist_model & model,\n",
    "        const int n_threads,\n",
    "        std::vector<float> digit,\n",
    "        const char * fname_cgraph\n",
    "        )\n",
    "{\n",
    "    static size_t buf_size = 100000 * sizeof(float) * 4;\n",
    "    static void * buf = malloc(buf_size);\n",
    "\n",
    "    struct ggml_init_params params = {\n",
    "        /*.mem_size   =*/ buf_size,\n",
    "        /*.mem_buffer =*/ buf,\n",
    "        /*.no_alloc   =*/ false,\n",
    "    };\n",
    "\n",
    "    struct ggml_context * ctx0 = ggml_init(params);\n",
    "    struct ggml_cgraph * gf = ggml_new_graph(ctx0);\n",
    "\n",
    "    struct ggml_tensor * input = ggml_new_tensor_1d(ctx0, GGML_TYPE_F32, 784);\n",
    "    memcpy(input->data, digit.data(), ggml_nbytes(input));\n",
    "    ggml_set_name(input, \"input\");\n",
    "    ggml_tensor * cur = ggml_add(ctx0, ggml_mul_mat(ctx0, model.w0, input), model.b0);\n",
    "    cur = ggml_relu(ctx0, cur);\n",
    "    cur = ggml_add(ctx0, ggml_mul_mat(ctx0, model.w1, cur), model.b1);\n",
    "    ggml_tensor * probs = ggml_soft_max(ctx0, cur);\n",
    "    ggml_set_name(probs, \"probs\");\n",
    "\n",
    "    ggml_build_forward_expand(gf, probs);\n",
    "    ggml_graph_compute_with_ctx(ctx0, gf, n_threads);\n",
    "\n",
    "    //ggml_graph_print(&gf);\n",
    "    ggml_graph_dump_dot(gf, NULL, \"mnist-cnn.dot\");\n",
    "\n",
    "    if (fname_cgraph) {\n",
    "        // export the compute graph for later use\n",
    "        // see the \"mnist-cpu\" example\n",
    "        ggml_graph_export(gf, fname_cgraph);\n",
    "\n",
    "        fprintf(stderr, \"%s: exported compute graph to '%s'\\n\", __func__, fname_cgraph);\n",
    "    }\n",
    "\n",
    "    const float * probs_data = ggml_get_data_f32(probs);\n",
    "    const int prediction = std::max_element(probs_data, probs_data + 10) - probs_data;\n",
    "    ggml_free(ctx0);\n",
    "    return prediction;\n",
    "}\n",
    "\n",
    "int main(int argc, char ** argv) {\n",
    "    srand(time(NULL));\n",
    "    ggml_time_init();\n",
    "\n",
    "    if (argc != 3) {\n",
    "        fprintf(stderr, \"Usage: %s models/mnist/mnist-cnn.gguf models/mnist/t10k-images.idx3-ubyte\\n\", argv[0]);\n",
    "        exit(0);\n",
    "    }\n",
    "\n",
    "    uint8_t buf[784];\n",
    "    mnist_model model;\n",
    "    std::vector<float> digit;\n",
    "\n",
    "    // load the model\n",
    "    {\n",
    "        const int64_t t_start_us = ggml_time_us();\n",
    "\n",
    "        if (!mnist_model_load(argv[1], model)) {\n",
    "            fprintf(stderr, \"%s: failed to load model from '%s'\\n\", __func__, argv[1]);\n",
    "            return 1;\n",
    "        }\n",
    "\n",
    "        const int64_t t_load_us = ggml_time_us() - t_start_us;\n",
    "\n",
    "        fprintf(stdout, \"%s: loaded model in %8.2f ms\\n\", __func__, t_load_us / 1000.0f);\n",
    "    }\n",
    "\n",
    "    // read a random digit from the test set\n",
    "    {\n",
    "        std::ifstream fin(argv[2], std::ios::binary);\n",
    "        if (!fin) {\n",
    "            fprintf(stderr, \"%s: failed to open '%s'\\n\", __func__, argv[2]);\n",
    "            return 1;\n",
    "        }\n",
    "\n",
    "        // seek to a random digit: 16-byte header + 28*28 * (random 0 - 10000)\n",
    "        fin.seekg(16 + 784 * (rand() % 10000));\n",
    "        fin.read((char *) &buf, sizeof(buf));\n",
    "    }\n",
    "\n",
    "    // render the digit in ASCII\n",
    "    {\n",
    "        digit.resize(sizeof(buf));\n",
    "\n",
    "        for (int row = 0; row < 28; row++) {\n",
    "            for (int col = 0; col < 28; col++) {\n",
    "                fprintf(stderr, \"%c \", (float)buf[row*28 + col] > 230 ? '*' : '_');\n",
    "                digit[row*28 + col] = ((float)buf[row*28 + col] / 255.0f);\n",
    "            }\n",
    "\n",
    "            fprintf(stderr, \"\\n\");\n",
    "        }\n",
    "\n",
    "        fprintf(stderr, \"\\n\");\n",
    "    }\n",
    "\n",
    "    const int prediction = mnist_eval(model, 1, digit, nullptr);\n",
    "    fprintf(stdout, \"%s: predicted digit is %d\\n\", __func__, prediction);\n",
    "    ggml_free(model.ctx);\n",
    "    return 0;\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- CMAKE_SYSTEM_PROCESSOR: x86_64\n",
      "GNU ld (GNU Binutils for Ubuntu) 2.34\n",
      "-- x86 detected\n",
      "-- Linux detected\n",
      "-- x86 detected\n",
      "-- Linux detected\n",
      "-- Configuring done\n",
      "-- Generating done\n",
      "-- Build files have been written to: /home/ubuntu/lsml-2024/ggml/build\n",
      "[ 18%] Built target common\n",
      "[ 63%] Built target ggml\n",
      "[ 81%] Built target common-ggml\n",
      "\u001b[35m\u001b[1mScanning dependencies of target mnist-artem\u001b[0m\n",
      "[ 90%] \u001b[32mBuilding CXX object examples/mnist/CMakeFiles/mnist-artem.dir/mnist-artem.cpp.o\u001b[0m\n",
      "[100%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/mnist-artem\u001b[0m\n",
      "[100%] Built target mnist-artem\n"
     ]
    }
   ],
   "source": [
    "!cd ggml/build && cmake .. && make -j4 mnist-artem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: loaded model in     3.14 ms\r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ * * * * * * * _ _ _ _ _ _ _ _ _ _ \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ * * * * * * * * * * _ _ _ _ _ _ _ \r\n",
      "_ _ _ _ _ _ _ _ _ _ * * * * * * * * * * * * _ _ _ _ _ _ \r\n",
      "_ _ _ _ _ _ _ _ * * * * * * _ _ _ _ * * * * _ _ _ _ _ _ \r\n",
      "_ _ _ _ _ _ _ _ * * * * _ _ _ _ _ _ * * * _ _ _ _ _ _ _ \r\n",
      "_ _ _ _ _ _ _ _ * * * _ _ _ _ _ _ _ * * _ _ _ _ _ _ _ _ \r\n",
      "_ _ _ _ _ _ _ * * * _ _ _ _ _ _ _ * * * _ _ _ _ _ _ _ _ \r\n",
      "_ _ _ _ _ _ * * * _ _ _ _ _ _ _ * * * _ _ _ _ _ _ _ _ _ \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ * * _ _ _ _ _ _ _ _ _ _ \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ * * * _ _ _ _ _ _ _ _ _ _ \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ * * _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ * * _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ * _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ * * _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ * * _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ * * _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ * * * _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ * _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ * _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ * _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "ggml_graph_dump_dot: dot -Tpng mnist-cnn.dot -o mnist-cnn.dot.png && open mnist-cnn.dot.png\r\n",
      "main: predicted digit is 4\r\n"
     ]
    }
   ],
   "source": [
    "!ggml/build/bin/mnist-artem model.gguf ggml/examples/mnist/models/mnist/t10k-images.idx3-ubyte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
