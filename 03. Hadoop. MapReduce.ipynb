{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hadoop and MapReduce\n",
    "\n",
    "Автор ноутбука: Алексей Космачев"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Основы MapReduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В своей сути MapRedcue это очень простая парадигма. Допустим у нас есть датасет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-25T20:25:44.158742Z",
     "start_time": "2021-01-25T20:25:41.796781Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 89.9M  100 89.9M    0     0  19.3M      0  0:00:04  0:00:04 --:--:-- 19.3M\n"
     ]
    }
   ],
   "source": [
    "! curl https://raw.githubusercontent.com/fivethirtyeight/russian-troll-tweets/master/IRAhandle_tweets_1.csv > tweets_1.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "external_author_id,author,content,region,language,publish_date,harvested_date,following,followers,updates,post_type,account_type,retweet,account_category,new_june_2018,alt_external_id,tweet_id,article_url,tco1_step1,tco2_step1,tco3_step1\r",
      "\r\n",
      "906000000000000000,10_GOP,\"\"\"We have a sitting Democrat US Senator on trial for corruption and you've barely heard a peep from the mainstream media.\"\" ~ @nedryun https://t.co/gh6g0D1oiC\",Unknown,English,10/1/2017 19:58,10/1/2017 19:59,1052,9636,253,,Right,0,RightTroll,0,905874659358453760,914580356430536707,http://twitter.com/905874659358453760/statuses/914580356430536707,https://twitter.com/10_gop/status/914580356430536707/video/1,,\r",
      "\r\n"
     ]
    }
   ],
   "source": [
    "! head -n 2 tweets_1.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы хотим в этом датасете что-нибудь найти. Например (сюрприз-сюрприз), посчитать количество уникальных слов. Мы могли бы сделать что-то такое:\n",
    "\n",
    "#### Вариант 1 \n",
    "Используем исключительно питон и наивный алгоритм"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-25T20:30:27.659125Z",
     "start_time": "2021-01-25T20:30:25.819459Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t\t268703\n",
      "co\t250375\n",
      "https\t221366\n",
      "the\t69350\n",
      "to\t55972\n",
      "a\t43420\n",
      "in\t37099\n",
      "s\t36085\n",
      "of\t33579\n",
      "http\t28661\n",
      "CPU times: user 4.03 s, sys: 45 ms, total: 4.07 s\n",
      "Wall time: 4.06 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from collections import Counter\n",
    "import csv\n",
    "import re\n",
    "import sys\n",
    "\n",
    "counter = Counter()\n",
    "pattern = re.compile(r\"[a-z]+\")\n",
    "\n",
    "with open('tweets_1.csv', 'r') as f:\n",
    "    reader = csv.reader(f, delimiter=',')\n",
    "    for row in reader:\n",
    "        content = row[2]\n",
    "        for match in pattern.finditer(content.lower()):\n",
    "            word = match.group(0)\n",
    "            counter[word] += 1\n",
    "\n",
    "for word, count in counter.most_common(10):\n",
    "    print(f\"{word}\\t{count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-25T20:30:53.751130Z",
     "start_time": "2021-01-25T20:30:53.746887Z"
    }
   },
   "source": [
    "Такое сработает только если у нас не очень много данных и они все вмещаются в оперативную память"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90M\ttweets_1.csv\r\n"
     ]
    }
   ],
   "source": [
    "! du -h tweets_1.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Вариант 2\n",
    "Используем парадигму Map Reduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом примере у нас всего 90 мегабайт данных, и на моем компьютере они обрабатываются за примерно 4 секунды с помощью питона. Теперь представим (это достаточно несложно), что у нас приходит новых данных приходит _десятки терабайт_ в сутки. Такое уже не поместится ни в один сервер, поэтому нам нужно придумать что-нибудь похитрее.\n",
    "\n",
    "MapReduce как раз является парадигмой, помогающей обрабатывать большие объемы данных, за счет простоты своего устройства.\n",
    "\n",
    "Приятная новость - для того, чтобы понять и научиться программировать программы в парадигме MapReduce вам потребуется... **5 секунд!**\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/ADKosm/lsml-2021-public/main/imgs/you-know-mapreduce.png\" width=\"400\">\n",
    "\n",
    "Все потому что вы уже прошли семинар по Bash и научились составлять большие программы в виде компоновки небольших  программ, соединенных пайпами. По своей сути программа на MapReduce - это хорошо отмасштабированная программа вида\n",
    "\n",
    "```bash\n",
    "cat data.txt | map | sort | reduce\n",
    "```\n",
    "\n",
    "Сортировку за вас выполняет сам фреймворк (и ее вы можете дополнительно настроить точно такое как и команду sort). А также он сам разбивает данные на части и параллельно запускает операции map и reduce. \n",
    "\n",
    "Таким образом на самом деле Hadoop - это всего лишь гигантская машина сортировки, которая дополнительно дает вам некоторые гарантии:\n",
    "\n",
    "* Для всех данных параллельно будет применена операция map\n",
    "* Данные будут отсортированы по указанному вами ключу\n",
    "* Каждый ключ будет целиком передан на один и только один reduce\n",
    "\n",
    "Программисту остается реализовать программу, которая состоит из двух компонент: `map` и `reduce`. \n",
    "\n",
    "Операция `map` -- это просто функция из одного элемента в другой элемент, у которого есть первичный ключ. \n",
    "\n",
    "Операция `reduce` -- это коммутативная и ассоциативная агрегация всех элементов по ключу. Чтобы эти операции совершить, надо разбить весь вход на куски данных и отправить их на машины, чтобы они выполнялись в параллель, а весь выход операции map идёт в операцию shuffle, которая по одним и тем же ключам определяет записи на одинаковые хосты. \n",
    "\n",
    "В итоге получается, что мы можем спокойно увеличивать количество worker'ов для map операций и с увеличением количества данных мы лишь будем линейно утилизировать количество машин, то же самое с операцией reduce -- мы можем добавлять машины с ростом увеличения количества ключей линейно, не боясь того, что мы не можем позволить на одной какой-то машине больше памяти или диска.\n",
    "\n",
    "Давайте напишем маппер и редьюсер на питоне для этой задачи:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tqdm\n",
      "  Downloading tqdm-4.66.2-py3-none-any.whl (78 kB)\n",
      "\u001b[K     |████████████████████████████████| 78 kB 972 kB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: tqdm\n",
      "Successfully installed tqdm-4.66.2\n"
     ]
    }
   ],
   "source": [
    "! sudo pip3 install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-25T20:45:38.075819Z",
     "start_time": "2021-01-25T20:45:38.069953Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing wordcount.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile wordcount.py\n",
    "import sys\n",
    "import csv\n",
    "import re\n",
    "\n",
    "\n",
    "def mapper():\n",
    "    pattern = re.compile(r\"[a-z]+\")\n",
    "    for row in csv.reader(iter(sys.stdin.readline, '')):\n",
    "        content = row[2]\n",
    "        for match in pattern.finditer(content.lower()):\n",
    "            word = match.group(0)\n",
    "            print(\"{}\\t{}\".format(word, 1))\n",
    "\n",
    "\n",
    "def reducer():\n",
    "    word, number = next(sys.stdin).split('\\t')\n",
    "    number = int(number)\n",
    "    for line in sys.stdin:\n",
    "        current_word, current_number = line.split('\\t')\n",
    "        current_number = int(current_number)\n",
    "        if current_word != word:\n",
    "            print(\"{}\\t{}\".format(word, number))\n",
    "            word = current_word\n",
    "            number = current_number\n",
    "        else:\n",
    "            number += current_number\n",
    "    print(\"{}\\t{}\".format(word, number))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    mr_command = sys.argv[1]\n",
    "    {\n",
    "        'map': mapper,\n",
    "        'reduce': reducer\n",
    "    }[mr_command]()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Важно еще удалить голову у таблицы, иначе подсчеты могут быть некоректными"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "! sed -i -e '1'd tweets_1.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "906000000000000000,10_GOP,\"\"\"We have a sitting Democrat US Senator on trial for corruption and you've barely heard a peep from the mainstream media.\"\" ~ @nedryun https://t.co/gh6g0D1oiC\",Unknown,English,10/1/2017 19:58,10/1/2017 19:59,1052,9636,253,,Right,0,RightTroll,0,905874659358453760,914580356430536707,http://twitter.com/905874659358453760/statuses/914580356430536707,https://twitter.com/10_gop/status/914580356430536707/video/1,,\r",
      "\r\n",
      "906000000000000000,10_GOP,Marshawn Lynch arrives to game in anti-Trump shirt. Judging by his sagging pants the shirt should say Lynch vs. belt https://t.co/mLH1i30LZZ,Unknown,English,10/1/2017 22:43,10/1/2017 22:43,1054,9637,254,,Right,0,RightTroll,0,905874659358453760,914621840496189440,http://twitter.com/905874659358453760/statuses/914621840496189440,https://twitter.com/damienwoody/status/914568524449959937/video/1,,\r",
      "\r\n"
     ]
    }
   ],
   "source": [
    "! head -n 2 tweets_1.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\t1\r\n",
      "a\t1\r\n",
      "a\t1\r\n",
      "a\t1\r\n",
      "a\t1\r\n",
      "a\t1\r\n",
      "a\t1\r\n",
      "a\t1\r\n",
      "a\t1\r\n",
      "a\t1\r\n",
      "sort: write failed: 'standard output': Broken pipe\r\n",
      "sort: write error\r\n"
     ]
    }
   ],
   "source": [
    "! cat tweets_1.csv | python wordcount.py map | sort -k1,1 | head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-25T20:47:23.728893Z",
     "start_time": "2021-01-25T20:46:51.895208Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████| 243891/243891 [00:07<00:00, 31141.82it/s]\n",
      "CPU times: user 226 ms, sys: 37.1 ms, total: 263 ms\n",
      "Wall time: 10.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "! cat tweets_1.csv | \\\n",
    "    tqdm --total $(cat tweets_1.csv | wc -l)| \\\n",
    "    python wordcount.py map | \\\n",
    "    sort -k1,1 | \\\n",
    "    python wordcount.py reduce > result.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\t43420\r\n",
      "aa\t151\r\n",
      "aaa\t13\r\n",
      "aaaaaa\t1\r\n",
      "aaaaaaaaaaaaand\t1\r\n",
      "aaaaaaaaaall\t1\r\n",
      "aaaaaaaamen\t1\r\n",
      "aaaaaaaand\t2\r\n",
      "aaaaaaargh\t1\r\n",
      "aaaaaand\t2\r\n"
     ]
    }
   ],
   "source": [
    "! head result.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-25T20:50:28.963822Z",
     "start_time": "2021-01-25T20:50:28.265568Z"
    }
   },
   "source": [
    "Отлично! Слова есть, осталось только найти top-10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting top10.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile top10.py\n",
    "import sys\n",
    "\n",
    "\n",
    "def _rewind_stream(stream):\n",
    "    for _ in stream:\n",
    "        pass\n",
    "\n",
    "\n",
    "def mapper():\n",
    "    for row in sys.stdin:\n",
    "        key, value = row.split('\\t')\n",
    "        print(\"{}+{}\\t\".format(key, value.strip()))\n",
    "\n",
    "\n",
    "def reducer():\n",
    "    for _ in range(10):\n",
    "        key, _ = next(sys.stdin).split('\\t')\n",
    "        word, count = key.split(\"+\")\n",
    "        print(\"{}\\t{}\".format(word, count))\n",
    "    _rewind_stream(sys.stdin)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    mr_command = sys.argv[1]\n",
    "    {\n",
    "        'map': mapper,\n",
    "        'reduce': reducer\n",
    "    }[mr_command]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████| 346613/346613 [00:00<00:00, 743928.29it/s]\n"
     ]
    }
   ],
   "source": [
    "! cat result.txt | \\\n",
    "    tqdm --total $(cat result.txt | wc -l) | \\\n",
    "    python top10.py map | \\\n",
    "    sort -t'+' -k2,2nr -k1,1 | \\\n",
    "    python top10.py reduce > top-10.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t\t268703\r\n",
      "co\t250375\r\n",
      "https\t221366\r\n",
      "the\t69350\r\n",
      "to\t55972\r\n",
      "a\t43420\r\n",
      "in\t37099\r\n",
      "s\t36085\r\n",
      "of\t33579\r\n",
      "http\t28661\r\n"
     ]
    }
   ],
   "source": [
    "! cat top-10.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На MapReduce мы задачу переписали, однако быстрее работать она пока не стала. Все дело в том, что мы это еще не на кластере запускали! Время запускать все на настоящем кластере!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Загружаем данные в HDFS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-25T20:52:22.492958Z",
     "start_time": "2021-01-25T20:52:22.382757Z"
    }
   },
   "source": [
    "При работе с HDFS нужно понимать, что есть два места, где хранятся данные\n",
    "\n",
    "1. На локальных жестких дисках машин кластера - это деволтная система, на нее можно посмотреть через `hdfs dfs -ls /`\n",
    "\n",
    "2. В Object Storage - для работы с ней, нужно указывать путь до бакета - `hdfs dfs -ls s3a://lsml2022alexius/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[1/13]: https://raw.githubusercontent.com/fivethirtyeight/russian-troll-tweets/master/IRAhandle_tweets_1.csv --> IRAhandle_tweets_1.csv\n",
      "--_curl_--https://raw.githubusercontent.com/fivethirtyeight/russian-troll-tweets/master/IRAhandle_tweets_1.csv\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 89.9M  100 89.9M    0     0  19.2M      0  0:00:04  0:00:04 --:--:-- 19.2M\n",
      "\n",
      "[2/13]: https://raw.githubusercontent.com/fivethirtyeight/russian-troll-tweets/master/IRAhandle_tweets_2.csv --> IRAhandle_tweets_2.csv\n",
      "--_curl_--https://raw.githubusercontent.com/fivethirtyeight/russian-troll-tweets/master/IRAhandle_tweets_2.csv\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 90.0M  100 90.0M    0     0  15.3M      0  0:00:05  0:00:05 --:--:-- 23.3M\n",
      "\n",
      "[3/13]: https://raw.githubusercontent.com/fivethirtyeight/russian-troll-tweets/master/IRAhandle_tweets_3.csv --> IRAhandle_tweets_3.csv\n",
      "--_curl_--https://raw.githubusercontent.com/fivethirtyeight/russian-troll-tweets/master/IRAhandle_tweets_3.csv\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 90.0M  100 90.0M    0     0  18.7M      0  0:00:04  0:00:04 --:--:-- 23.7M\n",
      "\n",
      "[4/13]: https://raw.githubusercontent.com/fivethirtyeight/russian-troll-tweets/master/IRAhandle_tweets_4.csv --> IRAhandle_tweets_4.csv\n",
      "--_curl_--https://raw.githubusercontent.com/fivethirtyeight/russian-troll-tweets/master/IRAhandle_tweets_4.csv\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 90.0M  100 90.0M    0     0  16.8M      0  0:00:05  0:00:05 --:--:-- 20.7M\n",
      "\n",
      "[5/13]: https://raw.githubusercontent.com/fivethirtyeight/russian-troll-tweets/master/IRAhandle_tweets_5.csv --> IRAhandle_tweets_5.csv\n",
      "--_curl_--https://raw.githubusercontent.com/fivethirtyeight/russian-troll-tweets/master/IRAhandle_tweets_5.csv\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 90.0M  100 90.0M    0     0  23.1M      0  0:00:03  0:00:03 --:--:-- 23.1M\n",
      "\n",
      "[6/13]: https://raw.githubusercontent.com/fivethirtyeight/russian-troll-tweets/master/IRAhandle_tweets_6.csv --> IRAhandle_tweets_6.csv\n",
      "--_curl_--https://raw.githubusercontent.com/fivethirtyeight/russian-troll-tweets/master/IRAhandle_tweets_6.csv\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 90.0M  100 90.0M    0     0  18.0M      0  0:00:05  0:00:05 --:--:-- 22.5M\n",
      "\n",
      "[7/13]: https://raw.githubusercontent.com/fivethirtyeight/russian-troll-tweets/master/IRAhandle_tweets_7.csv --> IRAhandle_tweets_7.csv\n",
      "--_curl_--https://raw.githubusercontent.com/fivethirtyeight/russian-troll-tweets/master/IRAhandle_tweets_7.csv\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 90.0M  100 90.0M    0     0  22.8M      0  0:00:03  0:00:03 --:--:-- 22.8M\n",
      "\n",
      "[8/13]: https://raw.githubusercontent.com/fivethirtyeight/russian-troll-tweets/master/IRAhandle_tweets_8.csv --> IRAhandle_tweets_8.csv\n",
      "--_curl_--https://raw.githubusercontent.com/fivethirtyeight/russian-troll-tweets/master/IRAhandle_tweets_8.csv\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 90.0M  100 90.0M    0     0  15.6M      0  0:00:05  0:00:05 --:--:-- 24.0M\n",
      "\n",
      "[9/13]: https://raw.githubusercontent.com/fivethirtyeight/russian-troll-tweets/master/IRAhandle_tweets_9.csv --> IRAhandle_tweets_9.csv\n",
      "--_curl_--https://raw.githubusercontent.com/fivethirtyeight/russian-troll-tweets/master/IRAhandle_tweets_9.csv\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 89.9M  100 89.9M    0     0  15.2M      0  0:00:05  0:00:05 --:--:-- 23.0M\n",
      "\n",
      "[10/13]: https://raw.githubusercontent.com/fivethirtyeight/russian-troll-tweets/master/IRAhandle_tweets_10.csv --> IRAhandle_tweets_10.csv\n",
      "--_curl_--https://raw.githubusercontent.com/fivethirtyeight/russian-troll-tweets/master/IRAhandle_tweets_10.csv\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 90.0M  100 90.0M    0     0  16.9M      0  0:00:05  0:00:05 --:--:-- 20.9M\n",
      "\n",
      "[11/13]: https://raw.githubusercontent.com/fivethirtyeight/russian-troll-tweets/master/IRAhandle_tweets_11.csv --> IRAhandle_tweets_11.csv\n",
      "--_curl_--https://raw.githubusercontent.com/fivethirtyeight/russian-troll-tweets/master/IRAhandle_tweets_11.csv\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 89.9M  100 89.9M    0     0  17.2M      0  0:00:05  0:00:05 --:--:-- 21.4M\n",
      "\n",
      "[12/13]: https://raw.githubusercontent.com/fivethirtyeight/russian-troll-tweets/master/IRAhandle_tweets_12.csv --> IRAhandle_tweets_12.csv\n",
      "--_curl_--https://raw.githubusercontent.com/fivethirtyeight/russian-troll-tweets/master/IRAhandle_tweets_12.csv\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 90.0M  100 90.0M    0     0  21.1M      0  0:00:04  0:00:04 --:--:-- 27.7M\n",
      "\n",
      "[13/13]: https://raw.githubusercontent.com/fivethirtyeight/russian-troll-tweets/master/IRAhandle_tweets_13.csv --> IRAhandle_tweets_13.csv\n",
      "--_curl_--https://raw.githubusercontent.com/fivethirtyeight/russian-troll-tweets/master/IRAhandle_tweets_13.csv\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 8045k  100 8045k    0     0  16.1M      0 --:--:-- --:--:-- --:--:-- 16.1M\n"
     ]
    }
   ],
   "source": [
    "! curl -O https://raw.githubusercontent.com/fivethirtyeight/russian-troll-tweets/master/IRAhandle_tweets_{`seq -s , 1 13`}.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подформатируем"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish 1\n",
      "Finish 2\n",
      "Finish 3\n",
      "Finish 4\n",
      "Finish 5\n",
      "Finish 6\n",
      "Finish 7\n",
      "Finish 8\n",
      "Finish 9\n",
      "Finish 10\n",
      "Finish 11\n",
      "Finish 12\n",
      "Finish 13\n"
     ]
    }
   ],
   "source": [
    "! for i in {1..13}; do sed IRAhandle_tweets_$i.csv -i -e '1'd && echo \"Finish $i\" ; done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим отдельную папку для этих данных в HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 items\r\n",
      "drwx------   - mapred hadoop          0 2024-02-08 06:57 /hadoop\r\n",
      "drwxrwxrwt   - hdfs   hadoop          0 2024-02-08 06:56 /tmp\r\n",
      "drwxrwxrwt   - hdfs   hadoop          0 2024-02-08 07:22 /user\r\n",
      "drwxrwxrwt   - hdfs   hadoop          0 2024-02-08 06:57 /var\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -ls /"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `/user/tweets/data': No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -rm -r /user/tweets/data\n",
    "! hdfs dfs -mkdir -p /user/tweets/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: все команды для hdfs смотреть здесь - https://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/FileSystemShell.html\n",
    "\n",
    "Заливаем данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "! hdfs dfs -put IRAhandle_tweets_* /user/tweets/data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-02-14 23:32:16,806 INFO balancer.Balancer: namenodes  = [hdfs://rc1a-dataproc-m-zkmve6o6f689a4so.mdb.yandexcloud.net:8020]\n",
      "2024-02-14 23:32:16,810 INFO balancer.Balancer: parameters = Balancer.BalancerParameters [BalancingPolicy.Node, threshold = 10.0, max idle iteration = 5, #excluded nodes = 0, #included nodes = 0, #source nodes = 0, #blockpools = 0, run during upgrade = false]\n",
      "2024-02-14 23:32:16,810 INFO balancer.Balancer: included nodes = []\n",
      "2024-02-14 23:32:16,810 INFO balancer.Balancer: excluded nodes = []\n",
      "2024-02-14 23:32:16,810 INFO balancer.Balancer: source nodes = []\n",
      "Time Stamp               Iteration#  Bytes Already Moved  Bytes Left To Move  Bytes Being Moved  NameNode\n",
      "2024-02-14 23:32:16,814 INFO balancer.NameNodeConnector: getBlocks calls for hdfs://rc1a-dataproc-m-zkmve6o6f689a4so.mdb.yandexcloud.net:8020 will be rate-limited to 20 per second\n",
      "2024-02-14 23:32:17,644 INFO balancer.Balancer: dfs.namenode.get-blocks.max-qps = 20 (default=20)\n",
      "2024-02-14 23:32:17,644 INFO balancer.Balancer: dfs.balancer.movedWinWidth = 5400000 (default=5400000)\n",
      "2024-02-14 23:32:17,644 INFO balancer.Balancer: dfs.balancer.moverThreads = 1000 (default=1000)\n",
      "2024-02-14 23:32:17,644 INFO balancer.Balancer: dfs.balancer.dispatcherThreads = 200 (default=200)\n",
      "2024-02-14 23:32:17,644 INFO balancer.Balancer: dfs.balancer.getBlocks.size = 2147483648 (default=2147483648)\n",
      "2024-02-14 23:32:17,644 INFO balancer.Balancer: dfs.balancer.getBlocks.min-block-size = 10485760 (default=10485760)\n",
      "2024-02-14 23:32:17,644 INFO balancer.Balancer: dfs.datanode.balance.max.concurrent.moves = 50 (default=50)\n",
      "2024-02-14 23:32:17,644 INFO balancer.Balancer: dfs.datanode.balance.bandwidthPerSec = 10485760 (default=10485760)\n",
      "2024-02-14 23:32:17,649 INFO balancer.Balancer: dfs.balancer.max-size-to-move = 10737418240 (default=10737418240)\n",
      "2024-02-14 23:32:17,649 INFO balancer.Balancer: dfs.blocksize = 268435456 (default=134217728)\n",
      "2024-02-14 23:32:17,665 INFO net.NetworkTopology: Adding a new node: /default-rack/10.128.0.12:9866\n",
      "2024-02-14 23:32:17,666 INFO balancer.Balancer: 0 over-utilized: []\n",
      "2024-02-14 23:32:17,666 INFO balancer.Balancer: 0 underutilized: []\n",
      "The cluster is balanced. Exiting...\n",
      "Feb 14, 2024 11:32:17 PM          0                  0 B                 0 B                0 B  hdfs://rc1a-dataproc-m-zkmve6o6f689a4so.mdb.yandexcloud.net:8020\n",
      "Feb 14, 2024 11:32:17 PM Balancing took 1.01 seconds\n"
     ]
    }
   ],
   "source": [
    "! sudo mkdir -p /usr/lib/hadoop/logs\n",
    "! sudo chmod 0777 /usr/lib/hadoop/logs\n",
    "! sudo -u hdfs hdfs balancer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configured Capacity: 135174471680 (125.89 GB)\r\n",
      "Present Capacity: 116267854346 (108.28 GB)\r\n",
      "DFS Remaining: 115118080000 (107.21 GB)\r\n",
      "DFS Used: 1149774346 (1.07 GB)\r\n",
      "DFS Used%: 0.99%\r\n",
      "Replicated Blocks:\r\n",
      "\tUnder replicated blocks: 0\r\n",
      "\tBlocks with corrupt replicas: 0\r\n",
      "\tMissing blocks: 0\r\n",
      "\tMissing blocks (with replication factor 1): 0\r\n",
      "\tLow redundancy blocks with highest priority to recover: 0\r\n",
      "\tPending deletion blocks: 0\r\n",
      "Erasure Coded Block Groups: \r\n",
      "\tLow redundancy block groups: 0\r\n",
      "\tBlock groups with corrupt internal blocks: 0\r\n",
      "\tMissing block groups: 0\r\n",
      "\tLow redundancy blocks with highest priority to recover: 0\r\n",
      "\tPending deletion blocks: 0\r\n",
      "\r\n",
      "-------------------------------------------------\r\n",
      "Live datanodes (1):\r\n",
      "\r\n",
      "Name: 10.128.0.12:9866 (rc1a-dataproc-d-t299z6uw2n0sko0q.mdb.yandexcloud.net)\r\n",
      "Hostname: rc1a-dataproc-d-t299z6uw2n0sko0q.mdb.yandexcloud.net\r\n",
      "Decommission Status : Normal\r\n",
      "Configured Capacity: 135174471680 (125.89 GB)\r\n",
      "DFS Used: 1149774346 (1.07 GB)\r\n",
      "Non DFS Used: 13339059702 (12.42 GB)\r\n",
      "DFS Remaining: 115118080000 (107.21 GB)\r\n",
      "DFS Used%: 0.85%\r\n",
      "DFS Remaining%: 85.16%\r\n",
      "Configured Cache Capacity: 0 (0 B)\r\n",
      "Cache Used: 0 (0 B)\r\n",
      "Cache Remaining: 0 (0 B)\r\n",
      "Cache Used%: 100.00%\r\n",
      "Cache Remaining%: 0.00%\r\n",
      "Xceivers: 1\r\n",
      "Last contact: Wed Feb 14 23:32:38 UTC 2024\r\n",
      "Last Block Report: Wed Feb 14 23:17:02 UTC 2024\r\n",
      "Num of Blocks: 18\r\n",
      "\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "! sudo -u hdfs hdfs dfsadmin -report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-02-14 23:32:42,809 INFO balancer.Balancer: Using a threshold of 1.0\n",
      "2024-02-14 23:32:42,810 INFO balancer.Balancer: namenodes  = [hdfs://rc1a-dataproc-m-zkmve6o6f689a4so.mdb.yandexcloud.net:8020]\n",
      "2024-02-14 23:32:42,812 INFO balancer.Balancer: parameters = Balancer.BalancerParameters [BalancingPolicy.Node, threshold = 1.0, max idle iteration = 5, #excluded nodes = 0, #included nodes = 0, #source nodes = 0, #blockpools = 0, run during upgrade = false]\n",
      "2024-02-14 23:32:42,812 INFO balancer.Balancer: included nodes = []\n",
      "2024-02-14 23:32:42,812 INFO balancer.Balancer: excluded nodes = []\n",
      "2024-02-14 23:32:42,812 INFO balancer.Balancer: source nodes = []\n",
      "Time Stamp               Iteration#  Bytes Already Moved  Bytes Left To Move  Bytes Being Moved  NameNode\n",
      "2024-02-14 23:32:42,814 INFO balancer.NameNodeConnector: getBlocks calls for hdfs://rc1a-dataproc-m-zkmve6o6f689a4so.mdb.yandexcloud.net:8020 will be rate-limited to 20 per second\n",
      "2024-02-14 23:32:43,622 INFO balancer.Balancer: dfs.namenode.get-blocks.max-qps = 20 (default=20)\n",
      "2024-02-14 23:32:43,622 INFO balancer.Balancer: dfs.balancer.movedWinWidth = 5400000 (default=5400000)\n",
      "2024-02-14 23:32:43,622 INFO balancer.Balancer: dfs.balancer.moverThreads = 1000 (default=1000)\n",
      "2024-02-14 23:32:43,622 INFO balancer.Balancer: dfs.balancer.dispatcherThreads = 200 (default=200)\n",
      "2024-02-14 23:32:43,622 INFO balancer.Balancer: dfs.balancer.getBlocks.size = 2147483648 (default=2147483648)\n",
      "2024-02-14 23:32:43,622 INFO balancer.Balancer: dfs.balancer.getBlocks.min-block-size = 10485760 (default=10485760)\n",
      "2024-02-14 23:32:43,622 INFO balancer.Balancer: dfs.datanode.balance.max.concurrent.moves = 50 (default=50)\n",
      "2024-02-14 23:32:43,623 INFO balancer.Balancer: dfs.datanode.balance.bandwidthPerSec = 10485760 (default=10485760)\n",
      "2024-02-14 23:32:43,627 INFO balancer.Balancer: dfs.balancer.max-size-to-move = 10737418240 (default=10737418240)\n",
      "2024-02-14 23:32:43,627 INFO balancer.Balancer: dfs.blocksize = 268435456 (default=134217728)\n",
      "2024-02-14 23:32:43,635 INFO net.NetworkTopology: Adding a new node: /default-rack/10.128.0.12:9866\n",
      "2024-02-14 23:32:43,636 INFO balancer.Balancer: 0 over-utilized: []\n",
      "2024-02-14 23:32:43,636 INFO balancer.Balancer: 0 underutilized: []\n",
      "The cluster is balanced. Exiting...\n",
      "Feb 14, 2024 11:32:43 PM          0                  0 B                 0 B                0 B  hdfs://rc1a-dataproc-m-zkmve6o6f689a4so.mdb.yandexcloud.net:8020\n",
      "Feb 14, 2024 11:32:43 PM Balancing took 946.0 milliseconds\n"
     ]
    }
   ],
   "source": [
    "! sudo -u hdfs hdfs balancer -threshold 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 13 items\r\n",
      "-rw-r--r--   1 ubuntu hadoop   94371561 2024-02-14 23:32 /user/tweets/data/IRAhandle_tweets_1.csv\r\n",
      "-rw-r--r--   1 ubuntu hadoop   94371615 2024-02-14 23:31 /user/tweets/data/IRAhandle_tweets_10.csv\r\n",
      "-rw-r--r--   1 ubuntu hadoop   94371552 2024-02-14 23:31 /user/tweets/data/IRAhandle_tweets_11.csv\r\n",
      "-rw-r--r--   1 ubuntu hadoop   94371703 2024-02-14 23:31 /user/tweets/data/IRAhandle_tweets_12.csv\r\n",
      "-rw-r--r--   1 ubuntu hadoop    8238864 2024-02-14 23:31 /user/tweets/data/IRAhandle_tweets_13.csv\r\n",
      "-rw-r--r--   1 ubuntu hadoop   94371748 2024-02-14 23:32 /user/tweets/data/IRAhandle_tweets_2.csv\r\n",
      "-rw-r--r--   1 ubuntu hadoop   94371796 2024-02-14 23:32 /user/tweets/data/IRAhandle_tweets_3.csv\r\n",
      "-rw-r--r--   1 ubuntu hadoop   94371606 2024-02-14 23:32 /user/tweets/data/IRAhandle_tweets_4.csv\r\n",
      "-rw-r--r--   1 ubuntu hadoop   94371616 2024-02-14 23:32 /user/tweets/data/IRAhandle_tweets_5.csv\r\n",
      "-rw-r--r--   1 ubuntu hadoop   94371646 2024-02-14 23:32 /user/tweets/data/IRAhandle_tweets_6.csv\r\n",
      "-rw-r--r--   1 ubuntu hadoop   94371711 2024-02-14 23:32 /user/tweets/data/IRAhandle_tweets_7.csv\r\n",
      "-rw-r--r--   1 ubuntu hadoop   94371727 2024-02-14 23:32 /user/tweets/data/IRAhandle_tweets_8.csv\r\n",
      "-rw-r--r--   1 ubuntu hadoop   94371542 2024-02-14 23:32 /user/tweets/data/IRAhandle_tweets_9.csv\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -ls /user/tweets/data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Запускаем MapReduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверяем, что скрипты на головной машине"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import sys\r\n",
      "import csv\r\n",
      "import re\r\n",
      "\r\n",
      "\r\n",
      "def mapper():\r\n",
      "    pattern = re.compile(r\"[a-z]+\")\r\n",
      "    for row in csv.reader(iter(sys.stdin.readline, '')):\r\n",
      "        content = row[2]\r\n",
      "        for match in pattern.finditer(content.lower()):\r\n",
      "            word = match.group(0)\r\n",
      "            print(\"{}\\t{}\".format(word, 1))\r\n",
      "\r\n",
      "\r\n",
      "def reducer():\r\n",
      "    word, number = next(sys.stdin).split('\\t')\r\n",
      "    number = int(number)\r\n",
      "    for line in sys.stdin:\r\n",
      "        current_word, current_number = line.split('\\t')\r\n",
      "        current_number = int(current_number)\r\n",
      "        if current_word != word:\r\n",
      "            print(\"{}\\t{}\".format(word, number))\r\n",
      "            word = current_word\r\n",
      "            number = current_number\r\n",
      "        else:\r\n",
      "            number += current_number\r\n",
      "    print(\"{}\\t{}\".format(word, number))\r\n",
      "\r\n",
      "\r\n",
      "if __name__ == '__main__':\r\n",
      "    mr_command = sys.argv[1]\r\n",
      "    {\r\n",
      "        'map': mapper,\r\n",
      "        'reduce': reducer\r\n",
      "    }[mr_command]()\r\n"
     ]
    }
   ],
   "source": [
    "! cat wordcount.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import sys\r\n",
      "\r\n",
      "\r\n",
      "def _rewind_stream(stream):\r\n",
      "    for _ in stream:\r\n",
      "        pass\r\n",
      "\r\n",
      "\r\n",
      "def mapper():\r\n",
      "    for row in sys.stdin:\r\n",
      "        key, value = row.split('\\t')\r\n",
      "        print(\"{}+{}\\t\".format(key, value.strip()))\r\n",
      "\r\n",
      "\r\n",
      "def reducer():\r\n",
      "    for _ in range(10):\r\n",
      "        key, _ = next(sys.stdin).split('\\t')\r\n",
      "        word, count = key.split(\"+\")\r\n",
      "        print(\"{}\\t{}\".format(word, count))\r\n",
      "    _rewind_stream(sys.stdin)\r\n",
      "\r\n",
      "if __name__ == '__main__':\r\n",
      "    mr_command = sys.argv[1]\r\n",
      "    {\r\n",
      "        'map': mapper,\r\n",
      "        'reduce': reducer\r\n",
      "    }[mr_command]()\r\n"
     ]
    }
   ],
   "source": [
    "! cat top10.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Собираем команду на запуск"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/lib/hadoop-mapreduce/hadoop-streaming.jar\r\n"
     ]
    }
   ],
   "source": [
    "! sudo find /usr/ -name hadoop-streaming.jar "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/lib/hadoop-mapreduce/hadoop-streaming.jar: symbolic link to hadoop-streaming-3.2.2.jar\r\n"
     ]
    }
   ],
   "source": [
    "! file /usr/lib/hadoop-mapreduce/hadoop-streaming.jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `/user/tweets/result': No such file or directory\n",
      "packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-3.2.2.jar] /tmp/streamjob8418463763561977212.jar tmpDir=null\n",
      "2024-02-14 23:34:41,952 INFO client.RMProxy: Connecting to ResourceManager at rc1a-dataproc-m-zkmve6o6f689a4so.mdb.yandexcloud.net/10.128.0.4:8032\n",
      "2024-02-14 23:34:42,121 INFO client.AHSProxy: Connecting to Application History server at rc1a-dataproc-m-zkmve6o6f689a4so.mdb.yandexcloud.net/10.128.0.4:10200\n",
      "2024-02-14 23:34:42,153 INFO client.RMProxy: Connecting to ResourceManager at rc1a-dataproc-m-zkmve6o6f689a4so.mdb.yandexcloud.net/10.128.0.4:8032\n",
      "2024-02-14 23:34:42,154 INFO client.AHSProxy: Connecting to Application History server at rc1a-dataproc-m-zkmve6o6f689a4so.mdb.yandexcloud.net/10.128.0.4:10200\n",
      "2024-02-14 23:34:42,348 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/ubuntu/.staging/job_1707952654779_0001\n",
      "2024-02-14 23:34:43,052 INFO mapred.FileInputFormat: Total input files to process : 13\n",
      "2024-02-14 23:34:43,162 INFO mapreduce.JobSubmitter: number of splits:13\n",
      "2024-02-14 23:34:43,420 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1707952654779_0001\n",
      "2024-02-14 23:34:43,421 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2024-02-14 23:34:43,620 INFO conf.Configuration: resource-types.xml not found\n",
      "2024-02-14 23:34:43,621 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2024-02-14 23:34:44,102 INFO impl.YarnClientImpl: Submitted application application_1707952654779_0001\n",
      "2024-02-14 23:34:44,135 INFO mapreduce.Job: The url to track the job: http://rc1a-dataproc-m-zkmve6o6f689a4so.mdb.yandexcloud.net:8088/proxy/application_1707952654779_0001/\n",
      "2024-02-14 23:34:44,136 INFO mapreduce.Job: Running job: job_1707952654779_0001\n",
      "2024-02-14 23:34:51,230 INFO mapreduce.Job: Job job_1707952654779_0001 running in uber mode : false\n",
      "2024-02-14 23:34:51,231 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2024-02-14 23:35:09,375 INFO mapreduce.Job:  map 18% reduce 0%\n",
      "2024-02-14 23:35:14,399 INFO mapreduce.Job:  map 21% reduce 0%\n",
      "2024-02-14 23:35:15,404 INFO mapreduce.Job:  map 23% reduce 0%\n",
      "2024-02-14 23:35:25,456 INFO mapreduce.Job:  map 28% reduce 0%\n",
      "2024-02-14 23:35:26,460 INFO mapreduce.Job:  map 31% reduce 0%\n",
      "2024-02-14 23:35:30,485 INFO mapreduce.Job:  map 36% reduce 0%\n",
      "2024-02-14 23:35:31,490 INFO mapreduce.Job:  map 44% reduce 0%\n",
      "2024-02-14 23:35:36,513 INFO mapreduce.Job:  map 46% reduce 0%\n",
      "2024-02-14 23:35:42,539 INFO mapreduce.Job:  map 51% reduce 0%\n",
      "2024-02-14 23:35:46,557 INFO mapreduce.Job:  map 62% reduce 0%\n",
      "2024-02-14 23:35:52,583 INFO mapreduce.Job:  map 62% reduce 7%\n",
      "2024-02-14 23:36:01,630 INFO mapreduce.Job:  map 74% reduce 7%\n",
      "2024-02-14 23:36:02,634 INFO mapreduce.Job:  map 77% reduce 7%\n",
      "2024-02-14 23:36:04,648 INFO mapreduce.Job:  map 77% reduce 9%\n",
      "2024-02-14 23:36:15,693 INFO mapreduce.Job:  map 85% reduce 9%\n",
      "2024-02-14 23:36:16,698 INFO mapreduce.Job:  map 92% reduce 9%\n",
      "2024-02-14 23:36:19,711 INFO mapreduce.Job:  map 100% reduce 9%\n",
      "2024-02-14 23:36:22,723 INFO mapreduce.Job:  map 100% reduce 14%\n",
      "2024-02-14 23:36:28,747 INFO mapreduce.Job:  map 100% reduce 22%\n",
      "2024-02-14 23:36:32,762 INFO mapreduce.Job:  map 100% reduce 45%\n",
      "2024-02-14 23:36:34,770 INFO mapreduce.Job:  map 100% reduce 49%\n",
      "2024-02-14 23:36:35,774 INFO mapreduce.Job:  map 100% reduce 77%\n",
      "2024-02-14 23:36:38,791 INFO mapreduce.Job:  map 100% reduce 82%\n",
      "2024-02-14 23:36:40,799 INFO mapreduce.Job:  map 100% reduce 86%\n",
      "2024-02-14 23:36:41,803 INFO mapreduce.Job:  map 100% reduce 92%\n",
      "2024-02-14 23:36:43,811 INFO mapreduce.Job:  map 100% reduce 97%\n",
      "2024-02-14 23:36:44,815 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "2024-02-14 23:36:45,823 INFO mapreduce.Job: Job job_1707952654779_0001 completed successfully\n",
      "2024-02-14 23:36:45,902 INFO mapreduce.Job: Counters: 54\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=32985838\n",
      "\t\tFILE: Number of bytes written=60861761\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=1140700654\n",
      "\t\tHDFS: Number of bytes written=30359819\n",
      "\t\tHDFS: Number of read operations=54\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=9\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=13\n",
      "\t\tLaunched reduce tasks=3\n",
      "\t\tData-local map tasks=13\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=605718\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=340419\n",
      "\t\tTotal time spent by all map tasks (ms)=201906\n",
      "\t\tTotal time spent by all reduce tasks (ms)=113473\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=201906\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=113473\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=620255232\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=348589056\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=2946207\n",
      "\t\tMap output records=41946754\n",
      "\t\tMap output bytes=313767903\n",
      "\t\tMap output materialized bytes=25332297\n",
      "\t\tInput split bytes=1967\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=2831736\n",
      "\t\tReduce shuffle bytes=25332297\n",
      "\t\tReduce input records=41946754\n",
      "\t\tReduce output records=2831736\n",
      "\t\tSpilled Records=106660897\n",
      "\t\tShuffled Maps =39\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=39\n",
      "\t\tGC time elapsed (ms)=2373\n",
      "\t\tCPU time spent (ms)=194070\n",
      "\t\tPhysical memory (bytes) snapshot=6052442112\n",
      "\t\tVirtual memory (bytes) snapshot=69448179712\n",
      "\t\tTotal committed heap usage (bytes)=6692012032\n",
      "\t\tPeak Map Physical memory (bytes)=380788736\n",
      "\t\tPeak Map Virtual memory (bytes)=4344307712\n",
      "\t\tPeak Reduce Physical memory (bytes)=443539456\n",
      "\t\tPeak Reduce Virtual memory (bytes)=4364943360\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=1140698687\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=30359819\n",
      "2024-02-14 23:36:45,902 INFO streaming.StreamJob: Output directory: /user/tweets/result/\n",
      "CPU times: user 2.22 s, sys: 371 ms, total: 2.59 s\n",
      "Wall time: 2min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "! hdfs dfs -rm -r /user/tweets/result || true\n",
    "! yarn jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "-D mapreduce.job.name=\"word-count\" \\\n",
    "-D mapreduce.job.reduces=3 \\\n",
    "-files wordcount.py \\\n",
    "-mapper \"python3 wordcount.py map\" \\\n",
    "-reducer \"python3 wordcount.py reduce\" \\\n",
    "-input /user/tweets/data/ \\\n",
    "-output /user/tweets/result/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Помимо того, что можно следить здесь в терминале, за выполнением можно наблюдать через UI-proxy в интерфейсе облака"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Смотрим результат\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 items\r\n",
      "-rw-r--r--   1 ubuntu hadoop          0 2023-01-24 16:02 /user/tweets/result/_SUCCESS\r\n",
      "-rw-r--r--   1 ubuntu hadoop   10107405 2023-01-24 16:02 /user/tweets/result/part-00000\r\n",
      "-rw-r--r--   1 ubuntu hadoop   10134121 2023-01-24 16:02 /user/tweets/result/part-00001\r\n",
      "-rw-r--r--   1 ubuntu hadoop   10118293 2023-01-24 16:02 /user/tweets/result/part-00002\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -ls /user/tweets/result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aa\t1726\r\n",
      "aaaaa\t7\r\n",
      "aaaaaaaaaaaaaa\t3\r\n",
      "aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaannnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnddddddddddddddddddddddddddddddddddddd\t1\r\n",
      "aaaaaaaaaaaaaaaaaaah\t1\r\n",
      "aaaaaaaaaaaaand\t1\r\n",
      "aaaaaaaaannnnnnnnnnnddddddddddddd\t1\r\n",
      "aaaaaaaah\t1\r\n",
      "aaaaaaaamen\t1\r\n",
      "aaaaaaagh\t2\r\n",
      "cat: Unable to write to output stream.\r\n",
      "cat: Unable to write to output stream.\r\n",
      "cat: Unable to write to output stream.\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -cat /user/tweets/result/part-* | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запустим вторую задачу "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `/user/tweets/top10/': No such file or directory\n",
      "packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-3.2.2.jar] /tmp/streamjob6913228636215651592.jar tmpDir=null\n",
      "2024-02-15 00:28:53,003 INFO client.RMProxy: Connecting to ResourceManager at rc1a-dataproc-m-zkmve6o6f689a4so.mdb.yandexcloud.net/10.128.0.4:8032\n",
      "2024-02-15 00:28:53,170 INFO client.AHSProxy: Connecting to Application History server at rc1a-dataproc-m-zkmve6o6f689a4so.mdb.yandexcloud.net/10.128.0.4:10200\n",
      "2024-02-15 00:28:53,197 INFO client.RMProxy: Connecting to ResourceManager at rc1a-dataproc-m-zkmve6o6f689a4so.mdb.yandexcloud.net/10.128.0.4:8032\n",
      "2024-02-15 00:28:53,198 INFO client.AHSProxy: Connecting to Application History server at rc1a-dataproc-m-zkmve6o6f689a4so.mdb.yandexcloud.net/10.128.0.4:10200\n",
      "2024-02-15 00:28:53,354 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/ubuntu/.staging/job_1707952654779_0002\n",
      "2024-02-15 00:28:53,618 INFO mapred.FileInputFormat: Total input files to process : 3\n",
      "2024-02-15 00:28:53,683 INFO mapreduce.JobSubmitter: number of splits:12\n",
      "2024-02-15 00:28:53,817 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1707952654779_0002\n",
      "2024-02-15 00:28:53,818 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2024-02-15 00:28:53,987 INFO conf.Configuration: resource-types.xml not found\n",
      "2024-02-15 00:28:53,988 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2024-02-15 00:28:54,046 INFO impl.YarnClientImpl: Submitted application application_1707952654779_0002\n",
      "2024-02-15 00:28:54,087 INFO mapreduce.Job: The url to track the job: http://rc1a-dataproc-m-zkmve6o6f689a4so.mdb.yandexcloud.net:8088/proxy/application_1707952654779_0002/\n",
      "2024-02-15 00:28:54,089 INFO mapreduce.Job: Running job: job_1707952654779_0002\n",
      "2024-02-15 00:29:00,163 INFO mapreduce.Job: Job job_1707952654779_0002 running in uber mode : false\n",
      "2024-02-15 00:29:00,164 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2024-02-15 00:29:10,251 INFO mapreduce.Job:  map 17% reduce 0%\n",
      "2024-02-15 00:29:11,257 INFO mapreduce.Job:  map 25% reduce 0%\n",
      "2024-02-15 00:29:18,294 INFO mapreduce.Job:  map 33% reduce 0%\n",
      "2024-02-15 00:29:19,299 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "2024-02-15 00:29:26,332 INFO mapreduce.Job:  map 58% reduce 0%\n",
      "2024-02-15 00:29:27,337 INFO mapreduce.Job:  map 67% reduce 0%\n",
      "2024-02-15 00:29:30,356 INFO mapreduce.Job:  map 75% reduce 0%\n",
      "2024-02-15 00:29:32,365 INFO mapreduce.Job:  map 83% reduce 0%\n",
      "2024-02-15 00:29:34,374 INFO mapreduce.Job:  map 92% reduce 0%\n",
      "2024-02-15 00:29:36,387 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "2024-02-15 00:29:42,414 INFO mapreduce.Job:  map 100% reduce 79%\n",
      "2024-02-15 00:29:43,420 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "2024-02-15 00:29:43,426 INFO mapreduce.Job: Job job_1707952654779_0002 completed successfully\n",
      "2024-02-15 00:29:43,493 INFO mapreduce.Job: Counters: 55\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=13564266\n",
      "\t\tFILE: Number of bytes written=31027076\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=31347773\n",
      "\t\tHDFS: Number of bytes written=106\n",
      "\t\tHDFS: Number of read operations=41\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=3\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tKilled map tasks=1\n",
      "\t\tLaunched map tasks=12\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=12\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=248820\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=47148\n",
      "\t\tTotal time spent by all map tasks (ms)=82940\n",
      "\t\tTotal time spent by all reduce tasks (ms)=15716\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=82940\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=15716\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=254791680\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=48279552\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=2831736\n",
      "\t\tMap output records=2831736\n",
      "\t\tMap output bytes=33191556\n",
      "\t\tMap output materialized bytes=14315030\n",
      "\t\tInput split bytes=1692\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=2831736\n",
      "\t\tReduce shuffle bytes=14315030\n",
      "\t\tReduce input records=2831736\n",
      "\t\tReduce output records=10\n",
      "\t\tSpilled Records=5663472\n",
      "\t\tShuffled Maps =12\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=12\n",
      "\t\tGC time elapsed (ms)=2260\n",
      "\t\tCPU time spent (ms)=63940\n",
      "\t\tPhysical memory (bytes) snapshot=6950387712\n",
      "\t\tVirtual memory (bytes) snapshot=56435351552\n",
      "\t\tTotal committed heap usage (bytes)=7648313344\n",
      "\t\tPeak Map Physical memory (bytes)=636485632\n",
      "\t\tPeak Map Virtual memory (bytes)=4357509120\n",
      "\t\tPeak Reduce Physical memory (bytes)=602157056\n",
      "\t\tPeak Reduce Virtual memory (bytes)=4357926912\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=31346081\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=106\n",
      "2024-02-15 00:29:43,493 INFO streaming.StreamJob: Output directory: /user/tweets/top10/\n",
      "CPU times: user 916 ms, sys: 163 ms, total: 1.08 s\n",
      "Wall time: 53.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "! hdfs dfs -rm -r /user/tweets/top10/\n",
    "! yarn jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "-D mapreduce.job.name=\"top-10\" \\\n",
    "-D mapreduce.job.reduces=1 \\\n",
    "-D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapreduce.lib.partition.KeyFieldBasedComparator \\\n",
    "-D mapreduce.partition.keycomparator.options=\"-k2,2nr -k1,1\" \\\n",
    "-D mapreduce.map.output.key.field.separator='+' \\\n",
    "-files top10.py \\\n",
    "-mapper \"python top10.py map\" \\\n",
    "-reducer \"python top10.py reduce\" \\\n",
    "-input /user/tweets/result/ \\\n",
    "-output /user/tweets/top10/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\r\n",
      "-rw-r--r--   1 ubuntu hadoop          0 2024-02-15 00:29 /user/tweets/top10/_SUCCESS\r\n",
      "-rw-r--r--   1 ubuntu hadoop        106 2024-02-15 00:29 /user/tweets/top10/part-00000\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -ls /user/tweets/top10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t\t3015051\r\n",
      "co\t2833375\r\n",
      "https\t2454132\r\n",
      "the\t591885\r\n",
      "to\t589004\r\n",
      "in\t457433\r\n",
      "a\t412888\r\n",
      "s\t397889\r\n",
      "http\t375299\r\n",
      "of\t350983\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -cat /user/tweets/top10/part-*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distributed cache\n",
    "\n",
    "Помимо самого скрипта, мы можем положить в MapReduce любой другой файл, который может пригодиться для работы программы. Например при подсчете количества слов мы бы хотели выкинуть \"стоп-слова\". Их количество скорее всего не очень большое поэтому смело может передавать их обычным файлом. Hadoop гарантирует, что доставит все файлы ко всем машинам."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "! hdfs dfs -cat /user/tweets/top10/part-* > stop-words.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Хозяйке на заметку\n",
    "\n",
    "В питоне уже есть хорошая стандартная библиотека, которая позволяет вам гораздо удобнее работать с такимим стримовыми данными. Давайте напишем новую задачу со стоп словами, чтобы они смотрелись поприличнее."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing wordcount2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile wordcount2.py\n",
    "\n",
    "import sys\n",
    "import csv\n",
    "import re\n",
    "from itertools import groupby\n",
    "\n",
    "\n",
    "def csv_stream():\n",
    "    return csv.reader(iter(sys.stdin.readline, ''))\n",
    "\n",
    "def kv_stream(sep=\"\\t\"):\n",
    "    return map(lambda x: x.split(sep), sys.stdin)\n",
    "\n",
    "\n",
    "def mapper():\n",
    "    pattern = re.compile(r\"[a-z]+\")\n",
    "    for row in csv_stream():\n",
    "        content = row[2]\n",
    "        for match in pattern.finditer(content.lower()):\n",
    "            word = match.group(0)\n",
    "            print(\"{}\\t{}\".format(word, 1))\n",
    "\n",
    "\n",
    "def reducer():\n",
    "    for key, group in groupby(kv_stream(), lambda x: x[0]):\n",
    "        word = key\n",
    "        number = sum(int(x) for _, x in group)\n",
    "        print(\"{}\\t{}\".format(word, number))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    mr_command = sys.argv[1]\n",
    "    {\n",
    "        'map': mapper,\n",
    "        'reduce': reducer\n",
    "    }[mr_command]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing top10-2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile top10-2.py\n",
    "\n",
    "import sys\n",
    "import collections\n",
    "from itertools import islice\n",
    "\n",
    "def build_stop_words():\n",
    "    with open('stop-words.txt', 'r') as f:\n",
    "        stop_words = {x.split('\\t')[0] for x in f}\n",
    "    return stop_words\n",
    "\n",
    "def kv_stream(sep=\"\\t\"):\n",
    "    return map(lambda x: x.split(sep), sys.stdin)\n",
    "\n",
    "def rewind():\n",
    "    collections.deque(sys.stdin, maxlen=0)\n",
    "\n",
    "def mapper():\n",
    "    for key, value in kv_stream():\n",
    "        print(\"{}+{}\\t\".format(key, value.strip()))\n",
    "\n",
    "def reducer():\n",
    "    stop_words = build_stop_words()\n",
    "    first_10_stream = islice(filter(lambda x: x[0] not in stop_words, kv_stream('+')), 10)\n",
    "    \n",
    "    for word, count in first_10_stream:\n",
    "        print(\"{}\\t{}\".format(word, count.strip()))\n",
    "    rewind()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    mr_command = sys.argv[1]\n",
    "    {\n",
    "        'map': mapper,\n",
    "        'reduce': reducer\n",
    "    }[mr_command]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t\t3015051\r\n",
      "co\t2833375\r\n",
      "https\t2454132\r\n",
      "the\t591885\r\n",
      "to\t589004\r\n",
      "in\t457433\r\n",
      "a\t412888\r\n",
      "s\t397889\r\n",
      "http\t375299\r\n",
      "of\t350983\r\n"
     ]
    }
   ],
   "source": [
    "! cat stop-words.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `/user/tweets/top10-stop-words/': No such file or directory\n",
      "packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-3.2.2.jar] /tmp/streamjob8995177600677172941.jar tmpDir=null\n",
      "2024-02-15 00:30:05,544 INFO client.RMProxy: Connecting to ResourceManager at rc1a-dataproc-m-zkmve6o6f689a4so.mdb.yandexcloud.net/10.128.0.4:8032\n",
      "2024-02-15 00:30:05,711 INFO client.AHSProxy: Connecting to Application History server at rc1a-dataproc-m-zkmve6o6f689a4so.mdb.yandexcloud.net/10.128.0.4:10200\n",
      "2024-02-15 00:30:05,736 INFO client.RMProxy: Connecting to ResourceManager at rc1a-dataproc-m-zkmve6o6f689a4so.mdb.yandexcloud.net/10.128.0.4:8032\n",
      "2024-02-15 00:30:05,737 INFO client.AHSProxy: Connecting to Application History server at rc1a-dataproc-m-zkmve6o6f689a4so.mdb.yandexcloud.net/10.128.0.4:10200\n",
      "2024-02-15 00:30:05,900 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/ubuntu/.staging/job_1707952654779_0003\n",
      "2024-02-15 00:30:06,584 INFO mapred.FileInputFormat: Total input files to process : 3\n",
      "2024-02-15 00:30:06,634 INFO mapreduce.JobSubmitter: number of splits:12\n",
      "2024-02-15 00:30:06,755 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1707952654779_0003\n",
      "2024-02-15 00:30:06,757 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2024-02-15 00:30:06,925 INFO conf.Configuration: resource-types.xml not found\n",
      "2024-02-15 00:30:06,926 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2024-02-15 00:30:06,980 INFO impl.YarnClientImpl: Submitted application application_1707952654779_0003\n",
      "2024-02-15 00:30:07,016 INFO mapreduce.Job: The url to track the job: http://rc1a-dataproc-m-zkmve6o6f689a4so.mdb.yandexcloud.net:8088/proxy/application_1707952654779_0003/\n",
      "2024-02-15 00:30:07,018 INFO mapreduce.Job: Running job: job_1707952654779_0003\n",
      "2024-02-15 00:30:12,124 INFO mapreduce.Job: Job job_1707952654779_0003 running in uber mode : false\n",
      "2024-02-15 00:30:12,125 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2024-02-15 00:30:22,216 INFO mapreduce.Job:  map 8% reduce 0%\n",
      "2024-02-15 00:30:23,224 INFO mapreduce.Job:  map 25% reduce 0%\n",
      "2024-02-15 00:30:31,267 INFO mapreduce.Job:  map 42% reduce 0%\n",
      "2024-02-15 00:30:32,273 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "2024-02-15 00:30:38,303 INFO mapreduce.Job:  map 58% reduce 0%\n",
      "2024-02-15 00:30:40,313 INFO mapreduce.Job:  map 67% reduce 0%\n",
      "2024-02-15 00:30:42,323 INFO mapreduce.Job:  map 75% reduce 0%\n",
      "2024-02-15 00:30:45,337 INFO mapreduce.Job:  map 83% reduce 0%\n",
      "2024-02-15 00:30:48,351 INFO mapreduce.Job:  map 92% reduce 0%\n",
      "2024-02-15 00:30:49,356 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "2024-02-15 00:30:54,378 INFO mapreduce.Job:  map 100% reduce 69%\n",
      "2024-02-15 00:30:57,392 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "2024-02-15 00:30:57,398 INFO mapreduce.Job: Job job_1707952654779_0003 completed successfully\n",
      "2024-02-15 00:30:57,465 INFO mapreduce.Job: Counters: 55\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=13564266\n",
      "\t\tFILE: Number of bytes written=31031249\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=31347773\n",
      "\t\tHDFS: Number of bytes written=109\n",
      "\t\tHDFS: Number of read operations=41\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=3\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tKilled map tasks=1\n",
      "\t\tLaunched map tasks=12\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=12\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=251526\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=53430\n",
      "\t\tTotal time spent by all map tasks (ms)=83842\n",
      "\t\tTotal time spent by all reduce tasks (ms)=17810\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=83842\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=17810\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=257562624\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=54712320\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=2831736\n",
      "\t\tMap output records=2831736\n",
      "\t\tMap output bytes=33191556\n",
      "\t\tMap output materialized bytes=14315030\n",
      "\t\tInput split bytes=1692\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=2831736\n",
      "\t\tReduce shuffle bytes=14315030\n",
      "\t\tReduce input records=2831736\n",
      "\t\tReduce output records=10\n",
      "\t\tSpilled Records=5663472\n",
      "\t\tShuffled Maps =12\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=12\n",
      "\t\tGC time elapsed (ms)=1986\n",
      "\t\tCPU time spent (ms)=66680\n",
      "\t\tPhysical memory (bytes) snapshot=6833750016\n",
      "\t\tVirtual memory (bytes) snapshot=56414224384\n",
      "\t\tTotal committed heap usage (bytes)=7640973312\n",
      "\t\tPeak Map Physical memory (bytes)=668930048\n",
      "\t\tPeak Map Virtual memory (bytes)=4340924416\n",
      "\t\tPeak Reduce Physical memory (bytes)=665083904\n",
      "\t\tPeak Reduce Virtual memory (bytes)=4402630656\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=31346081\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=109\n",
      "2024-02-15 00:30:57,466 INFO streaming.StreamJob: Output directory: /user/tweets/top10-stop-words/\n",
      "CPU times: user 917 ms, sys: 221 ms, total: 1.14 s\n",
      "Wall time: 55.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "! hdfs dfs -rm -r /user/tweets/top10-stop-words/ || true\n",
    "! yarn jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "-D mapreduce.job.name=\"top-10-stop-words\" \\\n",
    "-D mapreduce.job.reduces=1 \\\n",
    "-D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapreduce.lib.partition.KeyFieldBasedComparator \\\n",
    "-D mapreduce.partition.keycomparator.options=\"-k2,2nr -k1,1\" \\\n",
    "-D mapreduce.map.output.key.field.separator='+' \\\n",
    "-files top10-2.py,stop-words.txt \\\n",
    "-mapper \"python top10-2.py map\" \\\n",
    "-reducer \"python top10-2.py reduce\" \\\n",
    "-input /user/tweets/result/ \\\n",
    "-output /user/tweets/top10-stop-words/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i\t287232\r\n",
      "for\t272995\r\n",
      "and\t247749\r\n",
      "is\t246856\r\n",
      "on\t210172\r\n",
      "you\t196950\r\n",
      "trump\t169520\r\n",
      "news\t156101\r\n",
      "it\t152816\r\n",
      "with\t134178\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -cat /user/tweets/top10-stop-words/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ускоряем вычисления \n",
    "\n",
    "Несмотря на все оптимизации внутри Hadoop, самое узкое место - это передача данных от mapper к reducer. Таким образом если у нас получиться ускорить передачи данных в этом месте, мы сможем сильно ускорить весь процесс ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing wordcount3.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile wordcount3.py\n",
    "\n",
    "import sys\n",
    "import csv\n",
    "import re\n",
    "from itertools import groupby\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "def csv_stream():\n",
    "    return csv.reader(iter(sys.stdin.readline, ''))\n",
    "\n",
    "def kv_stream(sep=\"\\t\"):\n",
    "    return map(lambda x: x.split(sep), sys.stdin)\n",
    "\n",
    "\n",
    "def mapper():\n",
    "    counter = Counter()\n",
    "    pattern = re.compile(r\"[a-z]+\")\n",
    "    for row in csv_stream():\n",
    "        content = row[2]\n",
    "        for match in pattern.finditer(content.lower()):\n",
    "            word = match.group(0)\n",
    "            counter[word] += 1\n",
    "    \n",
    "    for word, number in counter.items():\n",
    "        print(\"{}\\t{}\".format(word, number))\n",
    "\n",
    "\n",
    "def reducer():\n",
    "    for key, group in groupby(kv_stream(), lambda x: x[0]):\n",
    "        word = key\n",
    "        number = sum(int(x) for _, x in group)\n",
    "        print(\"{}\\t{}\".format(word, number))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    mr_command = sys.argv[1]\n",
    "    {\n",
    "        'map': mapper,\n",
    "        'reduce': reducer\n",
    "    }[mr_command]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `/user/tweets/result-fast1': No such file or directory\n",
      "packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-3.2.2.jar] /tmp/streamjob7964666790986098633.jar tmpDir=null\n",
      "2024-02-15 00:31:35,584 INFO client.RMProxy: Connecting to ResourceManager at rc1a-dataproc-m-zkmve6o6f689a4so.mdb.yandexcloud.net/10.128.0.4:8032\n",
      "2024-02-15 00:31:35,761 INFO client.AHSProxy: Connecting to Application History server at rc1a-dataproc-m-zkmve6o6f689a4so.mdb.yandexcloud.net/10.128.0.4:10200\n",
      "2024-02-15 00:31:35,787 INFO client.RMProxy: Connecting to ResourceManager at rc1a-dataproc-m-zkmve6o6f689a4so.mdb.yandexcloud.net/10.128.0.4:8032\n",
      "2024-02-15 00:31:35,787 INFO client.AHSProxy: Connecting to Application History server at rc1a-dataproc-m-zkmve6o6f689a4so.mdb.yandexcloud.net/10.128.0.4:10200\n",
      "2024-02-15 00:31:35,956 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/ubuntu/.staging/job_1707952654779_0004\n",
      "2024-02-15 00:31:36,219 INFO mapred.FileInputFormat: Total input files to process : 13\n",
      "2024-02-15 00:31:36,276 INFO mapreduce.JobSubmitter: number of splits:13\n",
      "2024-02-15 00:31:36,394 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1707952654779_0004\n",
      "2024-02-15 00:31:36,396 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2024-02-15 00:31:36,571 INFO conf.Configuration: resource-types.xml not found\n",
      "2024-02-15 00:31:36,571 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2024-02-15 00:31:36,634 INFO impl.YarnClientImpl: Submitted application application_1707952654779_0004\n",
      "2024-02-15 00:31:36,660 INFO mapreduce.Job: The url to track the job: http://rc1a-dataproc-m-zkmve6o6f689a4so.mdb.yandexcloud.net:8088/proxy/application_1707952654779_0004/\n",
      "2024-02-15 00:31:36,661 INFO mapreduce.Job: Running job: job_1707952654779_0004\n",
      "2024-02-15 00:31:41,725 INFO mapreduce.Job: Job job_1707952654779_0004 running in uber mode : false\n",
      "2024-02-15 00:31:41,726 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2024-02-15 00:31:55,836 INFO mapreduce.Job:  map 8% reduce 0%\n",
      "2024-02-15 00:31:56,842 INFO mapreduce.Job:  map 23% reduce 0%\n",
      "2024-02-15 00:32:08,909 INFO mapreduce.Job:  map 31% reduce 0%\n",
      "2024-02-15 00:32:09,915 INFO mapreduce.Job:  map 46% reduce 0%\n",
      "2024-02-15 00:32:21,977 INFO mapreduce.Job:  map 54% reduce 0%\n",
      "2024-02-15 00:32:22,982 INFO mapreduce.Job:  map 62% reduce 0%\n",
      "2024-02-15 00:32:23,987 INFO mapreduce.Job:  map 69% reduce 0%\n",
      "2024-02-15 00:32:35,040 INFO mapreduce.Job:  map 85% reduce 0%\n",
      "2024-02-15 00:32:37,050 INFO mapreduce.Job:  map 85% reduce 9%\n",
      "2024-02-15 00:32:40,065 INFO mapreduce.Job:  map 92% reduce 9%\n",
      "2024-02-15 00:32:43,078 INFO mapreduce.Job:  map 92% reduce 10%\n",
      "2024-02-15 00:32:45,088 INFO mapreduce.Job:  map 100% reduce 10%\n",
      "2024-02-15 00:32:49,104 INFO mapreduce.Job:  map 100% reduce 26%\n",
      "2024-02-15 00:32:52,117 INFO mapreduce.Job:  map 100% reduce 67%\n",
      "2024-02-15 00:32:54,126 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "2024-02-15 00:32:54,133 INFO mapreduce.Job: Job job_1707952654779_0004 completed successfully\n",
      "2024-02-15 00:32:54,203 INFO mapreduce.Job: Counters: 55\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=16980366\n",
      "\t\tFILE: Number of bytes written=43922540\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=1140700654\n",
      "\t\tHDFS: Number of bytes written=30359819\n",
      "\t\tHDFS: Number of read operations=54\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=9\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tKilled map tasks=1\n",
      "\t\tLaunched map tasks=13\n",
      "\t\tLaunched reduce tasks=3\n",
      "\t\tData-local map tasks=13\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=429144\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=141210\n",
      "\t\tTotal time spent by all map tasks (ms)=143048\n",
      "\t\tTotal time spent by all reduce tasks (ms)=47070\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=143048\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=47070\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=439443456\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=144599040\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=2946207\n",
      "\t\tMap output records=4292696\n",
      "\t\tMap output bytes=44547794\n",
      "\t\tMap output materialized bytes=23076288\n",
      "\t\tInput split bytes=1967\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=2831736\n",
      "\t\tReduce shuffle bytes=23076288\n",
      "\t\tReduce input records=4292696\n",
      "\t\tReduce output records=2831736\n",
      "\t\tSpilled Records=8585392\n",
      "\t\tShuffled Maps =39\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=39\n",
      "\t\tGC time elapsed (ms)=1947\n",
      "\t\tCPU time spent (ms)=62640\n",
      "\t\tPhysical memory (bytes) snapshot=5588287488\n",
      "\t\tVirtual memory (bytes) snapshot=69434937344\n",
      "\t\tTotal committed heap usage (bytes)=5626134528\n",
      "\t\tPeak Map Physical memory (bytes)=378966016\n",
      "\t\tPeak Map Virtual memory (bytes)=4340436992\n",
      "\t\tPeak Reduce Physical memory (bytes)=310878208\n",
      "\t\tPeak Reduce Virtual memory (bytes)=4363526144\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=1140698687\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=30359819\n",
      "2024-02-15 00:32:54,203 INFO streaming.StreamJob: Output directory: /user/tweets/result-fast1/\n",
      "CPU times: user 1.4 s, sys: 256 ms, total: 1.65 s\n",
      "Wall time: 1min 21s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "! hdfs dfs -rm -r /user/tweets/result-fast1 || true\n",
    "! yarn jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "-D mapreduce.job.name=\"word-count\" \\\n",
    "-D mapreduce.job.reduces=3 \\\n",
    "-files wordcount3.py \\\n",
    "-mapper \"python3 wordcount3.py map\" \\\n",
    "-reducer \"python3 wordcount3.py reduce\" \\\n",
    "-input /user/tweets/data/ \\\n",
    "-output /user/tweets/result-fast1/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aa\t1726\n",
      "aaaaa\t7\n",
      "aaaaaaaaaaaaaa\t3\n",
      "aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaannnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnddddddddddddddddddddddddddddddddddddd\t1\n",
      "aaaaaaaaaaaaaaaaaaah\t1\n",
      "aaaaaaaaaaaaand\t1\n",
      "aaaaaaaaannnnnnnnnnnddddddddddddd\t1\n",
      "aaaaaaaah\t1\n",
      "aaaaaaaamen\t1\n",
      "aaaaaaagh\t2\n",
      "cat: Unable to write to output stream.\n",
      "cat: Unable to write to output stream.\n",
      "cat: Unable to write to output stream.\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -cat /user/tweets/result-fast1/* | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Однако у этого решения есть **очень большой минус** - сложность по памяти **O(n)**. Это означает, что вычисление может упасть если данные попадутся неудачные. \n",
    "\n",
    "Важный принцип работы с большими данными - все алгоритмы должны работать меньше чем за O(n). Это относится не только к MapReduce, а в целом почти к любым инструментам обработки больших данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5 items\r\n",
      "drwxr-xr-x   - ubuntu hadoop          0 2024-02-14 23:32 /user/tweets/data\r\n",
      "drwxr-xr-x   - ubuntu hadoop          0 2024-02-14 23:36 /user/tweets/result\r\n",
      "drwxr-xr-x   - ubuntu hadoop          0 2024-02-15 00:32 /user/tweets/result-fast1\r\n",
      "drwxr-xr-x   - ubuntu hadoop          0 2024-02-15 00:29 /user/tweets/top10\r\n",
      "drwxr-xr-x   - ubuntu hadoop          0 2024-02-15 00:30 /user/tweets/top10-stop-words\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -ls /user/tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Используем комбайнер\n",
    "\n",
    "Чтобы побороться с этой бедой, воспользуемся дополнительным инструментом в Hadoop - Combiner. По сути это маленький Reduce, который запускается после маппера. Это позволяет уменьшить количество выходных данных с Map стадии."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://habrastorage.org/getpro/habr/post_images/587/2d2/dfe/5872d2dfe12643665370708d225bc1d4.jpg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `/user/tweets/result-fast2': No such file or directory\n",
      "packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-3.2.2.jar] /tmp/streamjob7451730125063543791.jar tmpDir=null\n",
      "2024-02-15 00:35:00,627 INFO client.RMProxy: Connecting to ResourceManager at rc1a-dataproc-m-zkmve6o6f689a4so.mdb.yandexcloud.net/10.128.0.4:8032\n",
      "2024-02-15 00:35:00,806 INFO client.AHSProxy: Connecting to Application History server at rc1a-dataproc-m-zkmve6o6f689a4so.mdb.yandexcloud.net/10.128.0.4:10200\n",
      "2024-02-15 00:35:00,830 INFO client.RMProxy: Connecting to ResourceManager at rc1a-dataproc-m-zkmve6o6f689a4so.mdb.yandexcloud.net/10.128.0.4:8032\n",
      "2024-02-15 00:35:00,831 INFO client.AHSProxy: Connecting to Application History server at rc1a-dataproc-m-zkmve6o6f689a4so.mdb.yandexcloud.net/10.128.0.4:10200\n",
      "2024-02-15 00:35:01,029 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/ubuntu/.staging/job_1707952654779_0005\n",
      "2024-02-15 00:35:01,401 INFO mapred.FileInputFormat: Total input files to process : 13\n",
      "2024-02-15 00:35:01,533 INFO mapreduce.JobSubmitter: number of splits:13\n",
      "2024-02-15 00:35:01,683 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1707952654779_0005\n",
      "2024-02-15 00:35:01,684 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2024-02-15 00:35:01,842 INFO conf.Configuration: resource-types.xml not found\n",
      "2024-02-15 00:35:01,843 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2024-02-15 00:35:01,891 INFO impl.YarnClientImpl: Submitted application application_1707952654779_0005\n",
      "2024-02-15 00:35:01,919 INFO mapreduce.Job: The url to track the job: http://rc1a-dataproc-m-zkmve6o6f689a4so.mdb.yandexcloud.net:8088/proxy/application_1707952654779_0005/\n",
      "2024-02-15 00:35:01,920 INFO mapreduce.Job: Running job: job_1707952654779_0005\n",
      "2024-02-15 00:35:07,988 INFO mapreduce.Job: Job job_1707952654779_0005 running in uber mode : false\n",
      "2024-02-15 00:35:07,989 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2024-02-15 00:35:25,117 INFO mapreduce.Job:  map 10% reduce 0%\n",
      "2024-02-15 00:35:26,122 INFO mapreduce.Job:  map 15% reduce 0%\n",
      "2024-02-15 00:35:28,141 INFO mapreduce.Job:  map 18% reduce 0%\n",
      "2024-02-15 00:35:34,172 INFO mapreduce.Job:  map 23% reduce 0%\n",
      "2024-02-15 00:35:44,224 INFO mapreduce.Job:  map 28% reduce 0%\n",
      "2024-02-15 00:35:49,250 INFO mapreduce.Job:  map 31% reduce 0%\n",
      "2024-02-15 00:35:50,254 INFO mapreduce.Job:  map 41% reduce 0%\n",
      "2024-02-15 00:35:55,282 INFO mapreduce.Job:  map 44% reduce 0%\n",
      "2024-02-15 00:35:59,302 INFO mapreduce.Job:  map 46% reduce 0%\n",
      "2024-02-15 00:36:05,335 INFO mapreduce.Job:  map 51% reduce 0%\n",
      "2024-02-15 00:36:11,363 INFO mapreduce.Job:  map 54% reduce 0%\n",
      "2024-02-15 00:36:12,369 INFO mapreduce.Job:  map 59% reduce 0%\n",
      "2024-02-15 00:36:13,377 INFO mapreduce.Job:  map 62% reduce 0%\n",
      "2024-02-15 00:36:15,386 INFO mapreduce.Job:  map 62% reduce 7%\n",
      "2024-02-15 00:36:26,442 INFO mapreduce.Job:  map 67% reduce 7%\n",
      "2024-02-15 00:36:28,451 INFO mapreduce.Job:  map 74% reduce 7%\n",
      "2024-02-15 00:36:30,460 INFO mapreduce.Job:  map 77% reduce 7%\n",
      "2024-02-15 00:36:33,476 INFO mapreduce.Job:  map 77% reduce 9%\n",
      "2024-02-15 00:36:43,519 INFO mapreduce.Job:  map 82% reduce 9%\n",
      "2024-02-15 00:36:44,523 INFO mapreduce.Job:  map 92% reduce 9%\n",
      "2024-02-15 00:36:45,527 INFO mapreduce.Job:  map 92% reduce 10%\n",
      "2024-02-15 00:36:48,539 INFO mapreduce.Job:  map 100% reduce 10%\n",
      "2024-02-15 00:36:51,552 INFO mapreduce.Job:  map 100% reduce 22%\n",
      "2024-02-15 00:36:55,571 INFO mapreduce.Job:  map 100% reduce 33%\n",
      "2024-02-15 00:36:56,575 INFO mapreduce.Job:  map 100% reduce 67%\n",
      "2024-02-15 00:36:58,582 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "2024-02-15 00:36:58,588 INFO mapreduce.Job: Job job_1707952654779_0005 completed successfully\n",
      "2024-02-15 00:36:58,664 INFO mapreduce.Job: Counters: 55\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=32343116\n",
      "\t\tFILE: Number of bytes written=58215861\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=1140700654\n",
      "\t\tHDFS: Number of bytes written=30359819\n",
      "\t\tHDFS: Number of read operations=54\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=9\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tKilled map tasks=1\n",
      "\t\tLaunched map tasks=13\n",
      "\t\tLaunched reduce tasks=3\n",
      "\t\tData-local map tasks=13\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=700533\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=217911\n",
      "\t\tTotal time spent by all map tasks (ms)=233511\n",
      "\t\tTotal time spent by all reduce tasks (ms)=72637\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=233511\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=72637\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=717345792\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=223140864\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=2946207\n",
      "\t\tMap output records=41946754\n",
      "\t\tMap output bytes=313767903\n",
      "\t\tMap output materialized bytes=23426815\n",
      "\t\tInput split bytes=1967\n",
      "\t\tCombine input records=41946754\n",
      "\t\tCombine output records=4408900\n",
      "\t\tReduce input groups=2831736\n",
      "\t\tReduce shuffle bytes=23426815\n",
      "\t\tReduce input records=4408900\n",
      "\t\tReduce output records=2831736\n",
      "\t\tSpilled Records=11333992\n",
      "\t\tShuffled Maps =39\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=39\n",
      "\t\tGC time elapsed (ms)=1938\n",
      "\t\tCPU time spent (ms)=155610\n",
      "\t\tPhysical memory (bytes) snapshot=5612376064\n",
      "\t\tVirtual memory (bytes) snapshot=69451120640\n",
      "\t\tTotal committed heap usage (bytes)=6346506240\n",
      "\t\tPeak Map Physical memory (bytes)=383750144\n",
      "\t\tPeak Map Virtual memory (bytes)=4359618560\n",
      "\t\tPeak Reduce Physical memory (bytes)=319307776\n",
      "\t\tPeak Reduce Virtual memory (bytes)=4344000512\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=1140698687\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=30359819\n",
      "2024-02-15 00:36:58,664 INFO streaming.StreamJob: Output directory: /user/tweets/result-fast2/\n",
      "CPU times: user 2.08 s, sys: 372 ms, total: 2.45 s\n",
      "Wall time: 2min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "! hdfs dfs -rm -r /user/tweets/result-fast2 || true\n",
    "! yarn jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "-D mapreduce.job.name=\"word-count\" \\\n",
    "-D mapreduce.job.reduces=3 \\\n",
    "-files wordcount2.py \\\n",
    "-mapper \"python3 wordcount2.py map\" \\\n",
    "-combiner \"python3 wordcount2.py reduce\" \\\n",
    "-reducer \"python3 wordcount2.py reduce\" \\\n",
    "-input /user/tweets/data/ \\\n",
    "-output /user/tweets/result-fast2/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aa\t1726\r\n",
      "aaaaa\t7\r\n",
      "aaaaaaaaaaaaaa\t3\r\n",
      "aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaannnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnddddddddddddddddddddddddddddddddddddd\t1\r\n",
      "aaaaaaaaaaaaaaaaaaah\t1\r\n",
      "aaaaaaaaaaaaand\t1\r\n",
      "aaaaaaaaannnnnnnnnnnddddddddddddd\t1\r\n",
      "aaaaaaaah\t1\r\n",
      "aaaaaaaamen\t1\r\n",
      "aaaaaaagh\t2\r\n",
      "cat: Unable to write to output stream.\r\n",
      "cat: Unable to write to output stream.\r\n",
      "cat: Unable to write to output stream.\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -cat /user/tweets/result-fast1/* | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Часто combiner может просто совпадать с reducer однако это не всегда так по следующей причине - combiner не имеет права менять формат вывода после стадии map.\n",
    "\n",
    "Hadoop самостоятельно опеределяет целесообразность запуска combiner и может его не запускать вовсе.\n",
    "Или например задача может вообще не подходить под такую модель запуска. Если мы ищем среднее, то нельзя заранее подсчитывать среднее на стадии combiner - макмимум, что мы там можем запустить - это подсчет количество и суммы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting top10-3.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile top10-3.py\n",
    "\n",
    "import sys\n",
    "import collections\n",
    "from itertools import islice\n",
    "\n",
    "def build_stop_words():\n",
    "    with open('stop-words.txt', 'r') as f:\n",
    "        stop_words = {x.split('\\t')[0] for x in f}\n",
    "    return stop_words\n",
    "\n",
    "def kv_stream(sep=\"\\t\"):\n",
    "    return map(lambda x: x.split(sep), sys.stdin)\n",
    "\n",
    "def rewind():\n",
    "    collections.deque(sys.stdin, maxlen=0)\n",
    "\n",
    "def mapper():\n",
    "    for key, value in kv_stream():\n",
    "        print(\"{}+{}\\t\".format(key, value.strip()))\n",
    "\n",
    "def reducer():\n",
    "    stop_words = build_stop_words()\n",
    "    first_10_stream = islice(filter(lambda x: x[0] not in stop_words, kv_stream('+')), 10)\n",
    "    \n",
    "    for word, count in first_10_stream:\n",
    "        print(\"{}\\t{}\".format(word, count.strip()))\n",
    "    rewind()\n",
    "    \n",
    "def combiner():\n",
    "    stop_words = build_stop_words()\n",
    "    first_10_stream = islice(filter(lambda x: x[0] not in stop_words, kv_stream('+')), 10)\n",
    "    \n",
    "    for word, count in first_10_stream:\n",
    "        print(\"{}+{}\\t\".format(word, count.strip()))\n",
    "    rewind()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    mr_command = sys.argv[1]\n",
    "    {\n",
    "        'map': mapper,\n",
    "        'reduce': reducer,\n",
    "        'combiner': combiner\n",
    "    }[mr_command]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/tweets/top10-fast\n",
      "packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-3.2.2.jar] /tmp/streamjob2343487677005611575.jar tmpDir=null\n",
      "2023-01-24 16:33:22,355 INFO client.RMProxy: Connecting to ResourceManager at rc1a-dataproc-m-ymnek55ntqs3neie.mdb.yandexcloud.net/10.128.0.25:8032\n",
      "2023-01-24 16:33:22,566 INFO client.AHSProxy: Connecting to Application History server at rc1a-dataproc-m-ymnek55ntqs3neie.mdb.yandexcloud.net/10.128.0.25:10200\n",
      "2023-01-24 16:33:22,604 INFO client.RMProxy: Connecting to ResourceManager at rc1a-dataproc-m-ymnek55ntqs3neie.mdb.yandexcloud.net/10.128.0.25:8032\n",
      "2023-01-24 16:33:22,606 INFO client.AHSProxy: Connecting to Application History server at rc1a-dataproc-m-ymnek55ntqs3neie.mdb.yandexcloud.net/10.128.0.25:10200\n",
      "2023-01-24 16:33:22,828 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/ubuntu/.staging/job_1674572529553_0007\n",
      "2023-01-24 16:33:23,584 INFO mapred.FileInputFormat: Total input files to process : 3\n",
      "2023-01-24 16:33:23,667 INFO mapreduce.JobSubmitter: number of splits:30\n",
      "2023-01-24 16:33:24,229 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1674572529553_0007\n",
      "2023-01-24 16:33:24,231 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2023-01-24 16:33:24,405 INFO conf.Configuration: resource-types.xml not found\n",
      "2023-01-24 16:33:24,406 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2023-01-24 16:33:24,472 INFO impl.YarnClientImpl: Submitted application application_1674572529553_0007\n",
      "2023-01-24 16:33:24,501 INFO mapreduce.Job: The url to track the job: http://rc1a-dataproc-m-ymnek55ntqs3neie.mdb.yandexcloud.net:8088/proxy/application_1674572529553_0007/\n",
      "2023-01-24 16:33:24,502 INFO mapreduce.Job: Running job: job_1674572529553_0007\n",
      "2023-01-24 16:33:29,572 INFO mapreduce.Job: Job job_1674572529553_0007 running in uber mode : false\n",
      "2023-01-24 16:33:29,574 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2023-01-24 16:33:36,664 INFO mapreduce.Job:  map 3% reduce 0%\n",
      "2023-01-24 16:33:39,745 INFO mapreduce.Job:  map 20% reduce 0%\n",
      "2023-01-24 16:33:40,753 INFO mapreduce.Job:  map 27% reduce 0%\n",
      "2023-01-24 16:33:43,768 INFO mapreduce.Job:  map 30% reduce 0%\n",
      "2023-01-24 16:33:46,792 INFO mapreduce.Job:  map 33% reduce 0%\n",
      "2023-01-24 16:33:47,799 INFO mapreduce.Job:  map 43% reduce 0%\n",
      "2023-01-24 16:33:48,805 INFO mapreduce.Job:  map 53% reduce 0%\n",
      "2023-01-24 16:33:49,810 INFO mapreduce.Job:  map 57% reduce 0%\n",
      "2023-01-24 16:33:52,825 INFO mapreduce.Job:  map 60% reduce 0%\n",
      "2023-01-24 16:33:54,835 INFO mapreduce.Job:  map 63% reduce 20%\n",
      "2023-01-24 16:33:55,857 INFO mapreduce.Job:  map 67% reduce 20%\n",
      "2023-01-24 16:33:56,897 INFO mapreduce.Job:  map 73% reduce 20%\n",
      "2023-01-24 16:33:57,965 INFO mapreduce.Job:  map 83% reduce 20%\n",
      "2023-01-24 16:33:58,978 INFO mapreduce.Job:  map 87% reduce 20%\n",
      "2023-01-24 16:33:59,986 INFO mapreduce.Job:  map 93% reduce 20%\n",
      "2023-01-24 16:34:00,991 INFO mapreduce.Job:  map 93% reduce 30%\n",
      "2023-01-24 16:34:01,995 INFO mapreduce.Job:  map 100% reduce 30%\n",
      "2023-01-24 16:34:02,999 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "2023-01-24 16:34:04,010 INFO mapreduce.Job: Job job_1674572529553_0007 completed successfully\n",
      "2023-01-24 16:34:04,087 INFO mapreduce.Job: Counters: 56\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=2185\n",
      "\t\tFILE: Number of bytes written=7533150\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=31346198\n",
      "\t\tHDFS: Number of bytes written=109\n",
      "\t\tHDFS: Number of read operations=95\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=3\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tKilled map tasks=1\n",
      "\t\tLaunched map tasks=31\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=15\n",
      "\t\tRack-local map tasks=16\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=482061\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=62595\n",
      "\t\tTotal time spent by all map tasks (ms)=160687\n",
      "\t\tTotal time spent by all reduce tasks (ms)=20865\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=160687\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=20865\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=493630464\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=64097280\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=2831736\n",
      "\t\tMap output records=2831736\n",
      "\t\tMap output bytes=33191556\n",
      "\t\tMap output materialized bytes=3766\n",
      "\t\tInput split bytes=4410\n",
      "\t\tCombine input records=2831736\n",
      "\t\tCombine output records=300\n",
      "\t\tReduce input groups=300\n",
      "\t\tReduce shuffle bytes=3766\n",
      "\t\tReduce input records=300\n",
      "\t\tReduce output records=10\n",
      "\t\tSpilled Records=600\n",
      "\t\tShuffled Maps =30\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=30\n",
      "\t\tGC time elapsed (ms)=3859\n",
      "\t\tCPU time spent (ms)=84310\n",
      "\t\tPhysical memory (bytes) snapshot=12634013696\n",
      "\t\tVirtual memory (bytes) snapshot=134578229248\n",
      "\t\tTotal committed heap usage (bytes)=12844531712\n",
      "\t\tPeak Map Physical memory (bytes)=478486528\n",
      "\t\tPeak Map Virtual memory (bytes)=4354662400\n",
      "\t\tPeak Reduce Physical memory (bytes)=209358848\n",
      "\t\tPeak Reduce Virtual memory (bytes)=4345192448\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=31341788\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=109\n",
      "2023-01-24 16:34:04,088 INFO streaming.StreamJob: Output directory: /user/tweets/top10-fast/\n",
      "CPU times: user 744 ms, sys: 137 ms, total: 882 ms\n",
      "Wall time: 45.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "! hdfs dfs -rm -r /user/tweets/top10-fast || true\n",
    "! yarn jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "-D mapreduce.job.name=\"word-count\" \\\n",
    "-D mapreduce.job.reduces=1 \\\n",
    "-D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapreduce.lib.partition.KeyFieldBasedComparator \\\n",
    "-D mapreduce.partition.keycomparator.options=\"-k2,2nr -k1,1\" \\\n",
    "-D mapreduce.map.output.key.field.separator='+' \\\n",
    "-files top10-3.py,stop-words.txt \\\n",
    "-mapper \"python3 top10-3.py map\" \\\n",
    "-combiner \"python3 top10-3.py combiner\" \\\n",
    "-reducer \"python3 top10-3.py reduce\" \\\n",
    "-input /user/tweets/result-fast1 \\\n",
    "-output /user/tweets/top10-fast/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i\t287232\r\n",
      "for\t272995\r\n",
      "and\t247749\r\n",
      "is\t246856\r\n",
      "on\t210172\r\n",
      "you\t196950\r\n",
      "trump\t169520\r\n",
      "news\t156101\r\n",
      "it\t152816\r\n",
      "with\t134178\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -cat /user/tweets/top10-fast/* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Кастомный Partitioner\n",
    "\n",
    "В Hadoop MapReduce можно указывать свой кастомный партишенер, который будет определять, как разбивать данные по редюсерам.\n",
    "\n",
    "Это бывает важно, когда вы используете сложный ключ и много редюсеров - вполне возможно вы захотите, чтобы такие ключи определенным образом распределялись по редюс-задачам.\n",
    "\n",
    "Для наглядности давайте решим такую задачу - для каждого пользователя подсчитаем, на каких языках он писал твиты и в каком количестве"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing lang-distribution.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile lang-distribution.py\n",
    "\n",
    "import sys\n",
    "import csv\n",
    "from collections import Counter\n",
    "from itertools import groupby\n",
    "\n",
    "def kv_stream(sep=\"\\t\"):\n",
    "    return map(lambda x: x.split(sep), sys.stdin)\n",
    "\n",
    "def csv_stream():\n",
    "    return csv.reader(iter(sys.stdin.readline, ''))\n",
    "\n",
    "def mapper():\n",
    "    for row in csv_stream():\n",
    "        author, lang = row[1], row[4]\n",
    "        print(\"{}+{}\\t1\".format(author.strip(), lang.strip()))\n",
    "\n",
    "def reducer():\n",
    "    for author, records in groupby(kv_stream('+'), lambda x: x[0]):\n",
    "        langs_stream = (x.split('\\t') for _, x in records)\n",
    "        for lang, group in groupby(langs_stream, lambda x: x[0]):\n",
    "            count = sum(int(x) for _, x in group)\n",
    "            print(\"{}+{}\\t{}\".format(author, lang, count))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    mr_command = sys.argv[1]\n",
    "    {\n",
    "        'map': mapper,\n",
    "        'reduce': reducer,\n",
    "    }[mr_command]()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Параметры\n",
    "\n",
    "```\n",
    "-D mapred.partitioner.class=org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner \\\n",
    "-D mapreduce.partition.keypartitioner.options=\"-k1,1\" \\\n",
    "```\n",
    "\n",
    "Указывают на то, что разделятся на редюсеры записи должны не по полному ключу, а только по первой его части"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `/user/tweets/lang-dist': No such file or directory\n",
      "packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-3.2.2.jar] /tmp/streamjob3880496621809295615.jar tmpDir=null\n",
      "2024-02-15 00:38:41,492 INFO client.RMProxy: Connecting to ResourceManager at rc1a-dataproc-m-zkmve6o6f689a4so.mdb.yandexcloud.net/10.128.0.4:8032\n",
      "2024-02-15 00:38:41,667 INFO client.AHSProxy: Connecting to Application History server at rc1a-dataproc-m-zkmve6o6f689a4so.mdb.yandexcloud.net/10.128.0.4:10200\n",
      "2024-02-15 00:38:41,693 INFO client.RMProxy: Connecting to ResourceManager at rc1a-dataproc-m-zkmve6o6f689a4so.mdb.yandexcloud.net/10.128.0.4:8032\n",
      "2024-02-15 00:38:41,693 INFO client.AHSProxy: Connecting to Application History server at rc1a-dataproc-m-zkmve6o6f689a4so.mdb.yandexcloud.net/10.128.0.4:10200\n",
      "2024-02-15 00:38:41,854 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/ubuntu/.staging/job_1707952654779_0006\n",
      "2024-02-15 00:38:42,161 INFO mapred.FileInputFormat: Total input files to process : 13\n",
      "2024-02-15 00:38:42,269 INFO mapreduce.JobSubmitter: number of splits:13\n",
      "2024-02-15 00:38:42,390 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1707952654779_0006\n",
      "2024-02-15 00:38:42,391 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2024-02-15 00:38:42,524 INFO conf.Configuration: resource-types.xml not found\n",
      "2024-02-15 00:38:42,525 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2024-02-15 00:38:42,586 INFO impl.YarnClientImpl: Submitted application application_1707952654779_0006\n",
      "2024-02-15 00:38:42,643 INFO mapreduce.Job: The url to track the job: http://rc1a-dataproc-m-zkmve6o6f689a4so.mdb.yandexcloud.net:8088/proxy/application_1707952654779_0006/\n",
      "2024-02-15 00:38:42,644 INFO mapreduce.Job: Running job: job_1707952654779_0006\n",
      "2024-02-15 00:38:47,712 INFO mapreduce.Job: Job job_1707952654779_0006 running in uber mode : false\n",
      "2024-02-15 00:38:47,712 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2024-02-15 00:38:58,816 INFO mapreduce.Job:  map 23% reduce 0%\n",
      "2024-02-15 00:39:08,874 INFO mapreduce.Job:  map 46% reduce 0%\n",
      "2024-02-15 00:39:16,916 INFO mapreduce.Job:  map 54% reduce 0%\n",
      "2024-02-15 00:39:17,922 INFO mapreduce.Job:  map 69% reduce 0%\n",
      "2024-02-15 00:39:25,962 INFO mapreduce.Job:  map 85% reduce 0%\n",
      "2024-02-15 00:39:30,987 INFO mapreduce.Job:  map 92% reduce 0%\n",
      "2024-02-15 00:39:32,997 INFO mapreduce.Job:  map 92% reduce 10%\n",
      "2024-02-15 00:39:34,003 INFO mapreduce.Job:  map 100% reduce 10%\n",
      "2024-02-15 00:39:38,023 INFO mapreduce.Job:  map 100% reduce 67%\n",
      "2024-02-15 00:39:40,032 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "2024-02-15 00:39:40,038 INFO mapreduce.Job: Job job_1707952654779_0006 completed successfully\n",
      "2024-02-15 00:39:40,104 INFO mapreduce.Job: Counters: 55\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=359106\n",
      "\t\tFILE: Number of bytes written=4602636\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=1140700654\n",
      "\t\tHDFS: Number of bytes written=380326\n",
      "\t\tHDFS: Number of read operations=54\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=9\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tKilled map tasks=1\n",
      "\t\tLaunched map tasks=13\n",
      "\t\tLaunched reduce tasks=3\n",
      "\t\tData-local map tasks=13\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=301140\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=97410\n",
      "\t\tTotal time spent by all map tasks (ms)=100380\n",
      "\t\tTotal time spent by all reduce tasks (ms)=32470\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=100380\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=32470\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=308367360\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=99747840\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=2946207\n",
      "\t\tMap output records=2946207\n",
      "\t\tMap output bytes=67013403\n",
      "\t\tMap output materialized bytes=361612\n",
      "\t\tInput split bytes=1967\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=16091\n",
      "\t\tReduce shuffle bytes=361612\n",
      "\t\tReduce input records=2946207\n",
      "\t\tReduce output records=16091\n",
      "\t\tSpilled Records=5892414\n",
      "\t\tShuffled Maps =39\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=39\n",
      "\t\tGC time elapsed (ms)=2434\n",
      "\t\tCPU time spent (ms)=53870\n",
      "\t\tPhysical memory (bytes) snapshot=6740963328\n",
      "\t\tVirtual memory (bytes) snapshot=69454270464\n",
      "\t\tTotal committed heap usage (bytes)=6700924928\n",
      "\t\tPeak Map Physical memory (bytes)=480968704\n",
      "\t\tPeak Map Virtual memory (bytes)=4346130432\n",
      "\t\tPeak Reduce Physical memory (bytes)=411463680\n",
      "\t\tPeak Reduce Virtual memory (bytes)=4353155072\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=1140698687\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=380326\n",
      "2024-02-15 00:39:40,105 INFO streaming.StreamJob: Output directory: /user/tweets/lang-dist\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -rm -r /user/tweets/lang-dist || true\n",
    "! yarn jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "-D mapreduce.job.name=\"lang-dist\" \\\n",
    "-D mapreduce.job.reduces=3 \\\n",
    "-D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapreduce.lib.partition.KeyFieldBasedComparator \\\n",
    "-D mapreduce.partition.keycomparator.options=\"-k1,1 -k2,2\" \\\n",
    "-D mapreduce.map.output.key.field.separator='+' \\\n",
    "-D mapred.partitioner.class=org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner \\\n",
    "-D mapreduce.partition.keypartitioner.options=\"-k1,1\" \\\n",
    "-files lang-distribution.py \\\n",
    "-mapper \"python3 lang-distribution.py map\" \\\n",
    "-reducer \"python3 lang-distribution.py reduce\" \\\n",
    "-input /user/tweets/data/ \\\n",
    "-output /user/tweets/lang-dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1488REASONS+Russian\t50\r\n",
      "1488REASONS+Serbian\t1\r\n",
      "1488REASONS+Ukrainian\t1\r\n",
      "1D_NICOLE_+Albanian\t1\r\n",
      "1D_NICOLE_+English\t41\r\n",
      "1D_NICOLE_+Tagalog (Filipino)\t2\r\n",
      "1ERIK_LEE+English\t2\r\n",
      "459JISALGE+Russian\t1\r\n",
      "4MYSQUAD+Arabic\t5\r\n",
      "4MYSQUAD+Catalan\t1\r\n",
      "cat: Unable to write to output stream.\r\n",
      "cat: Unable to write to output stream.\r\n",
      "cat: Unable to write to output stream.\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -cat /user/tweets/lang-dist/* | head "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Как дебажить ошибки\n",
    "\n",
    "Через yarn можно выводить логи приложения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mistake.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mistake.py\n",
    "\n",
    "import sys\n",
    "import csv\n",
    "import re\n",
    "from itertools import groupby\n",
    "\n",
    "\n",
    "def csv_stream():\n",
    "    return csv.reader(iter(sys.stdin.readline, ''))\n",
    "\n",
    "def kv_stream(sep=\"\\t\"):\n",
    "    return map(lambda x: x.split(sep), sys.stdin)\n",
    "\n",
    "\n",
    "def mapper():\n",
    "    pattern = re.compile(r\"[a-z]+\")\n",
    "    for row in csv_stream():\n",
    "        content = row[2]\n",
    "        for match in pattern.finditer(content.lower()):\n",
    "            word = match.group(0)\n",
    "            strange_number = 1.0 / (len(word) - 1)\n",
    "            print(\"{}\\t{}\".format(word, strange_number))\n",
    "\n",
    "\n",
    "def reducer():\n",
    "    for key, group in groupby(kv_stream(), lambda x: x[0]):\n",
    "        word = key\n",
    "        number = sum(float(x) for _, x in group)\n",
    "        print(\"{}\\t{}\".format(word, number))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    mr_command = sys.argv[1]\n",
    "    {\n",
    "        'map': mapper,\n",
    "        'reduce': reducer\n",
    "    }[mr_command]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/tweets/mistake\n",
      "packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-3.2.2.jar] /tmp/streamjob472224327129279597.jar tmpDir=null\n",
      "2023-01-24 16:41:58,494 INFO client.RMProxy: Connecting to ResourceManager at rc1a-dataproc-m-ymnek55ntqs3neie.mdb.yandexcloud.net/10.128.0.25:8032\n",
      "2023-01-24 16:41:58,725 INFO client.AHSProxy: Connecting to Application History server at rc1a-dataproc-m-ymnek55ntqs3neie.mdb.yandexcloud.net/10.128.0.25:10200\n",
      "2023-01-24 16:41:58,768 INFO client.RMProxy: Connecting to ResourceManager at rc1a-dataproc-m-ymnek55ntqs3neie.mdb.yandexcloud.net/10.128.0.25:8032\n",
      "2023-01-24 16:41:58,769 INFO client.AHSProxy: Connecting to Application History server at rc1a-dataproc-m-ymnek55ntqs3neie.mdb.yandexcloud.net/10.128.0.25:10200\n",
      "2023-01-24 16:41:58,965 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/ubuntu/.staging/job_1674572529553_0009\n",
      "2023-01-24 16:41:59,714 INFO mapred.FileInputFormat: Total input files to process : 13\n",
      "2023-01-24 16:42:00,217 INFO mapreduce.JobSubmitter: number of splits:37\n",
      "2023-01-24 16:42:00,796 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1674572529553_0009\n",
      "2023-01-24 16:42:00,797 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2023-01-24 16:42:00,980 INFO conf.Configuration: resource-types.xml not found\n",
      "2023-01-24 16:42:00,980 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2023-01-24 16:42:01,058 INFO impl.YarnClientImpl: Submitted application application_1674572529553_0009\n",
      "2023-01-24 16:42:01,092 INFO mapreduce.Job: The url to track the job: http://rc1a-dataproc-m-ymnek55ntqs3neie.mdb.yandexcloud.net:8088/proxy/application_1674572529553_0009/\n",
      "2023-01-24 16:42:01,100 INFO mapreduce.Job: Running job: job_1674572529553_0009\n",
      "2023-01-24 16:42:06,176 INFO mapreduce.Job: Job job_1674572529553_0009 running in uber mode : false\n",
      "2023-01-24 16:42:06,177 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2023-01-24 16:42:10,344 INFO mapreduce.Job: Task Id : attempt_1674572529553_0009_m_000001_0, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:326)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:539)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:465)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:349)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:174)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:168)\n",
      "\n",
      "2023-01-24 16:42:10,356 INFO mapreduce.Job: Task Id : attempt_1674572529553_0009_m_000006_0, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:326)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:539)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:465)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:349)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:174)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:168)\n",
      "\n",
      "2023-01-24 16:42:10,364 INFO mapreduce.Job: Task Id : attempt_1674572529553_0009_m_000000_0, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:326)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:539)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:465)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:349)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:174)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:168)\n",
      "\n",
      "2023-01-24 16:42:12,388 INFO mapreduce.Job: Task Id : attempt_1674572529553_0009_m_000013_0, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:326)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:539)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:465)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:349)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:174)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:168)\n",
      "\n",
      "2023-01-24 16:42:12,389 INFO mapreduce.Job: Task Id : attempt_1674572529553_0009_m_000003_0, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:326)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:539)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:465)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:349)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:174)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:168)\n",
      "\n",
      "2023-01-24 16:42:12,390 INFO mapreduce.Job: Task Id : attempt_1674572529553_0009_m_000005_0, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:326)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:539)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:465)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:349)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:174)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:168)\n",
      "\n",
      "2023-01-24 16:42:12,392 INFO mapreduce.Job: Task Id : attempt_1674572529553_0009_m_000017_0, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:326)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:539)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:465)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:349)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:174)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:168)\n",
      "\n",
      "2023-01-24 16:42:12,393 INFO mapreduce.Job: Task Id : attempt_1674572529553_0009_m_000012_0, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:326)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:539)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:465)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:349)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:174)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:168)\n",
      "\n",
      "2023-01-24 16:42:12,394 INFO mapreduce.Job: Task Id : attempt_1674572529553_0009_m_000002_0, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:326)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:539)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:465)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:349)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:174)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:168)\n",
      "\n",
      "2023-01-24 16:42:12,395 INFO mapreduce.Job: Task Id : attempt_1674572529553_0009_m_000004_0, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:326)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:539)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:465)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:349)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:174)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:168)\n",
      "\n",
      "2023-01-24 16:42:12,397 INFO mapreduce.Job: Task Id : attempt_1674572529553_0009_m_000016_0, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:326)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:539)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:465)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:349)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:174)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:168)\n",
      "\n",
      "2023-01-24 16:42:12,399 INFO mapreduce.Job: Task Id : attempt_1674572529553_0009_m_000007_0, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:326)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:539)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:465)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:349)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:174)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:168)\n",
      "\n",
      "2023-01-24 16:42:15,441 INFO mapreduce.Job: Task Id : attempt_1674572529553_0009_m_000001_1, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:326)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:539)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:465)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:349)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:174)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:168)\n",
      "\n",
      "2023-01-24 16:42:17,508 INFO mapreduce.Job: Task Id : attempt_1674572529553_0009_m_000017_1, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:326)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:539)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:465)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:349)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:174)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:168)\n",
      "\n",
      "2023-01-24 16:42:17,511 INFO mapreduce.Job: Task Id : attempt_1674572529553_0009_m_000020_0, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:326)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:539)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:465)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:349)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:174)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:168)\n",
      "\n",
      "2023-01-24 16:42:17,517 INFO mapreduce.Job: Task Id : attempt_1674572529553_0009_m_000021_0, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:326)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:539)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:465)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:349)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:174)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:168)\n",
      "\n",
      "2023-01-24 16:42:17,523 INFO mapreduce.Job: Task Id : attempt_1674572529553_0009_m_000005_1, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:326)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:539)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:465)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:349)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:174)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:168)\n",
      "\n",
      "2023-01-24 16:42:17,527 INFO mapreduce.Job: Task Id : attempt_1674572529553_0009_m_000031_0, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:326)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:539)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:465)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:349)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:174)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:168)\n",
      "\n",
      "2023-01-24 16:42:17,529 INFO mapreduce.Job: Task Id : attempt_1674572529553_0009_m_000013_1, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:326)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:539)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:465)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:349)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:174)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:168)\n",
      "\n",
      "2023-01-24 16:42:18,540 INFO mapreduce.Job: Task Id : attempt_1674572529553_0009_m_000006_1, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:326)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:539)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:465)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:349)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:174)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:168)\n",
      "\n",
      "2023-01-24 16:42:18,542 INFO mapreduce.Job: Task Id : attempt_1674572529553_0009_m_000012_1, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:326)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:539)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:465)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:349)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:174)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:168)\n",
      "\n",
      "2023-01-24 16:42:18,544 INFO mapreduce.Job: Task Id : attempt_1674572529553_0009_m_000003_1, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:326)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:539)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:465)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:349)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:174)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:168)\n",
      "\n",
      "2023-01-24 16:42:18,545 INFO mapreduce.Job: Task Id : attempt_1674572529553_0009_m_000000_1, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:326)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:539)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:465)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:349)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:174)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:168)\n",
      "\n",
      "2023-01-24 16:42:18,547 INFO mapreduce.Job: Task Id : attempt_1674572529553_0009_m_000002_1, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:326)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:539)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:465)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:349)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:174)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:168)\n",
      "\n",
      "2023-01-24 16:42:20,613 INFO mapreduce.Job: Task Id : attempt_1674572529553_0009_m_000004_1, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:326)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:539)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:465)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:349)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:174)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:168)\n",
      "\n",
      "2023-01-24 16:42:21,676 INFO mapreduce.Job: Task Id : attempt_1674572529553_0009_m_000006_2, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:326)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:539)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:465)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:349)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:174)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:168)\n",
      "\n",
      "2023-01-24 16:42:22,684 INFO mapreduce.Job: Task Id : attempt_1674572529553_0009_m_000005_2, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:326)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:539)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:465)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:349)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:174)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:168)\n",
      "\n",
      "2023-01-24 16:42:23,729 INFO mapreduce.Job: Task Id : attempt_1674572529553_0009_m_000016_1, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:326)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:539)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:465)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:349)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:174)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:168)\n",
      "\n",
      "2023-01-24 16:42:23,745 INFO mapreduce.Job: Task Id : attempt_1674572529553_0009_m_000017_2, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:326)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:539)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:465)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:349)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:174)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:168)\n",
      "\n",
      "2023-01-24 16:42:23,760 INFO mapreduce.Job: Task Id : attempt_1674572529553_0009_m_000021_1, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:326)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:539)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:465)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:349)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:174)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:168)\n",
      "\n",
      "2023-01-24 16:42:23,784 INFO mapreduce.Job: Task Id : attempt_1674572529553_0009_m_000013_2, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:326)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:539)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:465)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:349)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:174)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:168)\n",
      "\n",
      "2023-01-24 16:42:23,813 INFO mapreduce.Job: Task Id : attempt_1674572529553_0009_m_000007_1, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:326)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:539)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:465)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:349)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:174)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:168)\n",
      "\n",
      "2023-01-24 16:42:23,829 INFO mapreduce.Job: Task Id : attempt_1674572529553_0009_m_000020_1, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:326)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:539)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:465)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:349)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:174)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:168)\n",
      "\n",
      "2023-01-24 16:42:23,850 INFO mapreduce.Job: Task Id : attempt_1674572529553_0009_m_000001_2, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:326)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:539)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:465)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:349)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:174)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:168)\n",
      "\n",
      "2023-01-24 16:42:23,854 INFO mapreduce.Job: Task Id : attempt_1674572529553_0009_m_000031_1, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:326)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:539)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:465)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:349)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:174)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:168)\n",
      "\n",
      "2023-01-24 16:42:24,897 INFO mapreduce.Job: Task Id : attempt_1674572529553_0009_m_000012_2, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:326)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:539)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:465)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:349)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:174)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:168)\n",
      "\n",
      "2023-01-24 16:42:26,998 INFO mapreduce.Job: Task Id : attempt_1674572529553_0009_m_000000_2, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:326)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:539)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:465)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:349)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:174)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:168)\n",
      "\n",
      "2023-01-24 16:42:27,004 INFO mapreduce.Job: Task Id : attempt_1674572529553_0009_m_000003_2, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:326)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:539)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:465)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:349)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:174)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:168)\n",
      "\n",
      "2023-01-24 16:42:28,022 INFO mapreduce.Job: Task Id : attempt_1674572529553_0009_m_000021_2, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:326)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:539)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:465)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:349)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:174)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:168)\n",
      "\n",
      "2023-01-24 16:42:30,095 INFO mapreduce.Job: Task Id : attempt_1674572529553_0009_m_000002_2, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:326)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:539)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:465)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:349)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:174)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:168)\n",
      "\n",
      "2023-01-24 16:42:31,099 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "2023-01-24 16:42:33,116 INFO mapreduce.Job: Job job_1674572529553_0009 failed with state FAILED due to: Task failed task_1674572529553_0009_m_000017\n",
      "Job failed as tasks failed. failedMaps:1 failedReduces:0 killedMaps:0 killedReduces: 0\n",
      "\n",
      "2023-01-24 16:42:33,190 INFO mapreduce.Job: Counters: 14\n",
      "\tJob Counters \n",
      "\t\tFailed map tasks=41\n",
      "\t\tKilled map tasks=36\n",
      "\t\tKilled reduce tasks=3\n",
      "\t\tLaunched map tasks=50\n",
      "\t\tOther local map tasks=35\n",
      "\t\tData-local map tasks=15\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=612042\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=0\n",
      "\t\tTotal time spent by all map tasks (ms)=204014\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=204014\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=626731008\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=0\n",
      "\t\tPhysical memory (bytes) snapshot=0\n",
      "\t\tVirtual memory (bytes) snapshot=0\n",
      "2023-01-24 16:42:33,190 ERROR streaming.StreamJob: Job not successful!\n",
      "Streaming Command Failed!\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -rm -r /user/tweets/mistake || true\n",
    "! yarn jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "-D mapreduce.job.name=\"mistake\" \\\n",
    "-D mapreduce.job.reduces=3 \\\n",
    "-files mistake.py \\\n",
    "-mapper \"python3 mistake.py map\" \\\n",
    "-reducer \"python3 mistake.py reduce\" \\\n",
    "-input /user/tweets/data/ \\\n",
    "-output /user/tweets/mistake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-24 16:43:07,268 INFO client.RMProxy: Connecting to ResourceManager at rc1a-dataproc-m-ymnek55ntqs3neie.mdb.yandexcloud.net/10.128.0.25:8032\n",
      "2023-01-24 16:43:07,551 INFO client.AHSProxy: Connecting to Application History server at rc1a-dataproc-m-ymnek55ntqs3neie.mdb.yandexcloud.net/10.128.0.25:10200\n",
      "Container: container_1674572529553_0009_01_000015 on rc1a-dataproc-d-1jipfldazn1nw9u1.mdb.yandexcloud.net_33887\n",
      "LogAggregationType: AGGREGATED\n",
      "===============================================================================================================\n",
      "LogType:stderr\n",
      "LogLastModifiedTime:Tue Jan 24 16:42:38 +0000 2023\n",
      "LogLength:208\n",
      "LogContents:\n",
      "Traceback (most recent call last):\n",
      "  File \"mistake.py\", line 34, in <module>\n",
      "    {\n",
      "  File \"mistake.py\", line 21, in mapper\n",
      "    strange_number = 1.0 / (len(word) - 1)\n",
      "ZeroDivisionError: float division by zero\n",
      "\n",
      "End of LogType:stderr\n",
      "***********************************************************************\n",
      "\n",
      "\n",
      "End of LogType:stderr\n",
      "***********************************************************************\n",
      "\n",
      "Container: container_1674572529553_0009_01_000011 on rc1a-dataproc-d-1jipfldazn1nw9u1.mdb.yandexcloud.net_33887\n",
      "LogAggregationType: AGGREGATED\n",
      "===============================================================================================================\n",
      "LogType:stderr\n",
      "LogLastModifiedTime:Tue Jan 24 16:42:38 +0000 2023\n",
      "LogLength:208\n",
      "LogContents:\n",
      "Traceback (most recent call last):\n",
      "  File \"mistake.py\", line 34, in <module>\n",
      "    {\n",
      "  File \"mistake.py\", line 21, in mapper\n",
      "    strange_number = 1.0 / (len(word) - 1)\n",
      "ZeroDivisionError: float division by zero\n",
      "\n",
      "End of LogType:stderr\n",
      "***********************************************************************\n",
      "\n",
      "Container: container_1674572529553_0009_01_000043 on rc1a-dataproc-d-1jipfldazn1nw9u1.mdb.yandexcloud.net_33887\n",
      "LogAggregationType: AGGREGATED\n",
      "===============================================================================================================\n",
      "LogType:stderr\n",
      "LogLastModifiedTime:Tue Jan 24 16:42:38 +0000 2023\n",
      "LogLength:208\n",
      "LogContents:\n",
      "Traceback (most recent call last):\n",
      "  File \"mistake.py\", line 34, in <module>\n",
      "    {\n",
      "  File \"mistake.py\", line 21, in mapper\n",
      "    strange_number = 1.0 / (len(word) - 1)\n"
     ]
    }
   ],
   "source": [
    "! yarn logs -applicationId application_1674572529553_0009 -log_files stderr | head -n 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Идентификатор можно найти логах запуска, в интерфейсе или посмотреть список всех и найти там"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-24 16:43:30,487 INFO client.RMProxy: Connecting to ResourceManager at rc1a-dataproc-m-ymnek55ntqs3neie.mdb.yandexcloud.net/10.128.0.25:8032\n",
      "2023-01-24 16:43:30,702 INFO client.AHSProxy: Connecting to Application History server at rc1a-dataproc-m-ymnek55ntqs3neie.mdb.yandexcloud.net/10.128.0.25:10200\n",
      "Total number of applications (application-types: [], states: [SUBMITTED, ACCEPTED, RUNNING] and tags: []):0\n",
      "                Application-Id\t    Application-Name\t    Application-Type\t      User\t     Queue\t             State\t       Final-State\t       Progress\t                       Tracking-URL\n"
     ]
    }
   ],
   "source": [
    "! yarn application -list "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы заранее прикончить задачу, можно также использовать yarn\n",
    "\n",
    "```bash\n",
    "yarn application -kill <application_id>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Переносим результаты\n",
    "\n",
    "Все данные, с которыми мы работали только что, лежат на жестких дисках кластера и доступны только через hdfs\n",
    "\n",
    "Если мы готовы презентовать наш результат миру, нужно его переместить в s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\r\n",
      "-rw-r--r--   1 ubuntu hadoop          0 2024-02-15 00:30 /user/tweets/top10-stop-words/_SUCCESS\r\n",
      "-rw-r--r--   1 ubuntu hadoop        109 2024-02-15 00:30 /user/tweets/top10-stop-words/part-00000\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -ls /user/tweets/top10-stop-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-02-15 00:40:23,549 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
      "2024-02-15 00:40:23,613 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
      "2024-02-15 00:40:23,613 INFO impl.MetricsSystemImpl: s3a-file-system metrics system started\n",
      "2024-02-15 00:40:26,509 INFO impl.MetricsSystemImpl: Stopping s3a-file-system metrics system...\n",
      "2024-02-15 00:40:26,509 INFO impl.MetricsSystemImpl: s3a-file-system metrics system stopped.\n",
      "2024-02-15 00:40:26,509 INFO impl.MetricsSystemImpl: s3a-file-system metrics system shutdown complete.\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -cp /user/tweets/top10-stop-words/part-00000 s3a://art591/top10.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно проверять через интерфейс - файл оказался в бакете"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hadoop жжет бабло\n",
    "\n",
    "<img src=\"http://vostokovod.ru/assets/images/blog/2013/000333.png\">\n",
    "\n",
    "Полноценный кластер - весьма дорогое удовольствие, поэтому отлючайте его после использования. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
