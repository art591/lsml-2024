{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apache Spark\n",
    "\n",
    "Автор ноутбука: Алексей Космачев\n",
    "\n",
    "Сегодня будет говорить про Apache Spark - более удобный фреймворк для обработки больших данных на базе Hadoop.\n",
    "\n",
    "С Spark можно работать из ноутбуков в Data Sphere, но так как нам еще потребуется запускать bash команды, то я буду запускать все команды ниже из ноутбука на мастер-ноде\n",
    "\n",
    "PySpark - это не обычная библиотека, поэтому по-умолчанию ее нет в списке установленных пакетов\n",
    "\n",
    "Чтобы решить эту проблему простым способом, добрые люди сделали небольшую библиотеку findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 34.89501953125,
      "end_time": 1612352915112.742
     }
    }
   },
   "outputs": [],
   "source": [
    "! pip install findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 68.73486328125,
      "end_time": 1612353023530.31
     }
    }
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "sc = pyspark.SparkContext(appName=\"lsml-app\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Работаем с RDD\n",
    "\n",
    "RDD - это базовый строительный блок для Spark. Спарк внимательно следит за тем, где лежат части RDD и как они были созданы. RDD по сути своей представляют упорядоченный набор записей. Большое число функций считают, что это пары ключ-значение (также как было в MapReduce), но на деле это может быть и произвольные данные.\n",
    "\n",
    "RDD сами по себе неизменяемые. Можно лишь получить новый RDD, применяя различные операции к изначальному RDD.\n",
    "\n",
    "Существуют два вида операций над RDD - Действия (actions) и Трансформации (transformations).\n",
    "\n",
    "Трансформации не применяются сразу - они лишь записываются в пул примененных операций. Чтобы что-то действительно началось считаться, нужно применить уже действие - тогда все указанные транформации действительно начнут считаться на кластере.\n",
    "\n",
    "Давайте сразу смотреть на примерах, чтобы стало понятно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 347.635009765625,
      "end_time": 1612353266398.446
     }
    }
   },
   "outputs": [],
   "source": [
    "rdd = sc.parallelize(range(10))  # Создаем rdd из обычного списка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 95.464111328125,
      "end_time": 1612353269918.691
     }
    }
   },
   "outputs": [],
   "source": [
    "rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 2343.68505859375,
      "end_time": 1612353297228.935
     }
    }
   },
   "outputs": [],
   "source": [
    "rdd.collect()  # Получить значение всего RDD в память. Аккуратнее - если RDD большой, у вас просто лопнем питон"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 335.6630859375,
      "end_time": 1612353373733.558
     }
    }
   },
   "outputs": [],
   "source": [
    "rdd.count()  # Считаем количество элементов в RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 356.94091796875,
      "end_time": 1612353382653.109
     }
    }
   },
   "outputs": [],
   "source": [
    "rdd.first()  # Берем только первый элемент"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 894.1611328125,
      "end_time": 1612353434994.542
     }
    }
   },
   "outputs": [],
   "source": [
    "rdd.take(2)  # Берем первые N элементов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 317.31005859375,
      "end_time": 1612353450417.845
     }
    }
   },
   "outputs": [],
   "source": [
    "rdd.mean()  # Считаем среднее по всем элементам. Важно, чтобы элементы внутри RDD поддерживали суммирование и деление"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 81.319091796875,
      "end_time": 1612353469108.061
     }
    }
   },
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([\"biba\", \"kuka\"])  # Можем положить и строки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 268.843017578125,
      "end_time": 1612353470625.169
     }
    }
   },
   "outputs": [],
   "source": [
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! hdfs dfs -ls /user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! hdfs dfs -mkdir -p /user/spark-example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 6073.139892578125,
      "end_time": 1612353482524.937
     }
    }
   },
   "outputs": [],
   "source": [
    "! hdfs dfs -rm -r /user/spark-example/biba_and_kuka.txt || true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 2377.29296875,
      "end_time": 1612353510464.038
     }
    }
   },
   "outputs": [],
   "source": [
    "rdd.saveAsTextFile(\"/user/spark-example/biba_and_kuka.txt\")  # Сохраняем RDD в HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 6995.572998046875,
      "end_time": 1612342093294.737
     }
    }
   },
   "outputs": [],
   "source": [
    "! hdfs dfs -ls /user/spark-example/biba_and_kuka.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 10939.029052734375,
      "end_time": 1612353580378.112
     }
    }
   },
   "outputs": [],
   "source": [
    "! hdfs dfs -cat /user/spark-example/biba_and_kuka.txt/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Добавим теперь еще трансформации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 62.633056640625,
      "end_time": 1612353596244.709
     }
    }
   },
   "outputs": [],
   "source": [
    "rdd = sc.parallelize(range(10))  # Создаем rdd из обычного списка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 57.074951171875,
      "end_time": 1612353744051.49
     }
    }
   },
   "outputs": [],
   "source": [
    "# Создаем rdd в котором каждый элемент возведен в квадрат\n",
    "# map работает примерно также как и map в MapReduce. \n",
    "# Разница - мы не обрабатываем блок самостоятельно, а пишем функцию для обработки ровно одной записи\n",
    "squares = rdd.map(lambda x: x**2).map(lambda x: x + 1)\n",
    "\n",
    "# ВАЖНО - на самом деле ничего считаться в этот момент не начало\n",
    "# Мы лишь записали наше желание получить новый RDD и записали это желание в squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 321.33203125,
      "end_time": 1612353744490.673
     }
    }
   },
   "outputs": [],
   "source": [
    "squares.first() \n",
    "\n",
    "# Так как мы применили Action то вот теперь все трансформации запустились\n",
    "# Но так как action требует только первую строку, то Spark оптимизировал вычисления\n",
    "# он прочитал только первую строку и для нее вычислил значение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 354.3359375,
      "end_time": 1612353744887.548
     }
    }
   },
   "outputs": [],
   "source": [
    "squares.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Начнем работать с данными"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Датасет - тот же, что и на предыдущем семинаре"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! hdfs dfs -ls /user/tweets/data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 296.0029296875,
      "end_time": 1612353840629.932
     }
    }
   },
   "outputs": [],
   "source": [
    "data = sc.textFile(\"/user/tweets/data/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 873.843994140625,
      "end_time": 1612353845982.603
     }
    }
   },
   "outputs": [],
   "source": [
    "data.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 136.76708984375,
      "end_time": 1612353905752.842
     }
    }
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def extract_text(raw_string):\n",
    "    parsed_line = next(csv.reader([raw_string]))\n",
    "    text = parsed_line[2]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 830.344970703125,
      "end_time": 1612353924927.376
     }
    }
   },
   "outputs": [],
   "source": [
    "data.map(extract_text).first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 91.820068359375,
      "end_time": 1612354017669.193
     }
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_words(text):\n",
    "    pattern = re.compile(r\"[a-z]+\")\n",
    "    result = []\n",
    "    for match in pattern.finditer(text.lower()):\n",
    "        word = match.group(0)\n",
    "        result.append(word)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 816.132080078125,
      "end_time": 1612354050310.284
     }
    }
   },
   "outputs": [],
   "source": [
    "data.map(extract_text).map(extract_words).take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 823.447021484375,
      "end_time": 1612354082447.02
     }
    }
   },
   "outputs": [],
   "source": [
    "data.map(extract_text).flatMap(extract_words).first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 814.89306640625,
      "end_time": 1612354091193.413
     }
    }
   },
   "outputs": [],
   "source": [
    "data.map(extract_text).flatMap(extract_words).take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Все трансформации вычисляются каждый раз с самого первого RDD. Чтобы уменьшить количество лишних вычислений можно закешировать временный результат. Тогда он будет по максимуму переиспользоваться."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 295.781982421875,
      "end_time": 1612354213990.163
     }
    }
   },
   "outputs": [],
   "source": [
    "words = data.map(extract_text).flatMap(extract_words).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 19594.058837890625,
      "end_time": 1612354236725.854
     }
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "words.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 2308.721923828125,
      "end_time": 1612354257919.374
     }
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "words.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На моем запуске второй запуск `words.count()` работал 2 секунды вместо 17"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word count\n",
    "\n",
    "Попробуем реализовать тот же алгоритм, что и для классического MapReduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 259.268798828125,
      "end_time": 1612354326981.429
     }
    }
   },
   "outputs": [],
   "source": [
    "words.map(lambda x: (x, 1)).first()  # Строим пары ключ значение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 23503.94384765625,
      "end_time": 1612354363998.507
     }
    }
   },
   "outputs": [],
   "source": [
    "(\n",
    "    words\n",
    "    .map(lambda x: (x, 1))\n",
    "    .groupByKey()  # Функция работает, только если RDD - это пары ключ-значение\n",
    "    .take(2)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 23478.2060546875,
      "end_time": 1612354432752.624
     }
    }
   },
   "outputs": [],
   "source": [
    "# ПЛОХОЙ ВАРИАНТ: материализуем весь x через list\n",
    "(\n",
    "    words\n",
    "    .map(lambda x: (x, 1))\n",
    "    .groupByKey()  # Функция работает, только если RDD - это пары ключ-значение\n",
    "    .map(lambda x: (x[0], sum(list(x[1]))))\n",
    "    .take(10)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ИСПРАВЛЕННЫЙ ВАРИАНТ\n",
    "(\n",
    "    words\n",
    "    .map(lambda x: (x, 1))\n",
    "    .groupByKey()  # Функция работает, только если RDD - это пары ключ-значение\n",
    "    .map(lambda x: (x[0], sum(x[1])))\n",
    "    .take(10)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 17406.94287109375,
      "end_time": 1612354558105.175
     }
    }
   },
   "outputs": [],
   "source": [
    "(\n",
    "    words\n",
    "    .map(lambda x: (x, 1))\n",
    "    .reduceByKey(lambda a, b: a + b)  # Если уже готовая функция для reduce\n",
    "    .take(10)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import add\n",
    "\n",
    "# то же самое с помощью оператора самого спарка\n",
    "\n",
    "(\n",
    "    words\n",
    "    .map(lambda x: (x, 1))\n",
    "    .reduceByKey(add) \n",
    "    .take(10)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 17426.239013671875,
      "end_time": 1612354616853.129
     }
    }
   },
   "outputs": [],
   "source": [
    "(\n",
    "    words\n",
    "    .map(lambda x: (x, 1))\n",
    "    .reduceByKey(lambda a, b: a + b)\n",
    "    .takeOrdered(10, lambda x: -x[1])  # Сортируем по значению функции\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 17466.43701171875,
      "end_time": 1612354698072.037
     }
    }
   },
   "outputs": [],
   "source": [
    "result_50 = (\n",
    "    words\n",
    "    .map(lambda x: (x, 1))\n",
    "    .reduceByKey(lambda a, b: a + b)\n",
    "    .takeOrdered(50, lambda x: -x[1])\n",
    ")\n",
    "\n",
    "stop_words = [word for word, _ in result_50]  # Предподсчитали стоп слова"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 66.18798828125,
      "end_time": 1612354698177.797
     }
    }
   },
   "outputs": [],
   "source": [
    "stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 23490.77099609375,
      "end_time": 1612354745158.972
     }
    }
   },
   "outputs": [],
   "source": [
    "(\n",
    "    words\n",
    "    .filter(lambda x: x not in stop_words)\n",
    "    .map(lambda x: (x, 1))\n",
    "    .reduceByKey(lambda a, b: a + b)\n",
    "    .takeOrdered(50, lambda x: -x[1])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кроме базовых, есть еще и много продвинутых сложных функций\n",
    "Например можем посчитать уникальные слова в датасете\n",
    "\n",
    "Список всех можно смотреть в документации\n",
    "\n",
    "https://spark.apache.org/docs/latest/rdd-programming-guide.html#actions\n",
    "\n",
    "https://spark.apache.org/docs/latest/rdd-programming-guide.html#transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 15403.302001953125,
      "end_time": 1612354828533.726
     }
    }
   },
   "outputs": [],
   "source": [
    "words.distinct().take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 17425.8349609375,
      "end_time": 1612354845992.719
     }
    }
   },
   "outputs": [],
   "source": [
    "words.distinct().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Однако иногда каких-то базовых примитивов может и не найтись. Например для RDD нет функции `limit` или около того.\n",
    "\n",
    "Поэтому чтобы решить задачу top10 и сохранить это в HDFS нужно применить некоторую изобретательность"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 29512.59814453125,
      "end_time": 1612354979955.66
     }
    }
   },
   "outputs": [],
   "source": [
    "(\n",
    "    words\n",
    "    .filter(lambda x: x not in stop_words)\n",
    "    .map(lambda x: (x, 1))\n",
    "    .reduceByKey(lambda a, b: a + b)\n",
    "    .map(lambda x: (x[1], x[0]))\n",
    "    .sortByKey(ascending=False)\n",
    "    .zipWithIndex()\n",
    "    .take(10)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 31554.372802734375,
      "end_time": 1612355051481.705
     }
    }
   },
   "outputs": [],
   "source": [
    "(\n",
    "    words\n",
    "    .filter(lambda x: x not in stop_words)\n",
    "    .map(lambda x: (x, 1))\n",
    "    .reduceByKey(lambda a, b: a + b)\n",
    "    .map(lambda x: (x[1], x[0]))\n",
    "    .sortByKey(ascending=False)\n",
    "    .zipWithIndex()\n",
    "    .filter(lambda x: x[1] < 10)\n",
    "    .map(lambda x: x[0])\n",
    "    .collect()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! hdfs dfs -ls /user/tweets/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 7770.451171875,
      "end_time": 1612355087945.224
     }
    }
   },
   "outputs": [],
   "source": [
    "! hdfs dfs -rm -r /user/tweets/spark/top10 || true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 33516.773193359375,
      "end_time": 1612355130340.745
     }
    }
   },
   "outputs": [],
   "source": [
    "(\n",
    "    words\n",
    "    .filter(lambda x: x not in stop_words)\n",
    "    .map(lambda x: (x, 1))\n",
    "    .reduceByKey(lambda a, b: a + b)\n",
    "    .map(lambda x: (x[1], x[0]))\n",
    "    .sortByKey(ascending=False)\n",
    "    .zipWithIndex()\n",
    "    .filter(lambda x: x[1] < 10)\n",
    "    .map(lambda x: x[0])\n",
    "    .saveAsTextFile('/user/tweets/spark/top10')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 8163.9921875,
      "end_time": 1612355138511.746
     }
    }
   },
   "outputs": [],
   "source": [
    "! hdfs dfs -cat /user/tweets/spark/top10/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Partitions\n",
    "\n",
    "Под капотом Spark эксплуатирует примерно те же идеи, что и классический MapReduce. Это означает, что при необходимости сортировки, он разбивает ключи на группы и передает редюсерам на обработку только их часть.\n",
    "\n",
    "На этот процесс также можно влиять. Это может позводить улучшить производительность программ, а также решить проблемы переполнения редюсеров."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 51.573974609375,
      "end_time": 1612355288369.505
     }
    }
   },
   "outputs": [],
   "source": [
    "words.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 48.348876953125,
      "end_time": 1612355336156.765
     }
    }
   },
   "outputs": [],
   "source": [
    "numbers = sc.parallelize(range(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 304.620849609375,
      "end_time": 1612355357451.627
     }
    }
   },
   "outputs": [],
   "source": [
    "numbers.glom().collect()  # Получаем доступ до данных в каждей партиции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 144.0498046875,
      "end_time": 1612355446560.521
     }
    }
   },
   "outputs": [],
   "source": [
    "squares = numbers.map(lambda x: (x, x**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 111.16015625,
      "end_time": 1612346527646.15
     }
    }
   },
   "source": [
    "Операции изменения партиций предполагают наличие ключа, поэтому вначале преобразуем данные к виду ключ-значение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 301.077880859375,
      "end_time": 1612355479629.363
     }
    }
   },
   "outputs": [],
   "source": [
    "squares.partitionBy(2).glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 800.200927734375,
      "end_time": 1612355511548.453
     }
    }
   },
   "outputs": [],
   "source": [
    "squares.partitionBy(15).glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 91.37109375,
      "end_time": 1612355551455.15
     }
    }
   },
   "outputs": [],
   "source": [
    "def custom_partitioner(value):\n",
    "    return value % 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 806.29296875,
      "end_time": 1612355558129.937
     }
    }
   },
   "outputs": [],
   "source": [
    "squares.partitionBy(3, custom_partitioner).glom().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Таким образом можно выбирать более удачные способы разбиения и например увеличивать количество редюсеров под вашу задачу.\n",
    "\n",
    "Или наоборот, уменьшать количество количество партиций, если они избыточны. Например вы отфильтровали гигантский датасет и теперь вам больше не требуется такое гигантское количество партиций для работы.\n",
    "\n",
    "Для этого можно использовать и `repartition` как делали выше, однако этот метод запустит пересортировку вообще всего RDD, что дорого и излишне. Чтобы так не было, можно использовать функцию `coalesce` - она просто схлопнуть вместе те партиции, котороые уже находятся на одной машине, что значительно уменьшит количество лишних телодвижений."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 335.170166015625,
      "end_time": 1612355585653.388
     }
    }
   },
   "outputs": [],
   "source": [
    "numbers = sc.parallelize(range(10), 10) # явно указали вторым аргументом количество партиций\n",
    "numbers.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "squares = numbers.map(lambda x: (x, x**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 267.27197265625,
      "end_time": 1612355612327.429
     }
    }
   },
   "outputs": [],
   "source": [
    "squares.filter(lambda x: x[0] >= 7).glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 816.72802734375,
      "end_time": 1612355755256.698
     }
    }
   },
   "outputs": [],
   "source": [
    "squares.filter(lambda x: x[0] >= 7).coalesce(2).glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "squares.filter(lambda x: x[0] >= 7).partitionBy(2).glom().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DataFrame и SQL\n",
    "\n",
    "Уже текущий набор функций - это большой шаг вперед относительно классического MapReduce. Однако на этом плюшки Spark не заканчиваются. Разработчики пошли дальше и начали внедрять еще более высокоуровневый интерфейс для работы с данными, который может сильно упростить жизнь разработчикам.\n",
    "\n",
    "DataFrame - это модель таблицы, построенная поверх RDD. О ней можно думать как о Pandas на стероидах."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 276.751953125,
      "end_time": 1612355862565.257
     }
    }
   },
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([(\"a\", 1), (\"a\", 2), (\"b\", 3), (\"b\", 4)])\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 184.1630859375,
      "end_time": 1612355868876.816
     }
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, Row\n",
    "se = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 3370.230224609375,
      "end_time": 1612355896303.066
     }
    }
   },
   "outputs": [],
   "source": [
    "df = se.createDataFrame(rdd)\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 790.541015625,
      "end_time": 1612355952915.348
     }
    }
   },
   "outputs": [],
   "source": [
    "df = se.createDataFrame(\n",
    "    rdd.map(lambda x: Row(pipa=x[0], pupa=x[1]))\n",
    ")\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для удобства есть встроенные функции конвертации в pandas и оттуда"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 397.39306640625,
      "end_time": 1612355971061.935
     }
    }
   },
   "outputs": [],
   "source": [
    "pandas_df = df.toPandas()\n",
    "pandas_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 769.448974609375,
      "end_time": 1612355984886.015
     }
    }
   },
   "outputs": [],
   "source": [
    "df = se.createDataFrame(pandas_df)\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Есть специальные функции, которые умеют работать с популярными форматами хранения таблиц, и строить их в HDFS.\n",
    "\n",
    "Прочтем нашу таблицу через DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 13481.74609375,
      "end_time": 1612356061360.093
     }
    }
   },
   "outputs": [],
   "source": [
    "df = se.read.csv('/user/tweets/data/*', header=False, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 69.850830078125,
      "end_time": 1612356061473.22
     }
    }
   },
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 81.527099609375,
      "end_time": 1612356081724.692
     }
    }
   },
   "outputs": [],
   "source": [
    "columns = [\n",
    "    'external_author_id',\n",
    "    'author',\n",
    "    'content',\n",
    "    'region',\n",
    "    'language',\n",
    "    'publish_date',\n",
    "    'harvested_date',\n",
    "    'following',\n",
    "    'followers',\n",
    "    'updates',\n",
    "    'post_type',\n",
    "    'account_type',\n",
    "    'retweet',\n",
    "    'account_category',\n",
    "    'new_june_2018',\n",
    "    'alt_external_id',\n",
    "    'tweet_id',\n",
    "    'article_url',\n",
    "    'tco1_step1',\n",
    "    'tco2_step1',\n",
    "    'tco3_step1'\n",
    "]\n",
    "df = df.toDF(*columns)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 792.657958984375,
      "end_time": 1612356091152.882
     }
    }
   },
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 810.4111328125,
      "end_time": 1612356117686.434
     }
    }
   },
   "outputs": [],
   "source": [
    "df[['author', 'content']].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 329.391845703125,
      "end_time": 1612356292166.772
     }
    }
   },
   "outputs": [],
   "source": [
    "df.registerTempTable('tweets')  # Регистрируем как временную таблицу для SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 3312.72412109375,
      "end_time": 1612356401518.544
     }
    }
   },
   "outputs": [],
   "source": [
    "se.sql(\"\"\"\n",
    "    SELECT author, content, followers\n",
    "    FROM tweets\n",
    "    WHERE followers > 100\n",
    "    LIMIT 10\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 11399.593017578125,
      "end_time": 1612356600761.666
     }
    }
   },
   "outputs": [],
   "source": [
    "se.sql(\"\"\"\n",
    "    SELECT language, count(*) as tw_count\n",
    "    FROM tweets\n",
    "    WHERE followers > 100\n",
    "    GROUP BY language\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 9376.43310546875,
      "end_time": 1612356690299.877
     }
    }
   },
   "outputs": [],
   "source": [
    "top5_lang = se.sql(\"\"\"\n",
    "    SELECT language, count(*) as tw_count\n",
    "    FROM tweets\n",
    "    WHERE followers > 100\n",
    "    GROUP BY language\n",
    "    ORDER BY tw_count DESC\n",
    "    LIMIT 5\n",
    "\"\"\")\n",
    "top5_lang.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 9388.822021484375,
      "end_time": 1612356771768.614
     }
    }
   },
   "outputs": [],
   "source": [
    "only_langs_df = se.sql(\"\"\"\n",
    "    SELECT language\n",
    "    FROM (\n",
    "        SELECT language, count(*) as tw_count\n",
    "        FROM tweets\n",
    "        WHERE followers > 100\n",
    "        GROUP BY language\n",
    "        ORDER BY tw_count DESC\n",
    "        LIMIT 5\n",
    "    )\n",
    "\"\"\")\n",
    "only_langs_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 261.72998046875,
      "end_time": 1612356783401.478
     }
    }
   },
   "outputs": [],
   "source": [
    "only_langs_df.registerTempTable('languages')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 9367.490966796875,
      "end_time": 1612356833696.505
     }
    }
   },
   "outputs": [],
   "source": [
    "se.sql(\"\"\"\n",
    "    SELECT author, language\n",
    "    FROM tweets\n",
    "    WHERE language in (SELECT * FROM languages)\n",
    "    LIMIT 10\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 9543.322021484375,
      "end_time": 1612356868278.254
     }
    }
   },
   "outputs": [],
   "source": [
    "se.sql(\"\"\"\n",
    "    SELECT author, t.language\n",
    "    FROM tweets t\n",
    "        inner join languages l on l.language = t.language\n",
    "    LIMIT 10\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 290.721923828125,
      "end_time": 1612356987925.346
     }
    }
   },
   "outputs": [],
   "source": [
    "# Из под датафрейма всегда можно вынуть RDD и работать напрямую уже с ним\n",
    "\n",
    "top5_lang.rdd.map(lambda x: x.language.upper()).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 798.162841796875,
      "end_time": 1612357015786.159
     }
    }
   },
   "outputs": [],
   "source": [
    "top5_lang.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
